{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StructType(List(StructField(Timestamp,LongType,true),StructField(Geohash,StringType,true),StructField(geopotential_height_lltw,FloatType,true),StructField(water_equiv_of_accum_snow_depth_surface,FloatType,true),StructField(drag_coefficient_surface,FloatType,true),StructField(sensible_heat_net_flux_surface,FloatType,true),StructField(categorical_ice_pellets_yes1_no0_surface,FloatType,true),StructField(visibility_surface,FloatType,true),StructField(number_of_soil_layers_in_root_zone_surface,FloatType,true),StructField(categorical_freezing_rain_yes1_no0_surface,FloatType,true),StructField(pressure_reduced_to_msl_msl,FloatType,true),StructField(upward_short_wave_rad_flux_surface,FloatType,true),StructField(relative_humidity_zerodegc_isotherm,FloatType,true),StructField(categorical_snow_yes1_no0_surface,FloatType,true),StructField(u-component_of_wind_tropopause,FloatType,true),StructField(surface_wind_gust_surface,FloatType,true),StructField(total_cloud_cover_entire_atmosphere,FloatType,true),StructField(upward_long_wave_rad_flux_surface,FloatType,true),StructField(land_cover_land1_sea0_surface,FloatType,true),StructField(vegitation_type_as_in_sib_surface,FloatType,true),StructField(v-component_of_wind_pblri,FloatType,true),StructField(albedo_surface,FloatType,true),StructField(lightning_surface,FloatType,true),StructField(ice_cover_ice1_no_ice0_surface,FloatType,true),StructField(convective_inhibition_surface,FloatType,true),StructField(pressure_surface,FloatType,true),StructField(transpiration_stress-onset_soil_moisture_surface,FloatType,true),StructField(soil_porosity_surface,FloatType,true),StructField(vegetation_surface,FloatType,true),StructField(categorical_rain_yes1_no0_surface,FloatType,true),StructField(downward_long_wave_rad_flux_surface,FloatType,true),StructField(planetary_boundary_layer_height_surface,FloatType,true),StructField(soil_type_as_in_zobler_surface,FloatType,true),StructField(geopotential_height_cloud_base,FloatType,true),StructField(friction_velocity_surface,FloatType,true),StructField(maximumcomposite_radar_reflectivity_entire_atmosphere,FloatType,true),StructField(plant_canopy_surface_water_surface,FloatType,true),StructField(v-component_of_wind_maximum_wind,FloatType,true),StructField(geopotential_height_zerodegc_isotherm,FloatType,true),StructField(mean_sea_level_pressure_nam_model_reduction_msl,FloatType,true),StructField(temperature_surface,FloatType,true),StructField(snow_cover_surface,FloatType,true),StructField(geopotential_height_surface,FloatType,true),StructField(convective_available_potential_energy_surface,FloatType,true),StructField(latent_heat_net_flux_surface,FloatType,true),StructField(surface_roughness_surface,FloatType,true),StructField(pressure_maximum_wind,FloatType,true),StructField(temperature_tropopause,FloatType,true),StructField(geopotential_height_pblri,FloatType,true),StructField(pressure_tropopause,FloatType,true),StructField(snow_depth_surface,FloatType,true),StructField(v-component_of_wind_tropopause,FloatType,true),StructField(downward_short_wave_rad_flux_surface,FloatType,true),StructField(u-component_of_wind_maximum_wind,FloatType,true),StructField(wilting_point_surface,FloatType,true),StructField(precipitable_water_entire_atmosphere,FloatType,true),StructField(u-component_of_wind_pblri,FloatType,true),StructField(direct_evaporation_cease_soil_moisture_surface,FloatType,true)))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql.types import StructType, StructField, FloatType, LongType, StringType\n",
    "\n",
    "feats = []\n",
    "f = open('features.txt')\n",
    "for line_num, line in enumerate(f):\n",
    "    if line_num == 0:\n",
    "        # Timestamp\n",
    "        feats.append(StructField(line.strip(), LongType(), True))\n",
    "    elif line_num == 1:\n",
    "        # Geohash\n",
    "        feats.append(StructField(line.strip(), StringType(), True))\n",
    "    else:\n",
    "        # Other features\n",
    "        feats.append(StructField(line.strip(), FloatType(), True))\n",
    "    \n",
    "schema = StructType(feats)\n",
    "\n",
    "print(schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#df = spark.read.format('csv').option('sep', '\\t').schema(schema).load('/Volumes/evo/Datasets/NAM_2015_S/*')\n",
    "df = spark.read.format('csv').option('sep', '\\t').schema(schema).load('hdfs://orion11:40910/nam_tiny.tdv')\n",
    "# df = spark.read.format('csv').option('sep', '\\t').schema(schema).load('hdfs://orion11:40910/datasets/nam_201501.tdv.gz')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_large = spark.read.format('csv').option('sep', '\\t').schema(schema).load('hdfs://orion11:40910/datasets/nam_201509.tdv.gz')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_entire = spark.read.format('csv').option('sep', '\\t').schema(schema).load('hdfs://orion11:40910/datasets/*')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "really_hot = df.filter(df.snow_depth_surface != 0).count()\n",
    "print(really_hot)\n",
    "\n",
    "hot_and_humid = df.filter(df.temperature_surface > 313).filter(df.relative_humidity_zerodegc_isotherm > .8).count()\n",
    "print(hot_and_humid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Timestamp=1426377600000, Geohash='cf7ecr4h2ps0', geopotential_height_lltw=136.53125, water_equiv_of_accum_snow_depth_surface=77.0, drag_coefficient_surface=0.0, sensible_heat_net_flux_surface=-39.57763671875, categorical_ice_pellets_yes1_no0_surface=0.0, visibility_surface=24221.587890625, number_of_soil_layers_in_root_zone_surface=3.0, categorical_freezing_rain_yes1_no0_surface=0.0, pressure_reduced_to_msl_msl=99602.0, upward_short_wave_rad_flux_surface=6.625, relative_humidity_zerodegc_isotherm=34.0, categorical_snow_yes1_no0_surface=0.0, u-component_of_wind_tropopause=27.527877807617188, surface_wind_gust_surface=16.158788681030273, total_cloud_cover_entire_atmosphere=100.0, upward_long_wave_rad_flux_surface=314.0560302734375, land_cover_land1_sea0_surface=1.0, vegitation_type_as_in_sib_surface=18.0, v-component_of_wind_pblri=12.31915283203125, albedo_surface=38.75, lightning_surface=0.0, ice_cover_ice1_no_ice0_surface=0.0, convective_inhibition_surface=-0.65234375, pressure_surface=97221.0, transpiration_stress-onset_soil_moisture_surface=0.3125, soil_porosity_surface=0.5, vegetation_surface=1.0, categorical_rain_yes1_no0_surface=0.0, downward_long_wave_rad_flux_surface=274.9662780761719, planetary_boundary_layer_height_surface=1905.5, soil_type_as_in_zobler_surface=3.0, geopotential_height_cloud_base=6696.0, friction_velocity_surface=0.6008660197257996, maximumcomposite_radar_reflectivity_entire_atmosphere=-20.0, plant_canopy_surface_water_surface=0.10999999940395355, v-component_of_wind_maximum_wind=-7.388824462890625, geopotential_height_zerodegc_isotherm=2380.0, mean_sea_level_pressure_nam_model_reduction_msl=99607.0, temperature_surface=272.7480163574219, snow_cover_surface=100.0, geopotential_height_surface=195.8200225830078, convective_available_potential_energy_surface=0.0, latent_heat_net_flux_surface=-0.0329132080078125, surface_roughness_surface=0.1750158965587616, pressure_maximum_wind=20726.736328125, temperature_tropopause=206.35687255859375, geopotential_height_pblri=300.42779541015625, pressure_tropopause=20582.23828125, snow_depth_surface=0.4211999773979187, v-component_of_wind_tropopause=-7.8085479736328125, downward_short_wave_rad_flux_surface=17.0, u-component_of_wind_maximum_wind=28.66766357421875, wilting_point_surface=0.04749999940395355, precipitable_water_entire_atmosphere=9.207815170288086, u-component_of_wind_pblri=4.711578369140625, direct_evaporation_cease_soil_moisture_surface=0.04749999940395355),\n",
       " Row(Timestamp=1426377600000, Geohash='ccn3u17cxe80', geopotential_height_lltw=4.28125, water_equiv_of_accum_snow_depth_surface=26.0, drag_coefficient_surface=0.0, sensible_heat_net_flux_surface=-12.95263671875, categorical_ice_pellets_yes1_no0_surface=0.0, visibility_surface=24221.587890625, number_of_soil_layers_in_root_zone_surface=4.0, categorical_freezing_rain_yes1_no0_surface=0.0, pressure_reduced_to_msl_msl=101712.0, upward_short_wave_rad_flux_surface=1.75, relative_humidity_zerodegc_isotherm=10.0, categorical_snow_yes1_no0_surface=0.0, u-component_of_wind_tropopause=16.527877807617188, surface_wind_gust_surface=6.90878963470459, total_cloud_cover_entire_atmosphere=0.0, upward_long_wave_rad_flux_surface=303.1810302734375, land_cover_land1_sea0_surface=1.0, vegitation_type_as_in_sib_surface=1.0, v-component_of_wind_pblri=5.63165283203125, albedo_surface=24.5, lightning_surface=0.0, ice_cover_ice1_no_ice0_surface=0.0, convective_inhibition_surface=-0.65234375, pressure_surface=96528.0, transpiration_stress-onset_soil_moisture_surface=0.3125, soil_porosity_surface=0.5, vegetation_surface=15.75, categorical_rain_yes1_no0_surface=0.0, downward_long_wave_rad_flux_surface=231.71627807617188, planetary_boundary_layer_height_surface=407.0, soil_type_as_in_zobler_surface=3.0, geopotential_height_cloud_base=-5000.0, friction_velocity_surface=0.3508659601211548, maximumcomposite_radar_reflectivity_entire_atmosphere=-20.0, plant_canopy_surface_water_surface=0.5, v-component_of_wind_maximum_wind=-34.263824462890625, geopotential_height_zerodegc_isotherm=2440.0, mean_sea_level_pressure_nam_model_reduction_msl=101723.0, temperature_surface=270.3730163574219, snow_cover_surface=100.0, geopotential_height_surface=426.32000732421875, convective_available_potential_energy_surface=0.0, latent_heat_net_flux_surface=-0.0329132080078125, surface_roughness_surface=1.9000158309936523, pressure_maximum_wind=22326.736328125, temperature_tropopause=209.85687255859375, geopotential_height_pblri=328.42779541015625, pressure_tropopause=21382.23828125, snow_depth_surface=0.052799999713897705, v-component_of_wind_tropopause=-33.18354797363281, downward_short_wave_rad_flux_surface=6.875, u-component_of_wind_maximum_wind=15.91766357421875, wilting_point_surface=0.04749999940395355, precipitable_water_entire_atmosphere=5.457814693450928, u-component_of_wind_pblri=2.274078369140625, direct_evaporation_cease_soil_moisture_surface=0.04749999940395355),\n",
       " Row(Timestamp=1426377600000, Geohash='f2fgf6usbe2p', geopotential_height_lltw=-472.96875, water_equiv_of_accum_snow_depth_surface=142.0, drag_coefficient_surface=0.0, sensible_heat_net_flux_surface=-7.07763671875, categorical_ice_pellets_yes1_no0_surface=0.0, visibility_surface=7621.5888671875, number_of_soil_layers_in_root_zone_surface=4.0, categorical_freezing_rain_yes1_no0_surface=0.0, pressure_reduced_to_msl_msl=101610.0, upward_short_wave_rad_flux_surface=0.0, relative_humidity_zerodegc_isotherm=77.0, categorical_snow_yes1_no0_surface=0.0, u-component_of_wind_tropopause=20.277877807617188, surface_wind_gust_surface=7.28378963470459, total_cloud_cover_entire_atmosphere=100.0, upward_long_wave_rad_flux_surface=291.5560302734375, land_cover_land1_sea0_surface=1.0, vegitation_type_as_in_sib_surface=1.0, v-component_of_wind_pblri=-1.68084716796875, albedo_surface=39.5, lightning_surface=0.0, ice_cover_ice1_no_ice0_surface=0.0, convective_inhibition_surface=-0.65234375, pressure_surface=97035.0, transpiration_stress-onset_soil_moisture_surface=0.3125, soil_porosity_surface=0.5, vegetation_surface=3.0, categorical_rain_yes1_no0_surface=0.0, downward_long_wave_rad_flux_surface=280.2162780761719, planetary_boundary_layer_height_surface=798.5, soil_type_as_in_zobler_surface=3.0, geopotential_height_cloud_base=1541.0, friction_velocity_surface=0.6258659958839417, maximumcomposite_radar_reflectivity_entire_atmosphere=11.25, plant_canopy_surface_water_surface=0.25999999046325684, v-component_of_wind_maximum_wind=-1.513824462890625, geopotential_height_zerodegc_isotherm=0.0, mean_sea_level_pressure_nam_model_reduction_msl=101625.0, temperature_surface=267.7480163574219, snow_cover_surface=100.0, geopotential_height_surface=362.32000732421875, convective_available_potential_energy_surface=0.0, latent_heat_net_flux_surface=12.717086791992188, surface_roughness_surface=1.9000158309936523, pressure_maximum_wind=12126.736328125, temperature_tropopause=219.73187255859375, geopotential_height_pblri=193.92779541015625, pressure_tropopause=25782.23828125, snow_depth_surface=0.5703999996185303, v-component_of_wind_tropopause=10.816452026367188, downward_short_wave_rad_flux_surface=0.0, u-component_of_wind_maximum_wind=30.91766357421875, wilting_point_surface=0.04749999940395355, precipitable_water_entire_atmosphere=9.082815170288086, u-component_of_wind_pblri=-5.475921630859375, direct_evaporation_cease_soil_moisture_surface=0.04749999940395355),\n",
       " Row(Timestamp=1426377600000, Geohash='9x786rfxxpzz', geopotential_height_lltw=3072.03125, water_equiv_of_accum_snow_depth_surface=0.0, drag_coefficient_surface=0.0, sensible_heat_net_flux_surface=-18.82763671875, categorical_ice_pellets_yes1_no0_surface=0.0, visibility_surface=24221.587890625, number_of_soil_layers_in_root_zone_surface=3.0, categorical_freezing_rain_yes1_no0_surface=0.0, pressure_reduced_to_msl_msl=102109.0, upward_short_wave_rad_flux_surface=29.25, relative_humidity_zerodegc_isotherm=49.0, categorical_snow_yes1_no0_surface=0.0, u-component_of_wind_tropopause=9.652877807617188, surface_wind_gust_surface=5.90878963470459, total_cloud_cover_entire_atmosphere=70.0, upward_long_wave_rad_flux_surface=344.6810302734375, land_cover_land1_sea0_surface=1.0, vegitation_type_as_in_sib_surface=10.0, v-component_of_wind_pblri=5.06915283203125, albedo_surface=19.25, lightning_surface=0.0, ice_cover_ice1_no_ice0_surface=0.0, convective_inhibition_surface=-0.65234375, pressure_surface=78016.0, transpiration_stress-onset_soil_moisture_surface=0.32999998331069946, soil_porosity_surface=0.5, vegetation_surface=3.0, categorical_rain_yes1_no0_surface=0.0, downward_long_wave_rad_flux_surface=262.8412780761719, planetary_boundary_layer_height_surface=2201.0, soil_type_as_in_zobler_surface=6.0, geopotential_height_cloud_base=8044.0, friction_velocity_surface=0.3258659839630127, maximumcomposite_radar_reflectivity_entire_atmosphere=-8.3125, plant_canopy_surface_water_surface=0.47999998927116394, v-component_of_wind_maximum_wind=-44.763824462890625, geopotential_height_zerodegc_isotherm=3440.0, mean_sea_level_pressure_nam_model_reduction_msl=101683.0, temperature_surface=279.1230163574219, snow_cover_surface=100.0, geopotential_height_surface=2258.570068359375, convective_available_potential_energy_surface=0.0, latent_heat_net_flux_surface=25.217086791992188, surface_roughness_surface=0.1750158965587616, pressure_maximum_wind=20326.736328125, temperature_tropopause=209.85687255859375, geopotential_height_pblri=193.67779541015625, pressure_tropopause=17782.23828125, snow_depth_surface=0.0007999999797903001, v-component_of_wind_tropopause=-40.30854797363281, downward_short_wave_rad_flux_surface=152.375, u-component_of_wind_maximum_wind=5.04266357421875, wilting_point_surface=0.06624999642372131, precipitable_water_entire_atmosphere=6.082814693450928, u-component_of_wind_pblri=5.836578369140625, direct_evaporation_cease_soil_moisture_surface=0.06624999642372131),\n",
       " Row(Timestamp=1426377600000, Geohash='cf36gb2z345b', geopotential_height_lltw=315.28125, water_equiv_of_accum_snow_depth_surface=82.0, drag_coefficient_surface=0.0, sensible_heat_net_flux_surface=-39.32763671875, categorical_ice_pellets_yes1_no0_surface=0.0, visibility_surface=24221.587890625, number_of_soil_layers_in_root_zone_surface=3.0, categorical_freezing_rain_yes1_no0_surface=0.0, pressure_reduced_to_msl_msl=99229.0, upward_short_wave_rad_flux_surface=7.875, relative_humidity_zerodegc_isotherm=23.0, categorical_snow_yes1_no0_surface=0.0, u-component_of_wind_tropopause=24.402877807617188, surface_wind_gust_surface=14.65878963470459, total_cloud_cover_entire_atmosphere=100.0, upward_long_wave_rad_flux_surface=315.6810302734375, land_cover_land1_sea0_surface=1.0, vegitation_type_as_in_sib_surface=18.0, v-component_of_wind_pblri=11.38165283203125, albedo_surface=35.0, lightning_surface=0.0, ice_cover_ice1_no_ice0_surface=0.0, convective_inhibition_surface=-0.65234375, pressure_surface=95390.0, transpiration_stress-onset_soil_moisture_surface=0.3125, soil_porosity_surface=0.5, vegetation_surface=1.0, categorical_rain_yes1_no0_surface=0.0, downward_long_wave_rad_flux_surface=310.3412780761719, planetary_boundary_layer_height_surface=1036.0, soil_type_as_in_zobler_surface=3.0, geopotential_height_cloud_base=1977.0, friction_velocity_surface=0.6758660078048706, maximumcomposite_radar_reflectivity_entire_atmosphere=8.8125, plant_canopy_surface_water_surface=0.13499999046325684, v-component_of_wind_maximum_wind=-11.763824462890625, geopotential_height_zerodegc_isotherm=2680.0, mean_sea_level_pressure_nam_model_reduction_msl=99250.0, temperature_surface=273.1230163574219, snow_cover_surface=100.0, geopotential_height_surface=321.32000732421875, convective_available_potential_energy_surface=0.0, latent_heat_net_flux_surface=9.092086791992188, surface_roughness_surface=0.1750158965587616, pressure_maximum_wind=11126.736328125, temperature_tropopause=208.10687255859375, geopotential_height_pblri=258.67779541015625, pressure_tropopause=20782.23828125, snow_depth_surface=0.3779999911785126, v-component_of_wind_tropopause=6.4414520263671875, downward_short_wave_rad_flux_surface=22.75, u-component_of_wind_maximum_wind=27.04266357421875, wilting_point_surface=0.04749999940395355, precipitable_water_entire_atmosphere=10.332815170288086, u-component_of_wind_pblri=0.274078369140625, direct_evaporation_cease_soil_moisture_surface=0.04749999940395355)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df.filter(df.snow_depth_surface != 0).take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Yifan Zhou's Warn_up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1. Unknown Feature: Choose a feature from the data dictionary above that you have never heard of before. Inspect some of the values for the feature (such as its average, min, max, etc.) and try to guess what it measures. Was your hypothesis correct?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+\n",
      "|avg(geopotential_height_pblri)|\n",
      "+------------------------------+\n",
      "|              406.451460521314|\n",
      "+------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg\n",
    "\n",
    "df.select(avg(df.geopotential_height_pblri)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This attribute may records geopotential height for each loaction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2. Hot hot hot: When and where was the hottest temperature observed in the dataset? Is it an anomaly?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(Geohash='d5dpds10m55b', temperature_surface=331.390625)\n"
     ]
    }
   ],
   "source": [
    "# Creating an SQL 'table'\n",
    "df_entire.createOrReplaceTempView(\"TEMP_DF\")\n",
    "\n",
    "# Let's get all the snow cover values:\n",
    "hot = spark.sql(\"SELECT Geohash,temperature_surface FROM TEMP_DF WHERE temperature_surface IN (SELECT MAX(temperature_surface) FROM TEMP_DF) \").collect()\n",
    "\n",
    "for x in hot:\n",
    "    print(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hottest temperature in this dataset is 331.39F (166.3C). Obviously, Human cannot live in such hot place."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3. So Snowy: Find a location that is snowy all year (there are several). Locate a nearby town/city and provide a small writeup about it. Include pictures if you’d like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an SQL 'table'\n",
    "df_large.createOrReplaceTempView(\"TEMP_DF\")\n",
    "\n",
    "# Let's get all the snow cover values:\n",
    "snow = spark.sql(\"SELECT Geohash FROM TEMP_DF WHERE Geohash NOT IN (SELECT DISTINCT Geohash FROM TEMP_DF WHERE categorical_snow_yes1_no0_surface = 0) \").collect()\n",
    "for x in snow:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total snowy location count: 5\n",
      "Row(Geohash='c43k6uu1egxb', avgSnow=0.37908011869436203)\n",
      "Row(Geohash='c43kcu3t702p', avgSnow=0.37908011869436203)\n",
      "Row(Geohash='c41ueb1jyypb', avgSnow=0.3775964391691395)\n",
      "Row(Geohash='c41uhb4r5n00', avgSnow=0.3775964391691395)\n",
      "Row(Geohash='c41v48pupf00', avgSnow=0.37537091988130566)\n",
      "Finished. it's been 520 seconds\n"
     ]
    }
   ],
   "source": [
    "# Creating an SQL 'table'\n",
    "df_entire.createOrReplaceTempView(\"TEMP_DF\")\n",
    "started_at = datetime.now()\n",
    "# Let's get all the snow cover values:\n",
    "# not_snow_all_year = spark.sql(\"SELECT DISTINCT Geohash FROM TEMP_DF WHERE categorical_snow_yes1_no0_surface = 0 \").collect()\n",
    "# all_Geohash = spark.sql(\"SELECT DISTINCT Geohash FROM TEMP_DF\").collect()\n",
    "# snow = spark.sql(\"SELECT DISTINCT Geohash from TEMP_DF group by Geohash having avg(snow_cover_surface) >= 100\").collect()\n",
    "snow = spark.sql(\"SELECT DISTINCT Geohash, avg(categorical_snow_yes1_no0_surface) as avgSnow from TEMP_DF group by Geohash order by avgSnow DESC limit 5\").collect()\n",
    "print('Total snowy location count: %d' %len(snow))\n",
    "for x in snow:\n",
    "    print(x)\n",
    "print(\"Finished. it's been \" + str((datetime.now()-started_at).seconds) + \" seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1. Strangely Snowy: Find a location that contains snow while its surroundings do not. Why does this occur? Is it a high mountain peak in a desert?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(Geohash='c1gxsefbz52p', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c43hd76g0npb', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c43g3722yrzz', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c1nt970j5b5b', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c1grzzxwt1bp', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c436f3k4p8xb', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c43jhzrrsbrz', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c44hk1hy9u2p', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c46sf3s53p80', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c43gkzxxn6bp', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c41tugrtwhzz', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c458r20zx080', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c43gb4ywdgrz', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c438x5esgf00', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c4459s8rz1xb', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c43c1zbmntbp', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c43e1f64v9bp', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c41z4k0q1v00', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c43uwe7z04zz', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c44n20bvf7pb', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c41uxkww12rz', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c439bnrmtb80', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c1q28yjg8080', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c1nt41ctyzeb', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c432rdwup080', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c43esqrrbgpb', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c1gr1wvsqzrz', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c44nftm8u1zz', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c1gnx42ddfpb', avg(snow_cover_surface)=99.91358024691358)\n",
      "Row(Geohash='c43ejwc60zzz', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c41z9hnc43rz', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c452rpbu2mbp', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c1nmmz1m37rz', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c1nrnydywy00', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c44pt3ddqnpb', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c43cukjs4q2p', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c304zyed5j5b', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c43brsccqvzz', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c460e30rt8xb', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c43m2snbpzbp', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c43hwpfhs9zz', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c1gyvgbcd7rz', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c43kp9tv6krz', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c1grm74n9f00', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c43ddfshx9zz', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c1gzku493krz', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c1gx2zw2p3bp', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c43upev6nkzz', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c44pbsgr0r2p', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c43d5fyjw72p', avg(snow_cover_surface)=99.90123456790124)\n",
      "Row(Geohash='c41t9qfhk9rz', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c433u9bub3bp', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c44jutrxvc2p', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c1q0t6n70g00', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c41xbfbfkp80', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c436yjg4v2rz', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c438fqgmsm00', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c1pcwbxkywrz', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c1gxje99en80', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c43fx06cyhrz', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c437b0xke580', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c439n53vsxzz', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c43dwx55yr2p', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c4591sqdmz2p', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c1nqxts9reeb', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c43cmkr5gvzz', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c1nkytgn25h0', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c1p5fmbjmkrz', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c460wvwuzxzz', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c43673wec2pb', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c43sdmg10700', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c1gzb0p913rz', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c1p89bngnb00', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c4645ugrspzz', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c41vbxvq6k2p', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c3k8v0znz280', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c43k6uu1egxb', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c45b6umxp9xb', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c41uhb4r5n00', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c3hzccj7b75b', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c41ueb1jyypb', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c3048znkdueb', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c1pe3dez7y5b', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c41w0qb2mxxb', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c445v1jx5z2p', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c4387rjkb0rz', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c3k0yw8m6us0', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c3k3419762xb', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c43b05v7222p', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c41wrxgfefpb', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c436rnjk6zxb', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c43hr0kbbvpb', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c460h33t7dpb', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c43u6zh1sg80', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c4585eur3580', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c307564rcz7z', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c458dsjt0xxb', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c1pfn4p9hy5b', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c437gvnt3d00', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c464r96ksbxb', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c44h0kxx1rxb', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c36u4wfm3jup', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c41gtb5rubpb', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c44p3thkr700', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c1uhcd9k1n2p', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c44hyv9kjj00', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c1gyqex11wpb', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c43ggzmuxn00', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c3k2d8k3h7pb', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c46190vf50rz', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c41xurr50ypb', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c41zubt02prz', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c3041ty0ftpb', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c1nuq5290jup', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c41yek3dwk2p', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c1gy5p6c9n2p', avg(snow_cover_surface)=99.98765432098766)\n",
      "Row(Geohash='c41zmbwmedpb', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c41w7gq1ggpb', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c30154vrv1xb', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c36uq45f8f00', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c1snbcp98xzz', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c43j14zyhszz', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c44hg15vqhxb', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c44wfrncngpb', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c43kcu3t702p', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c46400s6m9xb', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c1gqdwstr52p', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c1gwp7n9ecbp', avg(snow_cover_surface)=99.98765432098766)\n",
      "Row(Geohash='c301dkmu8u2p', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c4658sph98xb', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c44n7tqsw5xb', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c43e8f003n00', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c43ff7d4xz2p', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c4393q9myw00', avg(snow_cover_surface)=99.90123456790124)\n",
      "Row(Geohash='c1nkrkswv2xb', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c1gzguk0t9bp', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c439t50n482p', avg(snow_cover_surface)=99.90123456790124)\n",
      "Row(Geohash='c1gru77j5fzz', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c41v48pupf00', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c43s5qjhy0xb', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c43h57tx92xb', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c1snut3f7mbp', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c41yybzdh1zz', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c1gqy6bzqnpb', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c41v98n9w0xb', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c43f77ggy1rz', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c4r5m1vqf07z', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c1p48r7t4qh0', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c1gz1py1burz', avg(snow_cover_surface)=100.0)\n",
      "149\n"
     ]
    }
   ],
   "source": [
    "df_large.createOrReplaceTempView(\"TEMP_DF\")\n",
    "not_snow_all_year = spark.sql(\"SELECT  Geohash,avg(snow_cover_surface) FROM TEMP_DF group by Geohash having avg(snow_cover_surface) >= 99\").collect()\n",
    "\n",
    "for x in not_snow_all_year:\n",
    "    print(x)\n",
    "print(len(not_snow_all_year))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c1gx\n",
      "c43h\n",
      "c43g\n",
      "c1nt\n",
      "c1gr\n",
      "c436\n",
      "c43j\n",
      "c44h\n",
      "c46s\n",
      "c43g\n",
      "c41t\n",
      "c458\n",
      "c43g\n",
      "c438\n",
      "c445\n",
      "c43c\n",
      "c43e\n",
      "c41z\n",
      "c43u\n",
      "c44n\n",
      "c41u\n",
      "c439\n",
      "c1q2\n",
      "c1nt\n",
      "c432\n",
      "c43e\n",
      "c1gr\n",
      "c44n\n",
      "c1gn\n",
      "c43e\n",
      "c41z\n",
      "c452\n",
      "c1nm\n",
      "c1nr\n",
      "c44p\n",
      "c43c\n",
      "c304\n",
      "c43b\n",
      "c460\n",
      "c43m\n",
      "c43h\n",
      "c1gy\n",
      "c43k\n",
      "c1gr\n",
      "c43d\n",
      "c1gz\n",
      "c1gx\n",
      "c43u\n",
      "c44p\n",
      "c43d\n",
      "c41t\n",
      "c433\n",
      "c44j\n",
      "c1q0\n",
      "c41x\n",
      "c436\n",
      "c438\n",
      "c1pc\n",
      "c1gx\n",
      "c43f\n",
      "c437\n",
      "c439\n",
      "c43d\n",
      "c459\n",
      "c1nq\n",
      "c43c\n",
      "c1nk\n",
      "c1p5\n",
      "c460\n",
      "c436\n",
      "c43s\n",
      "c1gz\n",
      "c1p8\n",
      "c464\n",
      "c41v\n",
      "c3k8\n",
      "c43k\n",
      "c45b\n",
      "c41u\n",
      "c3hz\n",
      "c41u\n",
      "c304\n",
      "c1pe\n",
      "c41w\n",
      "c445\n",
      "c438\n",
      "c3k0\n",
      "c3k3\n",
      "c43b\n",
      "c41w\n",
      "c436\n",
      "c43h\n",
      "c460\n",
      "c43u\n",
      "c458\n",
      "c307\n",
      "c458\n",
      "c1pf\n",
      "c437\n",
      "c464\n",
      "c44h\n",
      "c36u\n",
      "c41g\n",
      "c44p\n",
      "c1uh\n",
      "c44h\n",
      "c1gy\n",
      "c43g\n",
      "c3k2\n",
      "c461\n",
      "c41x\n",
      "c41z\n",
      "c304\n",
      "c1nu\n",
      "c41y\n",
      "c1gy\n",
      "c41z\n",
      "c41w\n",
      "c301\n",
      "c36u\n",
      "c1sn\n",
      "c43j\n",
      "c44h\n",
      "c44w\n",
      "c43k\n",
      "c464\n",
      "c1gq\n",
      "c1gw\n",
      "c301\n",
      "c465\n",
      "c44n\n",
      "c43e\n",
      "c43f\n",
      "c439\n",
      "c1nk\n",
      "c1gz\n",
      "c439\n",
      "c1gr\n",
      "c41v\n",
      "c43s\n",
      "c43h\n",
      "c1sn\n",
      "c41y\n",
      "c1gq\n",
      "c41v\n",
      "c43f\n",
      "c4r5\n",
      "c1p4\n",
      "c1gz\n",
      "149\n",
      "74\n"
     ]
    }
   ],
   "source": [
    "four_set = set()\n",
    "for x in not_snow_all_year:\n",
    "    print(x.Geohash[0:4])\n",
    "    four_set.add(x.Geohash[0:4])\n",
    "print(len(not_snow_all_year))\n",
    "print(len(four_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c3hz%\n",
      "4\n",
      "c43g%\n",
      "5\n",
      "c43m%\n",
      "3\n",
      "c1p4%\n",
      "5\n",
      "c41z%\n",
      "4\n",
      "c36u%\n",
      "4\n",
      "c1nr%\n",
      "4\n",
      "c460%\n",
      "3\n",
      "c1p8%\n",
      "3\n",
      "c41g%\n",
      "4\n",
      "c436%\n",
      "4\n",
      "c1nt%\n",
      "4\n",
      "c433%\n",
      "4\n",
      "c41x%\n",
      "4\n",
      "c452%\n",
      "3\n",
      "c1q0%\n",
      "4\n",
      "c3k8%\n",
      "4\n",
      "c43e%\n",
      "4\n",
      "c43f%\n",
      "3\n",
      "c1uh%\n",
      "4\n",
      "c458%\n",
      "4\n",
      "c1nm%\n",
      "3\n",
      "c41y%\n",
      "3\n",
      "c43b%\n",
      "5\n",
      "c459%\n",
      "4\n",
      "c43u%\n",
      "4\n",
      "c1pf%\n",
      "3\n",
      "c3k3%\n",
      "4\n",
      "c44n%\n",
      "4\n",
      "c1gz%\n",
      "4\n",
      "c1p5%\n",
      "3\n",
      "c44h%\n",
      "5\n",
      "c1pc%\n",
      "4\n",
      "c1nu%\n",
      "4\n",
      "c432%\n",
      "5\n",
      "c44j%\n",
      "4\n",
      "c1nk%\n",
      "5\n",
      "c439%\n",
      "4\n",
      "c304%\n",
      "5\n",
      "c1gn%\n",
      "4\n",
      "c43s%\n",
      "4\n",
      "c3k0%\n",
      "5\n",
      "c43h%\n",
      "4\n",
      "c3k2%\n",
      "3\n",
      "c44w%\n",
      "3\n",
      "c43k%\n",
      "4\n",
      "c41w%\n",
      "5\n",
      "c461%\n",
      "4\n",
      "c43c%\n",
      "4\n",
      "c1gx%\n",
      "3\n",
      "c43d%\n",
      "4\n",
      "c1gr%\n",
      "5\n",
      "c44p%\n",
      "4\n",
      "c1nq%\n",
      "4\n",
      "c41v%\n",
      "5\n",
      "c1gq%\n",
      "4\n",
      "c1gw%\n",
      "4\n",
      "c41u%\n",
      "4\n",
      "c438%\n",
      "3\n",
      "c1gy%\n",
      "4\n",
      "c1sn%\n",
      "4\n",
      "c43j%\n",
      "2\n",
      "c41t%\n",
      "4\n",
      "c445%\n",
      "4\n",
      "c1q2%\n",
      "5\n",
      "c4r5%\n",
      "4\n",
      "c46s%\n",
      "4\n",
      "c45b%\n",
      "4\n",
      "c301%\n",
      "4\n",
      "c464%\n",
      "5\n",
      "c465%\n",
      "4\n",
      "c1pe%\n",
      "4\n",
      "c307%\n",
      "4\n",
      "c437%\n",
      "4\n",
      "74\n",
      "Complete\n"
     ]
    }
   ],
   "source": [
    "# df.createOrReplaceTempView(\"TEMP_DF\")\n",
    "groupList = list()\n",
    "for y in four_set:\n",
    "        param = y + '%'\n",
    "        group = spark.sql(\"SELECT distinct Geohash,avg(snow_cover_surface) as avgSnow FROM TEMP_DF WHERE Geohash like '%s' group by Geohash\" %param ).collect()\n",
    "        print(param ) \n",
    "        print(len(group))\n",
    "        groupList.append(group)\n",
    "        \n",
    "print(len(groupList))\n",
    "print('Complete')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(Geohash='c1uhvnv5p8pb', avgSnow=7.283950617283951), Row(Geohash='c1uh66r1r02p', avgSnow=9.74074074074074), Row(Geohash='c1uhcd9k1n2p', avgSnow=100.0), Row(Geohash='c1uhqne01400', avgSnow=2.9135802469135803)]\n",
      "[Row(Geohash='c3k0fgxqugeb', avgSnow=10.790123456790123), Row(Geohash='c3k0yw8m6us0', avgSnow=100.0), Row(Geohash='c3k0rj7e0zs0', avgSnow=21.506172839506174), Row(Geohash='c3k07d6swzkp', avgSnow=4.08641975308642), Row(Geohash='c3k00p3vw3gz', avgSnow=4.08641975308642)]\n"
     ]
    }
   ],
   "source": [
    "for x in groupList:\n",
    "    avg=0\n",
    "    for y in x:\n",
    "        avg= avg + y.avgSnow\n",
    "    avg= avg/len(x)\n",
    "    if avg<30:\n",
    "        print(x)\n",
    "#     print(avg)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c1uhcd9k1n2p is a mountain peak(lavender peak)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2. Lightning rod: Where are you most likely to be struck by lightning? Use a precision of at least 4 Geohash characters and provide the top 3 locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(Geohash='9eqepuxk7x20', avgLighting=0.3724035608308605)\n",
      "Row(Geohash='9g3h968ygj7z', avgLighting=0.36053412462908013)\n",
      "Row(Geohash='9g3y52sgeceb', avgLighting=0.3508902077151335)\n",
      "Row(Geohash='9g3ug8ckk4hp', avgLighting=0.3486646884272997)\n",
      "Row(Geohash='9g3m79nrf3zb', avgLighting=0.3479228486646884)\n",
      "Row(Geohash='9g3mq3f7y6eb', avgLighting=0.3479228486646884)\n",
      "Row(Geohash='9g3v7kxpuhh0', avgLighting=0.3464391691394659)\n",
      "Row(Geohash='d7mkkfvu34up', avgLighting=0.336053412462908)\n",
      "Row(Geohash='9g2vntcg8c2p', avgLighting=0.3353115727002967)\n",
      "Row(Geohash='9g35cqggdn5z', avgLighting=0.33456973293768544)\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "df_entire.createOrReplaceTempView(\"TEMP_DF\")\n",
    "started_at = datetime.now()\n",
    "lightning = spark.sql(\"SELECT  Geohash,avg(lightning_surface) as avgLighting FROM TEMP_DF group by Geohash order by avgLighting DESC Limit 10 \").collect()\n",
    "\n",
    "for x in lightning:\n",
    "    print(x)\n",
    "print(\"Finished. it's been \" + str((datetime.now()-started_at).seconds) + \" seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I calculate the average lightning chances. The top 3 location is 9eqe, 9g3h, 9g3y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3. Drying out: Choose a region in North America (defined by one or more Geohashes) and determine when its driest month is. This should include a histogram with data from each month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "dfList = list()\n",
    "for x in range(1,13):\n",
    "    if x<10:\n",
    "        x = '0'+str(x)\n",
    "    path = \"hdfs://orion11:40910/datasets/nam_2015%s.tdv.gz\" %x\n",
    "#     print(path)\n",
    "    df_1 = spark.read.format('csv').option('sep', '\\t').schema(schema).load(path)\n",
    "    dfList.append(df_1)\n",
    "print(len(dfList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(subGeo='9q9n', avgRain=0.0)\n",
      "Row(subGeo='9q9n', avgRain=0.0603448275862069)\n",
      "Row(subGeo='9q9n', avgRain=0.0)\n",
      "Row(subGeo='9q9n', avgRain=0.0777027027027027)\n",
      "Row(subGeo='9q9n', avgRain=0.020161290322580645)\n",
      "Row(subGeo='9q9n', avgRain=0.0022935779816513763)\n",
      "Row(subGeo='9q9n', avgRain=0.01639344262295082)\n",
      "Row(subGeo='9q9n', avgRain=0.008333333333333333)\n",
      "Row(subGeo='9q9n', avgRain=0.009259259259259259)\n",
      "Row(subGeo='9q9n', avgRain=0.0021551724137931034)\n",
      "Row(subGeo='9q9n', avgRain=0.0546218487394958)\n",
      "Row(subGeo='9q9n', avgRain=0.0872093023255814)\n"
     ]
    }
   ],
   "source": [
    "#9q9n categorical_rain_yes1_no0_surface\n",
    "from pyspark.sql.functions import substring\n",
    "started_at = datetime.now()\n",
    "for df_1 in dfList:  \n",
    "    df_1.createOrReplaceTempView(\"TEMP_DF\")\n",
    "    dry_place = spark.sql(\"SELECT  substring(Geohash, 1, 4) as subGeo, AVG(categorical_rain_yes1_no0_surface) as avgRain FROM TEMP_DF group by subGeo having subGeo = '9q9n'\").collect()\n",
    "    for x in dry_place:\n",
    "        print(x)\n",
    "print(\"Finished. it's been \" + str((datetime.now()-started_at).seconds) + \" seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the avgerage rain chances for Hayward. January, March do not rain at all. [Bay Area](bayArea.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q5. Escaping the fog: After becoming rich from your startup, you are looking for the perfect location to build your Bay Area mansion with unobstructed views. Find the locations that are the least foggy and show them on a map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(Geohash='9q9jnpfx5rxb', avgVisibility=23811.522961255705)\n",
      "Row(Geohash='9q9jv7p453zz', avgVisibility=23703.065985376827)\n",
      "Row(Geohash='9q9m35ffeg7z', avgVisibility=23698.021476396112)\n",
      "Row(Geohash='9q9kc8wprss0', avgVisibility=23675.469547196735)\n",
      "Row(Geohash='9q9jdrny12rz', avgVisibility=23616.419106960297)\n",
      "Row(Geohash='9q8zuzx3hzeb', avgVisibility=23600.692098283627)\n",
      "Row(Geohash='9q9k4m9efqh0', avgVisibility=23497.428004600173)\n",
      "Row(Geohash='9q9hp3c2xy5b', avgVisibility=23449.653520560052)\n",
      "Row(Geohash='9q9hwsyh477z', avgVisibility=23436.152038374712)\n",
      "Row(Geohash='9q9mbqzmqxrz', avgVisibility=23435.8553033342)\n",
      "Row(Geohash='9q9pesk2gbs0', avgVisibility=23400.098629552464)\n",
      "Row(Geohash='9q9j5h47k07z', avgVisibility=23329.623851402575)\n",
      "Row(Geohash='9q9py8hurvup', avgVisibility=23277.991815833026)\n",
      "Row(Geohash='9q9nky5sd52p', avgVisibility=23164.93542607814)\n",
      "Row(Geohash='9q9nxfh005kp', avgVisibility=23088.525926889935)\n",
      "Row(Geohash='9q9p887u91up', avgVisibility=22978.882014243467)\n",
      "Row(Geohash='9q8zms7pxw7z', avgVisibility=22889.71287378147)\n",
      "Row(Geohash='9q8zcgwg9y7z', avgVisibility=22882.88794112842)\n",
      "Row(Geohash='9q9n3f70mpeb', avgVisibility=22881.849376101167)\n",
      "Row(Geohash='9q9ppm0d2vgz', avgVisibility=22866.27073556238)\n",
      "Row(Geohash='9q9mkpejm7rz', avgVisibility=22846.537790437127)\n",
      "Row(Geohash='9q8vx7q925s0', avgVisibility=22698.1698534142)\n",
      "Row(Geohash='9q9mx57x2ps0', avgVisibility=22678.882005442498)\n",
      "Row(Geohash='9q9ph1pyzf5b', avgVisibility=22557.517036797384)\n",
      "Row(Geohash='9q9kusw6n7kp', avgVisibility=22483.184677927715)\n",
      "Row(Geohash='9q9km33ww4h0', avgVisibility=22449.653523746514)\n",
      "Row(Geohash='9q9j004nwjgz', avgVisibility=22337.042257518373)\n",
      "Row(Geohash='9q8yny6gc2pb', avgVisibility=22245.202497623795)\n",
      "Row(Geohash='9q9mp8qsnqrz', avgVisibility=22244.163904211287)\n",
      "Row(Geohash='9q8yy1rjchrz', avgVisibility=22123.540761658984)\n",
      "Row(Geohash='9q9he9jbjws0', avgVisibility=21851.87905278192)\n",
      "Row(Geohash='9q8z68ddprbp', avgVisibility=21642.383494119615)\n",
      "Row(Geohash='9q9ncjr6xueb', avgVisibility=21488.377561846544)\n",
      "Row(Geohash='9q8yejw8eb7z', avgVisibility=21146.982903568256)\n",
      "Row(Geohash='9q8v37qn62h0', avgVisibility=20924.28260454297)\n",
      "Row(Geohash='9q9h2tjgdy7z', avgVisibility=20923.095670479343)\n",
      "Row(Geohash='9q8vkrqk0g2p', avgVisibility=20726.95322760064)\n",
      "Row(Geohash='9q8vbyd0t880', avgVisibility=20611.226223915903)\n",
      "Row(Geohash='9q8y81w4x87z', avgVisibility=20583.48141743946)\n",
      "Row(Geohash='9q8y5f6qqy00', avgVisibility=20559.594181275865)\n",
      "40\n",
      "Finished. it's been 317 seconds\n"
     ]
    }
   ],
   "source": [
    "started_at = datetime.now()\n",
    "df_entire.createOrReplaceTempView(\"TEMP_DF\")\n",
    "visibility = spark.sql(\"SELECT  Geohash,avg(visibility_surface) as avgVisibility FROM TEMP_DF WHERE Geohash like '9q9p%' or Geohash like '9q9n%' or Geohash like '9q9j%' or Geohash like '9q9m%' or Geohash like '9q9k%' or Geohash like '9q9h%' or Geohash like '9q8v%' or Geohash like '9q8y%' or Geohash like '9q8z%' group by Geohash order by avgVisibility DESC \").collect()\n",
    "for x in visibility:\n",
    "    print(x)\n",
    "print(len(visibility))\n",
    "print(\"Finished. it's been \" + str((datetime.now()-started_at).seconds) + \" seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The list above has the best visibility in the bay area. \n",
    "Such as 9q9m35 is a good place to build a mansion. The address is S Grimmer Blvd:Fremont Blvd, Fremont, CA\n",
    "[Grimmer](Grimmer.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Option 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_option = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").csv('hdfs://orion11:40910/option_dataset/Police_Department_Incident_Reports__Historical_2003_to_May_2018.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "194694\n"
     ]
    }
   ],
   "source": [
    "notComplete = df_option.filter(df_option.Category == \"ASSAULT\").count()\n",
    "print(notComplete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+--------+---------+----------+-----+----------+--------------+--------------------+----------------+---------------+--------------------+--------------+\n",
      "|IncidntNum|Category|Descript|DayOfWeek|      Date| Time|PdDistrict|    Resolution|             Address|               X|              Y|            Location|          PdId|\n",
      "+----------+--------+--------+---------+----------+-----+----------+--------------+--------------------+----------------+---------------+--------------------+--------------+\n",
      "| 140646669| ASSAULT| BATTERY|   Monday|08/04/2014|08:56|   MISSION|ARREST, BOOKED|500 Block of SOUT...|-122.41747701285|37.764357751686|(37.764357751686,...|14064666904134|\n",
      "+----------+--------+--------+---------+----------+-----+----------+--------------+--------------------+----------------+---------------+--------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_option.createOrReplaceTempView(\"POLICE_DF\")\n",
    "spark.sql(\"select * from POLICE_DF limit 1\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1. 2003-2017, 1- 24. batman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(hour='00', timecount=113096)\n",
      "Row(hour='01', timecount=65182)\n",
      "Row(hour='02', timecount=54550)\n",
      "Row(hour='03', timecount=35596)\n",
      "Row(hour='04', timecount=25285)\n",
      "Row(hour='05', timecount=22413)\n",
      "Row(hour='06', timecount=33494)\n",
      "Row(hour='07', timecount=55551)\n",
      "Row(hour='08', timecount=82459)\n",
      "Row(hour='09', timecount=89303)\n",
      "Row(hour='10', timecount=95469)\n",
      "Row(hour='11', timecount=97620)\n",
      "Row(hour='12', timecount=132631)\n",
      "Row(hour='13', timecount=108540)\n",
      "Row(hour='14', timecount=112078)\n",
      "Row(hour='15', timecount=120190)\n",
      "Row(hour='16', timecount=125548)\n",
      "Row(hour='17', timecount=135481)\n",
      "Row(hour='18', timecount=140918)\n",
      "Row(hour='19', timecount=126404)\n",
      "Row(hour='20', timecount=115010)\n",
      "Row(hour='21', timecount=109559)\n",
      "Row(hour='22', timecount=113915)\n",
      "Row(hour='23', timecount=104732)\n",
      "24\n",
      "Finished. it's been 1 seconds\n"
     ]
    }
   ],
   "source": [
    "started_at = datetime.now()\n",
    "df_option.createOrReplaceTempView(\"POLICE_DF\")\n",
    "batman = spark.sql(\"SELECT substring(time, 1, 2 ) hour, count(*) as timecount FROM POLICE_DF  group by hour order by hour\").collect()\n",
    "for x in batman:\n",
    "    print(x)\n",
    "print(len(batman))\n",
    "print(\"Finished. it's been \" + str((datetime.now()-started_at).seconds) + \" seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2. Break in car (VANDALISM) when and where"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(PdDistrict='TENDERLOIN', c=4214)\n",
      "Row(PdDistrict='PARK', c=6554)\n",
      "Row(PdDistrict='RICHMOND', c=7837)\n",
      "Row(PdDistrict='TARAVAL', c=11607)\n",
      "Row(PdDistrict='CENTRAL', c=12586)\n",
      "Row(PdDistrict='INGLESIDE', c=13126)\n",
      "Row(PdDistrict='MISSION', c=14050)\n",
      "Row(PdDistrict='BAYVIEW', c=14103)\n",
      "Row(PdDistrict='NORTHERN', c=14533)\n",
      "Row(PdDistrict='SOUTHERN', c=17449)\n",
      "10\n",
      "There are totally 116059vandalism\n",
      "TENDERLOIN\n",
      "0.03630911863793415\n",
      "PARK\n",
      "0.05647127753987196\n",
      "RICHMOND\n",
      "0.06752599970704555\n",
      "TARAVAL\n",
      "0.10000947793794536\n",
      "CENTRAL\n",
      "0.10844484270931164\n",
      "INGLESIDE\n",
      "0.11309764860975884\n",
      "MISSION\n",
      "0.12105911648385735\n",
      "BAYVIEW\n",
      "0.12151578076667902\n",
      "NORTHERN\n",
      "0.12522079287259066\n",
      "SOUTHERN\n",
      "0.15034594473500548\n",
      "Finished. it's been 1 seconds\n"
     ]
    }
   ],
   "source": [
    "started_at = datetime.now()\n",
    "vandalism = spark.sql(\"SELECT PdDistrict, count(*) c FROM POLICE_DF WHERE Category ='VANDALISM' group by PdDistrict order by c\").collect()\n",
    "count = 0\n",
    "for x in vandalism:\n",
    "    count = count + x.c\n",
    "    print(x)\n",
    "print(len(vandalism))\n",
    "print('There are totally ' + str(count) + ' vandalisms')\n",
    "for y in vandalism:\n",
    "    print(y.PdDistrict)\n",
    "    print(y.c/count)\n",
    "print(\"Finished. it's been \" + str((datetime.now()-started_at).seconds) + \" seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(hour='00', timecount=6534)\n",
      "Row(hour='01', timecount=4562)\n",
      "Row(hour='02', timecount=4303)\n",
      "Row(hour='03', timecount=2898)\n",
      "Row(hour='04', timecount=1942)\n",
      "Row(hour='05', timecount=1611)\n",
      "Row(hour='06', timecount=1856)\n",
      "Row(hour='07', timecount=2518)\n",
      "Row(hour='08', timecount=3684)\n",
      "Row(hour='09', timecount=3474)\n",
      "Row(hour='10', timecount=3587)\n",
      "Row(hour='11', timecount=3395)\n",
      "Row(hour='12', timecount=4640)\n",
      "Row(hour='13', timecount=3600)\n",
      "Row(hour='14', timecount=4077)\n",
      "Row(hour='15', timecount=4801)\n",
      "Row(hour='16', timecount=5316)\n",
      "Row(hour='17', timecount=7077)\n",
      "Row(hour='18', timecount=8402)\n",
      "Row(hour='19', timecount=7619)\n",
      "Row(hour='20', timecount=7549)\n",
      "Row(hour='21', timecount=7683)\n",
      "Row(hour='22', timecount=7829)\n",
      "Row(hour='23', timecount=7102)\n",
      "24\n",
      "Finished. it's been 1 seconds\n"
     ]
    }
   ],
   "source": [
    "started_at = datetime.now()\n",
    "vandalism = spark.sql(\"SELECT substring(time, 1, 2 ) hour, count(*) as timecount FROM POLICE_DF WHERE Category ='VANDALISM' group by hour order by hour\").collect()\n",
    "for x in vandalism:\n",
    "    print(x)\n",
    "print(len(vandalism))\n",
    "\n",
    "print(\"Finished. it's been \" + str((datetime.now()-started_at).seconds) + \" seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "看上去多发生在半夜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(dates='01', timecount=9982)\n",
      "Row(dates='02', timecount=8958)\n",
      "Row(dates='03', timecount=10407)\n",
      "Row(dates='04', timecount=10173)\n",
      "Row(dates='05', timecount=9752)\n",
      "Row(dates='06', timecount=9336)\n",
      "Row(dates='07', timecount=10065)\n",
      "Row(dates='08', timecount=9718)\n",
      "Row(dates='09', timecount=9501)\n",
      "Row(dates='10', timecount=10112)\n",
      "Row(dates='11', timecount=9002)\n",
      "Row(dates='12', timecount=9053)\n",
      "12\n",
      "Finished. it's been 1 seconds\n"
     ]
    }
   ],
   "source": [
    "started_at = datetime.now()\n",
    "vandalism = spark.sql(\"SELECT substring(date, 1, 2 ) dates, count(*) as timecount FROM POLICE_DF WHERE Category ='VANDALISM' group by dates order by dates\").collect()\n",
    "for x in vandalism:\n",
    "    print(x)\n",
    "print(len(vandalism))\n",
    "\n",
    "print(\"Finished. it's been \" + str((datetime.now()-started_at).seconds) + \" seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "月份上看起来挺平均的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(year='2003', timecount=6448)\n",
      "Row(year='2004', timecount=6496)\n",
      "Row(year='2005', timecount=7013)\n",
      "Row(year='2006', timecount=7688)\n",
      "Row(year='2007', timecount=7566)\n",
      "Row(year='2008', timecount=7342)\n",
      "Row(year='2009', timecount=7604)\n",
      "Row(year='2010', timecount=7934)\n",
      "Row(year='2011', timecount=7243)\n",
      "Row(year='2012', timecount=7808)\n",
      "Row(year='2013', timecount=6921)\n",
      "Row(year='2014', timecount=7165)\n",
      "Row(year='2015', timecount=7675)\n",
      "Row(year='2016', timecount=8595)\n",
      "Row(year='2017', timecount=9765)\n",
      "Row(year='2018', timecount=2796)\n",
      "16\n",
      "Finished. it's been 1 seconds\n"
     ]
    }
   ],
   "source": [
    "started_at = datetime.now()\n",
    "vandalism = spark.sql(\"SELECT substring(date, 7, 4 ) year, count(*) as timecount FROM POLICE_DF WHERE Category ='VANDALISM' group by year order by year\").collect()\n",
    "# vandalism = spark.sql(\"SELECT substring(time, 1, 2 ) hour, date, PdDistrict FROM POLICE_DF WHERE Category ='VANDALISM' order by hour\").collect()\n",
    "for x in vandalism:\n",
    "    print(x)\n",
    "print(len(vandalism))\n",
    "\n",
    "print(\"Finished. it's been \" + str((datetime.now()-started_at).seconds) + \" seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "年份上也很平均"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3. \n",
    "### a. Top 10 Category and resolution rate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(Category='LARCENY/THEFT', totalCount=480448, resolveCount=42521)\n",
      "Row(Category='OTHER OFFENSES', totalCount=309358, resolveCount=221514)\n",
      "Row(Category='NON-CRIMINAL', totalCount=238323, resolveCount=53465)\n",
      "Row(Category='ASSAULT', totalCount=194694, resolveCount=80947)\n",
      "Row(Category='VEHICLE THEFT', totalCount=126602, resolveCount=10622)\n",
      "Row(Category='DRUG/NARCOTIC', totalCount=119628, resolveCount=109357)\n",
      "Row(Category='VANDALISM', totalCount=116059, resolveCount=14169)\n",
      "Row(Category='WARRANTS', totalCount=101379, resolveCount=95897)\n",
      "Row(Category='BURGLARY', totalCount=91543, resolveCount=14890)\n",
      "Row(Category='SUSPICIOUS OCC', totalCount=80444, resolveCount=9458)\n",
      "10\n",
      "Finished. it's been 2 seconds\n"
     ]
    }
   ],
   "source": [
    "started_at = datetime.now()\n",
    "category = spark.sql(\"SELECT a.Category, b.totalCount, a.resolveCount from (SELECT distinct Category, count(*) as resolveCount FROM POLICE_DF WHERE not Resolution ='NONE' group by Category) a inner join (SELECT distinct Category, count(*) as totalCount FROM POLICE_DF group by Category order by totalCount DESC limit 10) b on a.Category = b.Category order by b.totalCount DESC\").collect()\n",
    "for x in category:\n",
    "    print(x)\n",
    "print(len(category))\n",
    "\n",
    "print(\"Finished. it's been \" + str((datetime.now()-started_at).seconds) + \" seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top 10 category, and their resolution rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(Category='OTHER OFFENSES', count=221514)\n",
      "Row(Category='DRUG/NARCOTIC', count=109357)\n",
      "Row(Category='WARRANTS', count=95897)\n",
      "Row(Category='ASSAULT', count=80947)\n",
      "Row(Category='NON-CRIMINAL', count=53465)\n",
      "Row(Category='LARCENY/THEFT', count=42521)\n",
      "Row(Category='MISSING PERSON', count=34672)\n",
      "Row(Category='WEAPON LAWS', count=16164)\n",
      "Row(Category='PROSTITUTION', count=15851)\n",
      "Row(Category='BURGLARY', count=14890)\n",
      "10\n",
      "Finished. it's been 1 seconds\n"
     ]
    }
   ],
   "source": [
    "started_at = datetime.now()\n",
    "category = spark.sql(\"SELECT distinct Category, count(*) as count FROM POLICE_DF WHERE not Resolution ='NONE' group by Category order by count DESC limit 10\").collect()\n",
    "for x in category:\n",
    "    print(x)\n",
    "print(len(category))\n",
    "\n",
    "print(\"Finished. it's been \" + str((datetime.now()-started_at).seconds) + \" seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Relationship between criminal increase rate and solved criminal rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(Category='LARCENY/THEFT', totalCount=480448)\n",
      "Row(Category='OTHER OFFENSES', totalCount=309358)\n",
      "Row(Category='NON-CRIMINAL', totalCount=238323)\n",
      "Row(Category='ASSAULT', totalCount=194694)\n",
      "Row(Category='VEHICLE THEFT', totalCount=126602)\n",
      "Row(Category='DRUG/NARCOTIC', totalCount=119628)\n",
      "Row(Category='VANDALISM', totalCount=116059)\n",
      "Row(Category='WARRANTS', totalCount=101379)\n",
      "Row(Category='BURGLARY', totalCount=91543)\n",
      "Row(Category='SUSPICIOUS OCC', totalCount=80444)\n",
      "10\n",
      "Finished. it's been 1 seconds\n"
     ]
    }
   ],
   "source": [
    "started_at = datetime.now()\n",
    "category = spark.sql(\"SELECT distinct Category, count(*) as totalCount FROM POLICE_DF group by Category order by totalCount DESC limit 10\").collect()\n",
    "for x in category:\n",
    "    print(x)\n",
    "print(len(category))\n",
    "\n",
    "print(\"Finished. it's been \" + str((datetime.now()-started_at).seconds) + \" seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LARCENY/THEFT\n",
      "2003  total = 26393  resolution = 3844\n",
      "2004  total = 24505  resolution = 3389\n",
      "2005  total = 25319  resolution = 3346\n",
      "2006  total = 27352  resolution = 3345\n",
      "2007  total = 25770  resolution = 2879\n",
      "2008  total = 25807  resolution = 3099\n",
      "2009  total = 25585  resolution = 3014\n",
      "2010  total = 24446  resolution = 2973\n",
      "2011  total = 25905  resolution = 2729\n",
      "2012  total = 30976  resolution = 2358\n",
      "2013  total = 36412  resolution = 2777\n",
      "2014  total = 38003  resolution = 2367\n",
      "2015  total = 42068  resolution = 2135\n",
      "2016  total = 40449  resolution = 1837\n",
      "2017  total = 47826  resolution = 1882\n",
      "2018  total = 13632  resolution = 547\n",
      "OTHER OFFENSES\n",
      "2003  total = 21232  resolution = 16295\n",
      "2004  total = 20710  resolution = 15511\n",
      "2005  total = 17834  resolution = 12901\n",
      "2006  total = 18306  resolution = 12894\n",
      "2007  total = 19763  resolution = 14910\n",
      "2008  total = 23457  resolution = 18447\n",
      "2009  total = 24693  resolution = 19452\n",
      "2010  total = 20990  resolution = 15295\n",
      "2011  total = 19552  resolution = 14167\n",
      "2012  total = 18646  resolution = 12436\n",
      "2013  total = 19480  resolution = 13917\n",
      "2014  total = 20740  resolution = 14548\n",
      "2015  total = 20382  resolution = 13409\n",
      "2016  total = 19689  resolution = 12634\n",
      "2017  total = 18316  resolution = 11316\n",
      "2018  total = 5568  resolution = 3382\n",
      "NON-CRIMINAL\n",
      "2003  total = 13149  resolution = 1744\n",
      "2004  total = 13778  resolution = 2074\n",
      "2005  total = 14055  resolution = 2326\n",
      "2006  total = 13368  resolution = 2846\n",
      "2007  total = 12677  resolution = 2955\n",
      "2008  total = 12303  resolution = 2913\n",
      "2009  total = 12395  resolution = 2790\n",
      "2010  total = 13877  resolution = 3738\n",
      "2011  total = 15586  resolution = 4610\n",
      "2012  total = 16936  resolution = 4928\n",
      "2013  total = 21084  resolution = 8795\n",
      "2014  total = 19404  resolution = 6516\n",
      "2015  total = 19177  resolution = 2978\n",
      "2016  total = 17919  resolution = 2029\n",
      "2017  total = 17368  resolution = 1667\n",
      "2018  total = 5247  resolution = 556\n",
      "ASSAULT\n",
      "2003  total = 13461  resolution = 5551\n",
      "2004  total = 12899  resolution = 5034\n",
      "2005  total = 11601  resolution = 4142\n",
      "2006  total = 12449  resolution = 4606\n",
      "2007  total = 12518  resolution = 4846\n",
      "2008  total = 12681  resolution = 5141\n",
      "2009  total = 12284  resolution = 5529\n",
      "2010  total = 12387  resolution = 5660\n",
      "2011  total = 12279  resolution = 5605\n",
      "2012  total = 12181  resolution = 4997\n",
      "2013  total = 12580  resolution = 6419\n",
      "2014  total = 12402  resolution = 5253\n",
      "2015  total = 13115  resolution = 5316\n",
      "2016  total = 13603  resolution = 5647\n",
      "2017  total = 13655  resolution = 5388\n",
      "2018  total = 4599  resolution = 1813\n",
      "VEHICLE THEFT\n",
      "2003  total = 15325  resolution = 1339\n",
      "2004  total = 17884  resolution = 1454\n",
      "2005  total = 18194  resolution = 1561\n",
      "2006  total = 7291  resolution = 673\n",
      "2007  total = 6460  resolution = 537\n",
      "2008  total = 6053  resolution = 508\n",
      "2009  total = 5183  resolution = 431\n",
      "2010  total = 4346  resolution = 352\n",
      "2011  total = 4762  resolution = 398\n",
      "2012  total = 6183  resolution = 408\n",
      "2013  total = 6241  resolution = 521\n",
      "2014  total = 7108  resolution = 579\n",
      "2015  total = 7943  resolution = 575\n",
      "2016  total = 6422  resolution = 575\n",
      "2017  total = 5732  resolution = 563\n",
      "2018  total = 1475  resolution = 148\n",
      "DRUG/NARCOTIC\n",
      "2003  total = 9917  resolution = 9334\n",
      "2004  total = 9897  resolution = 9040\n",
      "2005  total = 8533  resolution = 7585\n",
      "2006  total = 9069  resolution = 8157\n",
      "2007  total = 10560  resolution = 9900\n",
      "2008  total = 11648  resolution = 10844\n",
      "2009  total = 11950  resolution = 11275\n",
      "2010  total = 9205  resolution = 8218\n",
      "2011  total = 6935  resolution = 6240\n",
      "2012  total = 6444  resolution = 5539\n",
      "2013  total = 6775  resolution = 6204\n",
      "2014  total = 5408  resolution = 4969\n",
      "2015  total = 4251  resolution = 3853\n",
      "2016  total = 4246  resolution = 3865\n",
      "2017  total = 3308  resolution = 3022\n",
      "2018  total = 1482  resolution = 1312\n",
      "VANDALISM\n",
      "2003  total = 6448  resolution = 765\n",
      "2004  total = 6496  resolution = 748\n",
      "2005  total = 7013  resolution = 644\n",
      "2006  total = 7688  resolution = 701\n",
      "2007  total = 7566  resolution = 893\n",
      "2008  total = 7342  resolution = 932\n",
      "2009  total = 7604  resolution = 961\n",
      "2010  total = 7934  resolution = 1152\n",
      "2011  total = 7243  resolution = 1045\n",
      "2012  total = 7808  resolution = 903\n",
      "2013  total = 6921  resolution = 1035\n",
      "2014  total = 7165  resolution = 978\n",
      "2015  total = 7675  resolution = 1003\n",
      "2016  total = 8595  resolution = 1022\n",
      "2017  total = 9765  resolution = 1054\n",
      "2018  total = 2796  resolution = 333\n",
      "WARRANTS\n",
      "2003  total = 9079  resolution = 8902\n",
      "2004  total = 8114  resolution = 7870\n",
      "2005  total = 6708  resolution = 6165\n",
      "2006  total = 6498  resolution = 6047\n",
      "2007  total = 7105  resolution = 6903\n",
      "2008  total = 5798  resolution = 5602\n",
      "2009  total = 5764  resolution = 5619\n",
      "2010  total = 6187  resolution = 5786\n",
      "2011  total = 6311  resolution = 5888\n",
      "2012  total = 6300  resolution = 5670\n",
      "2013  total = 7362  resolution = 6986\n",
      "2014  total = 6726  resolution = 6339\n",
      "2015  total = 6815  resolution = 6287\n",
      "2016  total = 5974  resolution = 5615\n",
      "2017  total = 5020  resolution = 4742\n",
      "2018  total = 1618  resolution = 1476\n",
      "BURGLARY\n",
      "2003  total = 6047  resolution = 840\n",
      "2004  total = 6753  resolution = 857\n",
      "2005  total = 7071  resolution = 895\n",
      "2006  total = 7004  resolution = 833\n",
      "2007  total = 5454  resolution = 778\n",
      "2008  total = 5679  resolution = 775\n",
      "2009  total = 5379  resolution = 844\n",
      "2010  total = 4966  resolution = 931\n",
      "2011  total = 4987  resolution = 1108\n",
      "2012  total = 6243  resolution = 1078\n",
      "2013  total = 6195  resolution = 1319\n",
      "2014  total = 6066  resolution = 1219\n",
      "2015  total = 5931  resolution = 1016\n",
      "2016  total = 5813  resolution = 1049\n",
      "2017  total = 5857  resolution = 994\n",
      "2018  total = 2098  resolution = 354\n",
      "SUSPICIOUS OCC\n",
      "2003  total = 4196  resolution = 370\n",
      "2004  total = 4489  resolution = 360\n",
      "2005  total = 4693  resolution = 550\n",
      "2006  total = 4775  resolution = 482\n",
      "2007  total = 4800  resolution = 518\n",
      "2008  total = 4751  resolution = 469\n",
      "2009  total = 4627  resolution = 512\n",
      "2010  total = 6004  resolution = 907\n",
      "2011  total = 6207  resolution = 868\n",
      "2012  total = 5860  resolution = 778\n",
      "2013  total = 5677  resolution = 989\n",
      "2014  total = 5230  resolution = 735\n",
      "2015  total = 5500  resolution = 731\n",
      "2016  total = 5802  resolution = 576\n",
      "2017  total = 6119  resolution = 476\n",
      "2018  total = 1714  resolution = 137\n",
      "Finished. it's been 22 seconds\n"
     ]
    }
   ],
   "source": [
    "started_at = datetime.now()\n",
    "lists = list()\n",
    "for x in category:\n",
    "    types = x.Category\n",
    "    yearResolution = spark.sql(\"SELECT a.year, a.totalCount, b.resolveCount from (SELECT substring(date, 7, 4 ) year, count(*) as totalCount FROM POLICE_DF WHERE Category = '%s' group by year) a inner join (SELECT substring(date, 7, 4 ) year, count(*) as resolveCount FROM POLICE_DF WHERE (not Resolution ='NONE') and Category = '%s' group by year )b on a.year = b.year order by a.year \" %(types,types)).collect()\n",
    "    print(types)\n",
    "    lists.append(yearResolution)\n",
    "    for x in yearResolution:\n",
    "        print(str(x.year) +'  total = ' + str(x.totalCount) + '  resolution = '+str(x.resolveCount))\n",
    "# for x in category:\n",
    "#     print(x)\n",
    "# print(len(category))\n",
    "\n",
    "print(\"Finished. it's been \" + str((datetime.now()-started_at).seconds) + \" seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2003  0.14564467851324214\n",
      "2004  0.13829830646806773\n",
      "2005  0.1321537185512856\n",
      "2006  0.12229453056449255\n",
      "2007  0.11171905316259216\n",
      "2008  0.1200836982214128\n",
      "2009  0.11780340042993942\n",
      "2010  0.12161498813711855\n",
      "2011  0.10534645821270025\n",
      "2012  0.07612345041322315\n",
      "2013  0.07626606613204438\n",
      "2014  0.06228455648238297\n",
      "2015  0.05075116478083103\n",
      "2016  0.045415214220376275\n",
      "2017  0.03935098063814661\n",
      "2018  0.04012617370892019\n",
      "*****************\n",
      "2003  0.7674736247174077\n",
      "2004  0.7489618541767262\n",
      "2005  0.7233935179993272\n",
      "2006  0.7043592264831203\n",
      "2007  0.7544401153671001\n",
      "2008  0.7864177004732062\n",
      "2009  0.7877536143846434\n",
      "2010  0.7286803239637922\n",
      "2011  0.7245806055646481\n",
      "2012  0.6669526976295184\n",
      "2013  0.7144250513347022\n",
      "2014  0.7014464802314369\n",
      "2015  0.6578844078108135\n",
      "2016  0.6416780943674133\n",
      "2017  0.6178204848220136\n",
      "2018  0.6073994252873564\n",
      "*****************\n",
      "2003  0.13263366035439958\n",
      "2004  0.1505298301640296\n",
      "2005  0.16549270722162931\n",
      "2006  0.21289646918013166\n",
      "2007  0.23309931371775658\n",
      "2008  0.23677151914167277\n",
      "2009  0.22509076240419523\n",
      "2010  0.26936657779058876\n",
      "2011  0.2957782625433081\n",
      "2012  0.29097779877184693\n",
      "2013  0.41714095996964523\n",
      "2014  0.3358070500927644\n",
      "2015  0.15529019137508474\n",
      "2016  0.11323176516546682\n",
      "2017  0.09598111469368954\n",
      "2018  0.10596531351248333\n",
      "*****************\n",
      "2003  0.4123764950598024\n",
      "2004  0.3902628110706256\n",
      "2005  0.3570381863632445\n",
      "2006  0.3699895573941682\n",
      "2007  0.3871225435373063\n",
      "2008  0.405409668007255\n",
      "2009  0.4500976880494953\n",
      "2010  0.4569306531040607\n",
      "2011  0.456470396612102\n",
      "2012  0.4102290452343814\n",
      "2013  0.5102543720190779\n",
      "2014  0.4235607160135462\n",
      "2015  0.40533739992375145\n",
      "2016  0.4151290156583107\n",
      "2017  0.39458073965580376\n",
      "2018  0.3942161339421613\n",
      "*****************\n",
      "2003  0.08737357259380098\n",
      "2004  0.08130172220979646\n",
      "2005  0.08579751566450478\n",
      "2006  0.09230558222466054\n",
      "2007  0.08312693498452012\n",
      "2008  0.08392532628448703\n",
      "2009  0.08315647308508586\n",
      "2010  0.08099401748734468\n",
      "2011  0.08357832843343133\n",
      "2012  0.06598738476467735\n",
      "2013  0.08348021150456658\n",
      "2014  0.08145751266178954\n",
      "2015  0.07239078433841117\n",
      "2016  0.08953597010277173\n",
      "2017  0.0982205163991626\n",
      "2018  0.10033898305084746\n",
      "*****************\n",
      "2003  0.9412120600988202\n",
      "2004  0.9134081034656967\n",
      "2005  0.8889019102308684\n",
      "2006  0.8994376447237843\n",
      "2007  0.9375\n",
      "2008  0.9309752747252747\n",
      "2009  0.9435146443514645\n",
      "2010  0.8927756653992396\n",
      "2011  0.8997837058399423\n",
      "2012  0.8595592799503414\n",
      "2013  0.915719557195572\n",
      "2014  0.9188239644970414\n",
      "2015  0.906374970595154\n",
      "2016  0.9102684879886952\n",
      "2017  0.9135429262394196\n",
      "2018  0.8852901484480432\n",
      "*****************\n",
      "2003  0.11864143920595534\n",
      "2004  0.11514778325123153\n",
      "2005  0.09182945957507486\n",
      "2006  0.09118106139438086\n",
      "2007  0.11802802008987576\n",
      "2008  0.1269408880414056\n",
      "2009  0.12638085218306155\n",
      "2010  0.14519788253087976\n",
      "2011  0.14427723319066685\n",
      "2012  0.11565061475409837\n",
      "2013  0.14954486345903772\n",
      "2014  0.13649685973482206\n",
      "2015  0.13068403908794787\n",
      "2016  0.1189063408958697\n",
      "2017  0.10793650793650794\n",
      "2018  0.11909871244635194\n",
      "*****************\n",
      "2003  0.9805044608437052\n",
      "2004  0.9699285186098102\n",
      "2005  0.919051878354204\n",
      "2006  0.9305940289319791\n",
      "2007  0.9715693173821253\n",
      "2008  0.9661952397378406\n",
      "2009  0.9748438584316447\n",
      "2010  0.9351866817520608\n",
      "2011  0.9329741720804944\n",
      "2012  0.9\n",
      "2013  0.9489269220320565\n",
      "2014  0.9424620874219447\n",
      "2015  0.9225238444607483\n",
      "2016  0.939906260462002\n",
      "2017  0.9446215139442231\n",
      "2018  0.9122373300370828\n",
      "*****************\n",
      "2003  0.13891185711923268\n",
      "2004  0.12690656004738635\n",
      "2005  0.12657332767642485\n",
      "2006  0.11893203883495146\n",
      "2007  0.14264759809314265\n",
      "2008  0.13646768797323472\n",
      "2009  0.15690648819483174\n",
      "2010  0.18747482883608538\n",
      "2011  0.22217766192099458\n",
      "2012  0.17267339420150568\n",
      "2013  0.2129136400322841\n",
      "2014  0.20095614902736564\n",
      "2015  0.17130332153093913\n",
      "2016  0.18045759504558748\n",
      "2017  0.16971145637698482\n",
      "2018  0.16873212583412775\n",
      "*****************\n",
      "2003  0.08817921830314586\n",
      "2004  0.08019603475161506\n",
      "2005  0.1171958235670147\n",
      "2006  0.10094240837696335\n",
      "2007  0.10791666666666666\n",
      "2008  0.09871605977688908\n",
      "2009  0.11065485195591096\n",
      "2010  0.1510659560293138\n",
      "2011  0.13984211374254873\n",
      "2012  0.13276450511945392\n",
      "2013  0.174211731548353\n",
      "2014  0.14053537284894838\n",
      "2015  0.13290909090909092\n",
      "2016  0.09927611168562564\n",
      "2017  0.07779048864193495\n",
      "2018  0.07992998833138856\n",
      "*****************\n"
     ]
    }
   ],
   "source": [
    "for x in lists:\n",
    "    for y in x:\n",
    "        print(str(y.year) + '  ' + str(y.resolveCount/ y.totalCount))\n",
    "    print('*****************')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
