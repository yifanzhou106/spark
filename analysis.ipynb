{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StructType(List(StructField(Timestamp,LongType,true),StructField(Geohash,StringType,true),StructField(geopotential_height_lltw,FloatType,true),StructField(water_equiv_of_accum_snow_depth_surface,FloatType,true),StructField(drag_coefficient_surface,FloatType,true),StructField(sensible_heat_net_flux_surface,FloatType,true),StructField(categorical_ice_pellets_yes1_no0_surface,FloatType,true),StructField(visibility_surface,FloatType,true),StructField(number_of_soil_layers_in_root_zone_surface,FloatType,true),StructField(categorical_freezing_rain_yes1_no0_surface,FloatType,true),StructField(pressure_reduced_to_msl_msl,FloatType,true),StructField(upward_short_wave_rad_flux_surface,FloatType,true),StructField(relative_humidity_zerodegc_isotherm,FloatType,true),StructField(categorical_snow_yes1_no0_surface,FloatType,true),StructField(u-component_of_wind_tropopause,FloatType,true),StructField(surface_wind_gust_surface,FloatType,true),StructField(total_cloud_cover_entire_atmosphere,FloatType,true),StructField(upward_long_wave_rad_flux_surface,FloatType,true),StructField(land_cover_land1_sea0_surface,FloatType,true),StructField(vegitation_type_as_in_sib_surface,FloatType,true),StructField(v-component_of_wind_pblri,FloatType,true),StructField(albedo_surface,FloatType,true),StructField(lightning_surface,FloatType,true),StructField(ice_cover_ice1_no_ice0_surface,FloatType,true),StructField(convective_inhibition_surface,FloatType,true),StructField(pressure_surface,FloatType,true),StructField(transpiration_stress-onset_soil_moisture_surface,FloatType,true),StructField(soil_porosity_surface,FloatType,true),StructField(vegetation_surface,FloatType,true),StructField(categorical_rain_yes1_no0_surface,FloatType,true),StructField(downward_long_wave_rad_flux_surface,FloatType,true),StructField(planetary_boundary_layer_height_surface,FloatType,true),StructField(soil_type_as_in_zobler_surface,FloatType,true),StructField(geopotential_height_cloud_base,FloatType,true),StructField(friction_velocity_surface,FloatType,true),StructField(maximumcomposite_radar_reflectivity_entire_atmosphere,FloatType,true),StructField(plant_canopy_surface_water_surface,FloatType,true),StructField(v-component_of_wind_maximum_wind,FloatType,true),StructField(geopotential_height_zerodegc_isotherm,FloatType,true),StructField(mean_sea_level_pressure_nam_model_reduction_msl,FloatType,true),StructField(temperature_surface,FloatType,true),StructField(snow_cover_surface,FloatType,true),StructField(geopotential_height_surface,FloatType,true),StructField(convective_available_potential_energy_surface,FloatType,true),StructField(latent_heat_net_flux_surface,FloatType,true),StructField(surface_roughness_surface,FloatType,true),StructField(pressure_maximum_wind,FloatType,true),StructField(temperature_tropopause,FloatType,true),StructField(geopotential_height_pblri,FloatType,true),StructField(pressure_tropopause,FloatType,true),StructField(snow_depth_surface,FloatType,true),StructField(v-component_of_wind_tropopause,FloatType,true),StructField(downward_short_wave_rad_flux_surface,FloatType,true),StructField(u-component_of_wind_maximum_wind,FloatType,true),StructField(wilting_point_surface,FloatType,true),StructField(precipitable_water_entire_atmosphere,FloatType,true),StructField(u-component_of_wind_pblri,FloatType,true),StructField(direct_evaporation_cease_soil_moisture_surface,FloatType,true)))\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, FloatType, LongType, StringType\n",
    "\n",
    "feats = []\n",
    "f = open('features.txt')\n",
    "for line_num, line in enumerate(f):\n",
    "    if line_num == 0:\n",
    "        # Timestamp\n",
    "        feats.append(StructField(line.strip(), LongType(), True))\n",
    "    elif line_num == 1:\n",
    "        # Geohash\n",
    "        feats.append(StructField(line.strip(), StringType(), True))\n",
    "    else:\n",
    "        # Other features\n",
    "        feats.append(StructField(line.strip(), FloatType(), True))\n",
    "    \n",
    "schema = StructType(feats)\n",
    "\n",
    "print(schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = spark.read.format('csv').option('sep', '\\t').schema(schema).load('/Volumes/evo/Datasets/NAM_2015_S/*')\n",
    "# df = spark.read.format('csv').option('sep', '\\t').schema(schema).load('hdfs://orion11:40910/nam_tiny.tdv')\n",
    "# df = spark.read.format('csv').option('sep', '\\t').schema(schema).load('hdfs://orion11:40910/datasets/nam_201501.tdv.gz')\n",
    "\n",
    "# df = spark.read.format('csv').option('sep', '\\t').schema(schema).load('hdfs://orion11:37000/data/nam/*')\n",
    "# df_large = spark.read.format('csv').option('sep', '\\t').schema(schema).load('hdfs://orion11:37000/data/nam/nam_201509.tdv.gz')\n",
    "df_entire = spark.read.format('csv').option('sep', '\\t').schema(schema).load('hdfs://orion11:37000/data/nam/*')\n",
    "\n",
    "# df.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_large = spark.read.format('csv').option('sep', '\\t').schema(schema).load('hdfs://orion11:40910/datasets/nam_201509.tdv.gz')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_entire = spark.read.format('csv').option('sep', '\\t').schema(schema).load('hdfs://orion11:40910/datasets/*')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Timestamp=1426377600000, Geohash='cf7ecr4h2ps0', geopotential_height_lltw=136.53125, water_equiv_of_accum_snow_depth_surface=77.0, drag_coefficient_surface=0.0, sensible_heat_net_flux_surface=-39.57763671875, categorical_ice_pellets_yes1_no0_surface=0.0, visibility_surface=24221.587890625, number_of_soil_layers_in_root_zone_surface=3.0, categorical_freezing_rain_yes1_no0_surface=0.0, pressure_reduced_to_msl_msl=99602.0, upward_short_wave_rad_flux_surface=6.625, relative_humidity_zerodegc_isotherm=34.0, categorical_snow_yes1_no0_surface=0.0, u-component_of_wind_tropopause=27.527877807617188, surface_wind_gust_surface=16.158788681030273, total_cloud_cover_entire_atmosphere=100.0, upward_long_wave_rad_flux_surface=314.0560302734375, land_cover_land1_sea0_surface=1.0, vegitation_type_as_in_sib_surface=18.0, v-component_of_wind_pblri=12.31915283203125, albedo_surface=38.75, lightning_surface=0.0, ice_cover_ice1_no_ice0_surface=0.0, convective_inhibition_surface=-0.65234375, pressure_surface=97221.0, transpiration_stress-onset_soil_moisture_surface=0.3125, soil_porosity_surface=0.5, vegetation_surface=1.0, categorical_rain_yes1_no0_surface=0.0, downward_long_wave_rad_flux_surface=274.9662780761719, planetary_boundary_layer_height_surface=1905.5, soil_type_as_in_zobler_surface=3.0, geopotential_height_cloud_base=6696.0, friction_velocity_surface=0.6008660197257996, maximumcomposite_radar_reflectivity_entire_atmosphere=-20.0, plant_canopy_surface_water_surface=0.10999999940395355, v-component_of_wind_maximum_wind=-7.388824462890625, geopotential_height_zerodegc_isotherm=2380.0, mean_sea_level_pressure_nam_model_reduction_msl=99607.0, temperature_surface=272.7480163574219, snow_cover_surface=100.0, geopotential_height_surface=195.8200225830078, convective_available_potential_energy_surface=0.0, latent_heat_net_flux_surface=-0.0329132080078125, surface_roughness_surface=0.1750158965587616, pressure_maximum_wind=20726.736328125, temperature_tropopause=206.35687255859375, geopotential_height_pblri=300.42779541015625, pressure_tropopause=20582.23828125, snow_depth_surface=0.4211999773979187, v-component_of_wind_tropopause=-7.8085479736328125, downward_short_wave_rad_flux_surface=17.0, u-component_of_wind_maximum_wind=28.66766357421875, wilting_point_surface=0.04749999940395355, precipitable_water_entire_atmosphere=9.207815170288086, u-component_of_wind_pblri=4.711578369140625, direct_evaporation_cease_soil_moisture_surface=0.04749999940395355),\n",
       " Row(Timestamp=1426377600000, Geohash='ccn3u17cxe80', geopotential_height_lltw=4.28125, water_equiv_of_accum_snow_depth_surface=26.0, drag_coefficient_surface=0.0, sensible_heat_net_flux_surface=-12.95263671875, categorical_ice_pellets_yes1_no0_surface=0.0, visibility_surface=24221.587890625, number_of_soil_layers_in_root_zone_surface=4.0, categorical_freezing_rain_yes1_no0_surface=0.0, pressure_reduced_to_msl_msl=101712.0, upward_short_wave_rad_flux_surface=1.75, relative_humidity_zerodegc_isotherm=10.0, categorical_snow_yes1_no0_surface=0.0, u-component_of_wind_tropopause=16.527877807617188, surface_wind_gust_surface=6.90878963470459, total_cloud_cover_entire_atmosphere=0.0, upward_long_wave_rad_flux_surface=303.1810302734375, land_cover_land1_sea0_surface=1.0, vegitation_type_as_in_sib_surface=1.0, v-component_of_wind_pblri=5.63165283203125, albedo_surface=24.5, lightning_surface=0.0, ice_cover_ice1_no_ice0_surface=0.0, convective_inhibition_surface=-0.65234375, pressure_surface=96528.0, transpiration_stress-onset_soil_moisture_surface=0.3125, soil_porosity_surface=0.5, vegetation_surface=15.75, categorical_rain_yes1_no0_surface=0.0, downward_long_wave_rad_flux_surface=231.71627807617188, planetary_boundary_layer_height_surface=407.0, soil_type_as_in_zobler_surface=3.0, geopotential_height_cloud_base=-5000.0, friction_velocity_surface=0.3508659601211548, maximumcomposite_radar_reflectivity_entire_atmosphere=-20.0, plant_canopy_surface_water_surface=0.5, v-component_of_wind_maximum_wind=-34.263824462890625, geopotential_height_zerodegc_isotherm=2440.0, mean_sea_level_pressure_nam_model_reduction_msl=101723.0, temperature_surface=270.3730163574219, snow_cover_surface=100.0, geopotential_height_surface=426.32000732421875, convective_available_potential_energy_surface=0.0, latent_heat_net_flux_surface=-0.0329132080078125, surface_roughness_surface=1.9000158309936523, pressure_maximum_wind=22326.736328125, temperature_tropopause=209.85687255859375, geopotential_height_pblri=328.42779541015625, pressure_tropopause=21382.23828125, snow_depth_surface=0.052799999713897705, v-component_of_wind_tropopause=-33.18354797363281, downward_short_wave_rad_flux_surface=6.875, u-component_of_wind_maximum_wind=15.91766357421875, wilting_point_surface=0.04749999940395355, precipitable_water_entire_atmosphere=5.457814693450928, u-component_of_wind_pblri=2.274078369140625, direct_evaporation_cease_soil_moisture_surface=0.04749999940395355),\n",
       " Row(Timestamp=1426377600000, Geohash='f2fgf6usbe2p', geopotential_height_lltw=-472.96875, water_equiv_of_accum_snow_depth_surface=142.0, drag_coefficient_surface=0.0, sensible_heat_net_flux_surface=-7.07763671875, categorical_ice_pellets_yes1_no0_surface=0.0, visibility_surface=7621.5888671875, number_of_soil_layers_in_root_zone_surface=4.0, categorical_freezing_rain_yes1_no0_surface=0.0, pressure_reduced_to_msl_msl=101610.0, upward_short_wave_rad_flux_surface=0.0, relative_humidity_zerodegc_isotherm=77.0, categorical_snow_yes1_no0_surface=0.0, u-component_of_wind_tropopause=20.277877807617188, surface_wind_gust_surface=7.28378963470459, total_cloud_cover_entire_atmosphere=100.0, upward_long_wave_rad_flux_surface=291.5560302734375, land_cover_land1_sea0_surface=1.0, vegitation_type_as_in_sib_surface=1.0, v-component_of_wind_pblri=-1.68084716796875, albedo_surface=39.5, lightning_surface=0.0, ice_cover_ice1_no_ice0_surface=0.0, convective_inhibition_surface=-0.65234375, pressure_surface=97035.0, transpiration_stress-onset_soil_moisture_surface=0.3125, soil_porosity_surface=0.5, vegetation_surface=3.0, categorical_rain_yes1_no0_surface=0.0, downward_long_wave_rad_flux_surface=280.2162780761719, planetary_boundary_layer_height_surface=798.5, soil_type_as_in_zobler_surface=3.0, geopotential_height_cloud_base=1541.0, friction_velocity_surface=0.6258659958839417, maximumcomposite_radar_reflectivity_entire_atmosphere=11.25, plant_canopy_surface_water_surface=0.25999999046325684, v-component_of_wind_maximum_wind=-1.513824462890625, geopotential_height_zerodegc_isotherm=0.0, mean_sea_level_pressure_nam_model_reduction_msl=101625.0, temperature_surface=267.7480163574219, snow_cover_surface=100.0, geopotential_height_surface=362.32000732421875, convective_available_potential_energy_surface=0.0, latent_heat_net_flux_surface=12.717086791992188, surface_roughness_surface=1.9000158309936523, pressure_maximum_wind=12126.736328125, temperature_tropopause=219.73187255859375, geopotential_height_pblri=193.92779541015625, pressure_tropopause=25782.23828125, snow_depth_surface=0.5703999996185303, v-component_of_wind_tropopause=10.816452026367188, downward_short_wave_rad_flux_surface=0.0, u-component_of_wind_maximum_wind=30.91766357421875, wilting_point_surface=0.04749999940395355, precipitable_water_entire_atmosphere=9.082815170288086, u-component_of_wind_pblri=-5.475921630859375, direct_evaporation_cease_soil_moisture_surface=0.04749999940395355),\n",
       " Row(Timestamp=1426377600000, Geohash='9x786rfxxpzz', geopotential_height_lltw=3072.03125, water_equiv_of_accum_snow_depth_surface=0.0, drag_coefficient_surface=0.0, sensible_heat_net_flux_surface=-18.82763671875, categorical_ice_pellets_yes1_no0_surface=0.0, visibility_surface=24221.587890625, number_of_soil_layers_in_root_zone_surface=3.0, categorical_freezing_rain_yes1_no0_surface=0.0, pressure_reduced_to_msl_msl=102109.0, upward_short_wave_rad_flux_surface=29.25, relative_humidity_zerodegc_isotherm=49.0, categorical_snow_yes1_no0_surface=0.0, u-component_of_wind_tropopause=9.652877807617188, surface_wind_gust_surface=5.90878963470459, total_cloud_cover_entire_atmosphere=70.0, upward_long_wave_rad_flux_surface=344.6810302734375, land_cover_land1_sea0_surface=1.0, vegitation_type_as_in_sib_surface=10.0, v-component_of_wind_pblri=5.06915283203125, albedo_surface=19.25, lightning_surface=0.0, ice_cover_ice1_no_ice0_surface=0.0, convective_inhibition_surface=-0.65234375, pressure_surface=78016.0, transpiration_stress-onset_soil_moisture_surface=0.32999998331069946, soil_porosity_surface=0.5, vegetation_surface=3.0, categorical_rain_yes1_no0_surface=0.0, downward_long_wave_rad_flux_surface=262.8412780761719, planetary_boundary_layer_height_surface=2201.0, soil_type_as_in_zobler_surface=6.0, geopotential_height_cloud_base=8044.0, friction_velocity_surface=0.3258659839630127, maximumcomposite_radar_reflectivity_entire_atmosphere=-8.3125, plant_canopy_surface_water_surface=0.47999998927116394, v-component_of_wind_maximum_wind=-44.763824462890625, geopotential_height_zerodegc_isotherm=3440.0, mean_sea_level_pressure_nam_model_reduction_msl=101683.0, temperature_surface=279.1230163574219, snow_cover_surface=100.0, geopotential_height_surface=2258.570068359375, convective_available_potential_energy_surface=0.0, latent_heat_net_flux_surface=25.217086791992188, surface_roughness_surface=0.1750158965587616, pressure_maximum_wind=20326.736328125, temperature_tropopause=209.85687255859375, geopotential_height_pblri=193.67779541015625, pressure_tropopause=17782.23828125, snow_depth_surface=0.0007999999797903001, v-component_of_wind_tropopause=-40.30854797363281, downward_short_wave_rad_flux_surface=152.375, u-component_of_wind_maximum_wind=5.04266357421875, wilting_point_surface=0.06624999642372131, precipitable_water_entire_atmosphere=6.082814693450928, u-component_of_wind_pblri=5.836578369140625, direct_evaporation_cease_soil_moisture_surface=0.06624999642372131),\n",
       " Row(Timestamp=1426377600000, Geohash='cf36gb2z345b', geopotential_height_lltw=315.28125, water_equiv_of_accum_snow_depth_surface=82.0, drag_coefficient_surface=0.0, sensible_heat_net_flux_surface=-39.32763671875, categorical_ice_pellets_yes1_no0_surface=0.0, visibility_surface=24221.587890625, number_of_soil_layers_in_root_zone_surface=3.0, categorical_freezing_rain_yes1_no0_surface=0.0, pressure_reduced_to_msl_msl=99229.0, upward_short_wave_rad_flux_surface=7.875, relative_humidity_zerodegc_isotherm=23.0, categorical_snow_yes1_no0_surface=0.0, u-component_of_wind_tropopause=24.402877807617188, surface_wind_gust_surface=14.65878963470459, total_cloud_cover_entire_atmosphere=100.0, upward_long_wave_rad_flux_surface=315.6810302734375, land_cover_land1_sea0_surface=1.0, vegitation_type_as_in_sib_surface=18.0, v-component_of_wind_pblri=11.38165283203125, albedo_surface=35.0, lightning_surface=0.0, ice_cover_ice1_no_ice0_surface=0.0, convective_inhibition_surface=-0.65234375, pressure_surface=95390.0, transpiration_stress-onset_soil_moisture_surface=0.3125, soil_porosity_surface=0.5, vegetation_surface=1.0, categorical_rain_yes1_no0_surface=0.0, downward_long_wave_rad_flux_surface=310.3412780761719, planetary_boundary_layer_height_surface=1036.0, soil_type_as_in_zobler_surface=3.0, geopotential_height_cloud_base=1977.0, friction_velocity_surface=0.6758660078048706, maximumcomposite_radar_reflectivity_entire_atmosphere=8.8125, plant_canopy_surface_water_surface=0.13499999046325684, v-component_of_wind_maximum_wind=-11.763824462890625, geopotential_height_zerodegc_isotherm=2680.0, mean_sea_level_pressure_nam_model_reduction_msl=99250.0, temperature_surface=273.1230163574219, snow_cover_surface=100.0, geopotential_height_surface=321.32000732421875, convective_available_potential_energy_surface=0.0, latent_heat_net_flux_surface=9.092086791992188, surface_roughness_surface=0.1750158965587616, pressure_maximum_wind=11126.736328125, temperature_tropopause=208.10687255859375, geopotential_height_pblri=258.67779541015625, pressure_tropopause=20782.23828125, snow_depth_surface=0.3779999911785126, v-component_of_wind_tropopause=6.4414520263671875, downward_short_wave_rad_flux_surface=22.75, u-component_of_wind_maximum_wind=27.04266357421875, wilting_point_surface=0.04749999940395355, precipitable_water_entire_atmosphere=10.332815170288086, u-component_of_wind_pblri=0.274078369140625, direct_evaporation_cease_soil_moisture_surface=0.04749999940395355)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df.filter(df.snow_depth_surface != 0).take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1. [1 pt] Strangely Snowy: Find a location that contains snow while its surroundings do not. Why does this occur? Is it a high mountain peak in a desert?¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-b8ef9e02667e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mstarted_at\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mnot_snow_all_year\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"SELECT  Geohash,avg(snow_cover_surface) FROM TEMP_DF group by Geohash having avg(snow_cover_surface) >= 99\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnot_snow_all_year\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark-2.3.1-bin-hadoop2.7/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    464\u001b[0m         \"\"\"\n\u001b[1;32m    465\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectToPython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    467\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBatchedSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPickleSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark-2.3.1-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1253\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1255\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1257\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m/usr/local/spark-2.3.1-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m    983\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 985\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark-2.3.1-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1152\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1153\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home2/anaconda3/lib/python3.6/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "df_large.createOrReplaceTempView(\"TEMP_DF\")\n",
    "\n",
    "started_at = datetime.now()\n",
    "\n",
    "not_snow_all_year = spark.sql(\"SELECT  Geohash,avg(snow_cover_surface) FROM TEMP_DF group by Geohash having avg(snow_cover_surface) >= 99\").collect()\n",
    "\n",
    "for x in not_snow_all_year:\n",
    "    print(x)\n",
    "print(len(not_snow_all_year))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'not_snow_all_year' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-956e95471c9a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfour_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnot_snow_all_year\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGeohash\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mfour_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGeohash\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnot_snow_all_year\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'not_snow_all_year' is not defined"
     ]
    }
   ],
   "source": [
    "four_set = set()\n",
    "for x in not_snow_all_year:\n",
    "    print(x.Geohash[0:4])\n",
    "    four_set.add(x.Geohash[0:4])\n",
    "print(len(not_snow_all_year))\n",
    "print(len(four_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c3hz%\n",
      "4\n",
      "c43g%\n",
      "5\n",
      "c43m%\n",
      "3\n",
      "c1p4%\n",
      "5\n",
      "c41z%\n",
      "4\n",
      "c36u%\n",
      "4\n",
      "c1nr%\n",
      "4\n",
      "c460%\n",
      "3\n",
      "c1p8%\n",
      "3\n",
      "c41g%\n",
      "4\n",
      "c436%\n",
      "4\n",
      "c1nt%\n",
      "4\n",
      "c433%\n",
      "4\n",
      "c41x%\n",
      "4\n",
      "c452%\n",
      "3\n",
      "c1q0%\n",
      "4\n",
      "c3k8%\n",
      "4\n",
      "c43e%\n",
      "4\n",
      "c43f%\n",
      "3\n",
      "c1uh%\n",
      "4\n",
      "c458%\n",
      "4\n",
      "c1nm%\n",
      "3\n",
      "c41y%\n",
      "3\n",
      "c43b%\n",
      "5\n",
      "c459%\n",
      "4\n",
      "c43u%\n",
      "4\n",
      "c1pf%\n",
      "3\n",
      "c3k3%\n",
      "4\n",
      "c44n%\n",
      "4\n",
      "c1gz%\n",
      "4\n",
      "c1p5%\n",
      "3\n",
      "c44h%\n",
      "5\n",
      "c1pc%\n",
      "4\n",
      "c1nu%\n",
      "4\n",
      "c432%\n",
      "5\n",
      "c44j%\n",
      "4\n",
      "c1nk%\n",
      "5\n",
      "c439%\n",
      "4\n",
      "c304%\n",
      "5\n",
      "c1gn%\n",
      "4\n",
      "c43s%\n",
      "4\n",
      "c3k0%\n",
      "5\n",
      "c43h%\n",
      "4\n",
      "c3k2%\n",
      "3\n",
      "c44w%\n",
      "3\n",
      "c43k%\n",
      "4\n",
      "c41w%\n",
      "5\n",
      "c461%\n",
      "4\n",
      "c43c%\n",
      "4\n",
      "c1gx%\n",
      "3\n",
      "c43d%\n",
      "4\n",
      "c1gr%\n",
      "5\n",
      "c44p%\n",
      "4\n",
      "c1nq%\n",
      "4\n",
      "c41v%\n",
      "5\n",
      "c1gq%\n",
      "4\n",
      "c1gw%\n",
      "4\n",
      "c41u%\n",
      "4\n",
      "c438%\n",
      "3\n",
      "c1gy%\n",
      "4\n",
      "c1sn%\n",
      "4\n",
      "c43j%\n",
      "2\n",
      "c41t%\n",
      "4\n",
      "c445%\n",
      "4\n",
      "c1q2%\n",
      "5\n",
      "c4r5%\n",
      "4\n",
      "c46s%\n",
      "4\n",
      "c45b%\n",
      "4\n",
      "c301%\n",
      "4\n",
      "c464%\n",
      "5\n",
      "c465%\n",
      "4\n",
      "c1pe%\n",
      "4\n",
      "c307%\n",
      "4\n",
      "c437%\n",
      "4\n",
      "74\n",
      "Complete\n"
     ]
    }
   ],
   "source": [
    "# df.createOrReplaceTempView(\"TEMP_DF\")\n",
    "groupList = list()\n",
    "for y in four_set:\n",
    "        param = y + '%'\n",
    "        group = spark.sql(\"SELECT distinct Geohash,avg(snow_cover_surface) as avgSnow FROM TEMP_DF WHERE Geohash like '%s' group by Geohash\" %param ).collect()\n",
    "        print(param ) \n",
    "        print(len(group))\n",
    "        groupList.append(group)\n",
    "        \n",
    "print(len(groupList))\n",
    "print('Complete')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(Geohash='c1uhvnv5p8pb', avgSnow=7.283950617283951), Row(Geohash='c1uh66r1r02p', avgSnow=9.74074074074074), Row(Geohash='c1uhcd9k1n2p', avgSnow=100.0), Row(Geohash='c1uhqne01400', avgSnow=2.9135802469135803)]\n",
      "[Row(Geohash='c3k0fgxqugeb', avgSnow=10.790123456790123), Row(Geohash='c3k0yw8m6us0', avgSnow=100.0), Row(Geohash='c3k0rj7e0zs0', avgSnow=21.506172839506174), Row(Geohash='c3k07d6swzkp', avgSnow=4.08641975308642), Row(Geohash='c3k00p3vw3gz', avgSnow=4.08641975308642)]\n"
     ]
    }
   ],
   "source": [
    "for x in groupList:\n",
    "    avg=0\n",
    "    for y in x:\n",
    "        avg= avg + y.avgSnow\n",
    "    avg= avg/len(x)\n",
    "    if avg<30:\n",
    "        print(x)\n",
    "#     print(avg)\n",
    "\n",
    "print(\"Finished. it's been \" + str((datetime.now()-started_at).seconds) + \" seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c1uhcd9k1n2p is a mountain peak(lavender peak)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2. [1 pt] Lightning rod: Where are you most likely to be struck by lightning? Use a precision of at least 4 Geohash characters and provide the top 3 locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(Geohash='9eqepuxk7x20', avgLighting=0.3952922077922078)\n",
      "Row(Geohash='9g3h968ygj7z', avgLighting=0.3871753246753247)\n",
      "Row(Geohash='9g3y52sgeceb', avgLighting=0.3758116883116883)\n",
      "Row(Geohash='9g3v7kxpuhh0', avgLighting=0.3741883116883117)\n",
      "Row(Geohash='9g3ug8ckk4hp', avgLighting=0.3741883116883117)\n",
      "Row(Geohash='9g3m79nrf3zb', avgLighting=0.37337662337662336)\n",
      "Row(Geohash='9g3mq3f7y6eb', avgLighting=0.3685064935064935)\n",
      "Row(Geohash='9g2vntcg8c2p', avgLighting=0.35957792207792205)\n",
      "Row(Geohash='9g35cqggdn5z', avgLighting=0.35714285714285715)\n",
      "Row(Geohash='9g3hegkvh55z', avgLighting=0.35714285714285715)\n",
      "Finished. it's been 851 seconds\n"
     ]
    }
   ],
   "source": [
    "df_entire.createOrReplaceTempView(\"TEMP_DF\")\n",
    "started_at = datetime.now()\n",
    "lightning = spark.sql(\"SELECT  Geohash,avg(lightning_surface) as avgLighting FROM TEMP_DF group by Geohash order by avgLighting DESC Limit 10 \").collect()\n",
    "\n",
    "for x in lightning:\n",
    "    print(x)\n",
    "print(\"Finished. it's been \" + str((datetime.now()-started_at).seconds) + \" seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I calculate the average lightning chances. The top 3 location is 9eqe, 9g3h, 9g3y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3. [1 pt] Drying out: Choose a region in North America (defined by one or more Geohashes) and determine when its driest month is. This should include a histogram with data from each month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "dfList = list()\n",
    "for x in range(1,13):\n",
    "    if x<10:\n",
    "        x = '0'+str(x)\n",
    "    path = \"hdfs://orion11:40910/datasets/nam_2015%s.tdv.gz\" %x\n",
    "#     print(path)\n",
    "    df_1 = spark.read.format('csv').option('sep', '\\t').schema(schema).load(path)\n",
    "    dfList.append(df_1)\n",
    "print(len(dfList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(subGeo='9q9n', avgRain=0.0)\n",
      "Row(subGeo='9q9n', avgRain=0.0603448275862069)\n",
      "Row(subGeo='9q9n', avgRain=0.0)\n",
      "Row(subGeo='9q9n', avgRain=0.0777027027027027)\n",
      "Row(subGeo='9q9n', avgRain=0.020161290322580645)\n",
      "Row(subGeo='9q9n', avgRain=0.0022935779816513763)\n",
      "Row(subGeo='9q9n', avgRain=0.01639344262295082)\n",
      "Row(subGeo='9q9n', avgRain=0.008333333333333333)\n",
      "Row(subGeo='9q9n', avgRain=0.009259259259259259)\n",
      "Row(subGeo='9q9n', avgRain=0.0021551724137931034)\n",
      "Row(subGeo='9q9n', avgRain=0.0546218487394958)\n",
      "Row(subGeo='9q9n', avgRain=0.0872093023255814)\n"
     ]
    }
   ],
   "source": [
    "#9q9n categorical_rain_yes1_no0_surface\n",
    "from pyspark.sql.functions import substring\n",
    "started_at = datetime.now()\n",
    "for df_1 in dfList:  \n",
    "    df_1.createOrReplaceTempView(\"TEMP_DF\")\n",
    "    dry_place = spark.sql(\"SELECT  substring(Geohash, 1, 4) as subGeo, AVG(categorical_rain_yes1_no0_surface) as avgRain FROM TEMP_DF group by subGeo having subGeo = '9q9n'\").collect()\n",
    "    for x in dry_place:\n",
    "        print(x)\n",
    "print(\"Finished. it's been \" + str((datetime.now()-started_at).seconds) + \" seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the avgerage rain chances for Hayward. January, March do not rain at all. \n",
    "<br>Bay Area<br>\n",
    "![](./images/bayArea.jpg)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4. [2 pt] Travel Startup: After graduating from USF, you found a startup that aims to provide personalized travel itineraries using big data analysis. Given your own personal preferences, build a plan for a year of travel across 5 locations. Or, in other words: pick 5 regions. What is the best time of year to visit them based on the dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fList = list()\n",
    "for x in range(1,13):\n",
    "    if x<10:\n",
    "        x = '0'+str(x)\n",
    "    path = \"hdfs://orion11:37000/data/nam/nam_2015%s.tdv.gz\" %x\n",
    "#     print(path)\n",
    "    df_1 = spark.read.format('csv').option('sep', '\\t').schema(schema).load(path)\n",
    "    dfList.append(df_1)\n",
    "print(len(dfList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime.datetime.now().replace(microsecond=0)\n",
    "\n",
    "month = 1\n",
    "for df_1 in dfList:  \n",
    "    df_1.createOrReplaceTempView(\"TRAVEL_DF\")\n",
    "\n",
    "    # F = 9/5(K − 273.15) + 32  68 to 72 °F, so K should be 293.15 to 295.37\n",
    "    best_time = spark.sql(\"SELECT Timestamp, Geohash, avg(temperature_surface) as avgtemper, \\\n",
    "                             avg(relative_humidity_zerodegc_isotherm) as avghum FROM TRAVEL_DF \\\n",
    "                                group by Timestamp, Geohash \\\n",
    "                                  having avg(temperature_surface) > 293.15 \\\n",
    "                                     and avg(temperature_surface) < 295.37\").collect()\n",
    "    print('month:', month)\n",
    "    month = month + 1\n",
    "    \n",
    "    count = 0\n",
    "    for x in best_time:\n",
    "        f = 9/5 * (x.avgtemper - 273.15) + 32\n",
    "        c = (f - 32) * 5/9\n",
    "        THI1 = 1.8 * c - (1 - x.avghum / 100) * (c - 14.3) + 32\n",
    "    #     print(THI1)\n",
    "        if count > 4 :\n",
    "            break\n",
    "        if THI1 >= 65 and THI1 < 75:\n",
    "            count = count + 1\n",
    "            print(x)\n",
    "            print('THI1 value is:', THI1)\n",
    "\n",
    "end = datetime.datetime.now().replace(microsecond=0)\n",
    "print('Job running time:', end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q5. [1 pt] Escaping the fog: After becoming rich from your startup, you are looking for the perfect location to build your Bay Area mansion with unobstructed views. Find the locations that are the least foggy and show them on a map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(Geohash='9q9jnpfx5rxb', avgVisibility=23811.522961255705)\n",
      "Row(Geohash='9q9jv7p453zz', avgVisibility=23703.065985376827)\n",
      "Row(Geohash='9q9m35ffeg7z', avgVisibility=23698.021476396112)\n",
      "Row(Geohash='9q9kc8wprss0', avgVisibility=23675.469547196735)\n",
      "Row(Geohash='9q9jdrny12rz', avgVisibility=23616.419106960297)\n",
      "Row(Geohash='9q8zuzx3hzeb', avgVisibility=23600.692098283627)\n",
      "Row(Geohash='9q9k4m9efqh0', avgVisibility=23497.428004600173)\n",
      "Row(Geohash='9q9hp3c2xy5b', avgVisibility=23449.653520560052)\n",
      "Row(Geohash='9q9hwsyh477z', avgVisibility=23436.152038374712)\n",
      "Row(Geohash='9q9mbqzmqxrz', avgVisibility=23435.8553033342)\n",
      "Row(Geohash='9q9pesk2gbs0', avgVisibility=23400.098629552464)\n",
      "Row(Geohash='9q9j5h47k07z', avgVisibility=23329.623851402575)\n",
      "Row(Geohash='9q9py8hurvup', avgVisibility=23277.991815833026)\n",
      "Row(Geohash='9q9nky5sd52p', avgVisibility=23164.93542607814)\n",
      "Row(Geohash='9q9nxfh005kp', avgVisibility=23088.525926889935)\n",
      "Row(Geohash='9q9p887u91up', avgVisibility=22978.882014243467)\n",
      "Row(Geohash='9q8zms7pxw7z', avgVisibility=22889.71287378147)\n",
      "Row(Geohash='9q8zcgwg9y7z', avgVisibility=22882.88794112842)\n",
      "Row(Geohash='9q9n3f70mpeb', avgVisibility=22881.849376101167)\n",
      "Row(Geohash='9q9ppm0d2vgz', avgVisibility=22866.27073556238)\n",
      "Row(Geohash='9q9mkpejm7rz', avgVisibility=22846.537790437127)\n",
      "Row(Geohash='9q8vx7q925s0', avgVisibility=22698.1698534142)\n",
      "Row(Geohash='9q9mx57x2ps0', avgVisibility=22678.882005442498)\n",
      "Row(Geohash='9q9ph1pyzf5b', avgVisibility=22557.517036797384)\n",
      "Row(Geohash='9q9kusw6n7kp', avgVisibility=22483.184677927715)\n",
      "Row(Geohash='9q9km33ww4h0', avgVisibility=22449.653523746514)\n",
      "Row(Geohash='9q9j004nwjgz', avgVisibility=22337.042257518373)\n",
      "Row(Geohash='9q8yny6gc2pb', avgVisibility=22245.202497623795)\n",
      "Row(Geohash='9q9mp8qsnqrz', avgVisibility=22244.163904211287)\n",
      "Row(Geohash='9q8yy1rjchrz', avgVisibility=22123.540761658984)\n",
      "Row(Geohash='9q9he9jbjws0', avgVisibility=21851.87905278192)\n",
      "Row(Geohash='9q8z68ddprbp', avgVisibility=21642.383494119615)\n",
      "Row(Geohash='9q9ncjr6xueb', avgVisibility=21488.377561846544)\n",
      "Row(Geohash='9q8yejw8eb7z', avgVisibility=21146.982903568256)\n",
      "Row(Geohash='9q8v37qn62h0', avgVisibility=20924.28260454297)\n",
      "Row(Geohash='9q9h2tjgdy7z', avgVisibility=20923.095670479343)\n",
      "Row(Geohash='9q8vkrqk0g2p', avgVisibility=20726.95322760064)\n",
      "Row(Geohash='9q8vbyd0t880', avgVisibility=20611.226223915903)\n",
      "Row(Geohash='9q8y81w4x87z', avgVisibility=20583.48141743946)\n",
      "Row(Geohash='9q8y5f6qqy00', avgVisibility=20559.594181275865)\n",
      "40\n",
      "Finished. it's been 317 seconds\n"
     ]
    }
   ],
   "source": [
    "started_at = datetime.now()\n",
    "df_entire.createOrReplaceTempView(\"TEMP_DF\")\n",
    "visibility = spark.sql(\"SELECT  Geohash,avg(visibility_surface) as avgVisibility FROM TEMP_DF WHERE Geohash like '9q9p%' or Geohash like '9q9n%' or Geohash like '9q9j%' or Geohash like '9q9m%' or Geohash like '9q9k%' or Geohash like '9q9h%' or Geohash like '9q8v%' or Geohash like '9q8y%' or Geohash like '9q8z%' group by Geohash order by avgVisibility DESC \").collect()\n",
    "for x in visibility:\n",
    "    print(x)\n",
    "print(len(visibility))\n",
    "print(\"Finished. it's been \" + str((datetime.now()-started_at).seconds) + \" seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The list above has the best visibility in the bay area. \n",
    "Such as 9q9m35 is a good place to build a mansion. The address is S Grimmer Blvd:Fremont Blvd, Fremont, CA\n",
    "<br>\n",
    "![](./images/Grimmer.jpg)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q6. [2 pt] SolarWind, Inc.: You get bored enjoying the amazing views from your mansion, so you start a new company; here, you want to help power companies plan out the locations of solar and wind farms across North America. Locate the top 3 places for solar and wind farms, as well as a combination of both (solar + wind farm). You will report a total of 9 Geohashes as well as their relevant attributes (for example, cloud cover and wind speeds)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime.datetime.now().replace(microsecond=0)\n",
    "\n",
    "df.createOrReplaceTempView(\"SOLAR_WIND_DF\")\n",
    "\n",
    "print('Top 3 places for solar:')\n",
    "cloud = spark.sql(\"SELECT Geohash, avg(total_cloud_cover_entire_atmosphere) as avgcloud, \\\n",
    "                     avg(surface_wind_gust_surface) as avgwind \\\n",
    "                       FROM SOLAR_WIND_DF WHERE land_cover_land1_sea0_surface = 1 \\\n",
    "                         and total_cloud_cover_entire_atmosphere >= 0 \\\n",
    "                           group by Geohash order by avgcloud ASC limit 3\").collect()\n",
    "\n",
    "for c in cloud:\n",
    "    print(c)\n",
    "\n",
    "print('Top 3 places for wind farms:')    \n",
    "wind = spark.sql(\"SELECT Geohash, avg(total_cloud_cover_entire_atmosphere) as avgcloud, \\\n",
    "                    avg(surface_wind_gust_surface) as avgwind \\\n",
    "                      FROM SOLAR_WIND_DF WHERE land_cover_land1_sea0_surface = 1 \\\n",
    "                        and geopotential_height_surface > 80 \\\n",
    "                          and total_cloud_cover_entire_atmosphere >= 0 \\\n",
    "                            group by Geohash having avgwind > 6.5 order by avgwind DESC limit 3\").collect()\n",
    "\n",
    "for w in wind:\n",
    "    print(w)\n",
    "\n",
    "print('Top 3 places for solar and wind farms:')\n",
    "solar_wind = spark.sql(\"SELECT Geohash, avg(total_cloud_cover_entire_atmosphere) as avgcloud, \\\n",
    "                          avg(surface_wind_gust_surface) as avgwind \\\n",
    "                            FROM SOLAR_WIND_DF WHERE land_cover_land1_sea0_surface = 1 \\\n",
    "                              and geopotential_height_surface > 80 \\\n",
    "                                and total_cloud_cover_entire_atmosphere >= 0 \\\n",
    "                                  group by Geohash having avgwind > 6.5\").collect()\n",
    "\n",
    "\n",
    "def comparator(e):\n",
    "    return e.avgwind - e.avgcloud\n",
    "\n",
    "solar_wind.sort(reverse=True, key=comparator)\n",
    "\n",
    "count = 0\n",
    "for sw in solar_wind:\n",
    "    if count > 2:\n",
    "        break\n",
    "    count = count + 1\n",
    "    print(sw, 'comb_solar_wind:', sw.avgwind - sw.avgcloud)\n",
    "\n",
    "end = datetime.datetime.now().replace(microsecond=0)\n",
    "print('Job running time:', end-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q7. [2 pt] Climate Chart: Given a Geohash prefix, create a climate chart for the region. This includes high, low, and average temperatures, as well as monthly average rainfall (precipitation). Here’s a (poor quality) script that will generate this for you.\n",
    "* Earn up to 1 point of extra credit for enhancing/improving this chart (or porting it to a more feature-rich visualization library)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prefix geohash is 9q94r cutted from 9q94rzdk9 and it is a location in San Francisco."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = spark.read.format('csv').option('sep', '\\t').schema(schema).load('/Volumes/evo/Datasets/NAM_2015_S/*')\n",
    "\n",
    "dfList = list()\n",
    "for x in range(1,13):\n",
    "    if x<10:\n",
    "        x = '0'+str(x)\n",
    "    path = \"hdfs://orion11:20910/datasets/nam_2015%s.tdv.gz\" %x\n",
    "    df_month = spark.read.format('csv').option('sep', '\\t').schema(schema).load(path)\n",
    "    dfList.append(df_month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "started_at = datetime.now()\n",
    "\n",
    "# key: month\n",
    "# value: month_data_list max_temp, min_temp, avg_temp, avg_rain\n",
    "all_temp_rain_dict = dict()\n",
    "\n",
    "month_index = 1\n",
    "for df_month in dfList:\n",
    "    df_month.createOrReplaceTempView(\"temp_rain_table\")\n",
    "    max_temp = 0\n",
    "    min_temp = 0\n",
    "    avg_temp = 0\n",
    "    avg_rain = 0\n",
    "    month_data = list()\n",
    "    \n",
    "    temp_rain = spark.sql(\"select max(temperature_surface) as max_temp,\\\n",
    "    min(temperature_surface) as min_temp,\\\n",
    "    avg(temperature_surface) as avg_temp,\\\n",
    "    avg(precipitable_water_entire_atmosphere) as avg_rain\\\n",
    "    from temp_rain_table\").collect()\n",
    "\n",
    "    for ele in temp_rain:\n",
    "        month_data.append(ele.max_temp)\n",
    "        month_data.append(ele.min_temp)\n",
    "        month_data.append(ele.avg_temp)\n",
    "        month_data.append(ele.avg_rain)\n",
    "    \n",
    "    all_temp_rain_dict[month_index] = month_data\n",
    "    month_index += 1\n",
    "    \n",
    "print(all_temp_rain_dict)\n",
    "print(\"Finished. it's been \" + str((datetime.now()-started_at).seconds) + \" seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.gridspec as gridspec\n",
    "import matplotlib\n",
    "from matplotlib import rc\n",
    "\n",
    "def c2f(t):\n",
    "    return (t*9/5.0)+32\n",
    "\n",
    "def k2c(t):\n",
    "    return t-273.15\n",
    "\n",
    "def k2f(t):\n",
    "    return (t*9/5.0)-459.67\n",
    "\n",
    "def disable_spines(ax):\n",
    "    for s in ax.spines:\n",
    "        ax.spines[s].set_visible(False)\n",
    "\n",
    "rc('font',**{'family':'sans-serif','sans-serif':['Arial']})\n",
    "use_c = False\n",
    "converter = k2f\n",
    "if use_c:\n",
    "    converter = k2c\n",
    "\n",
    "first_line = 'Temperature and Precipitable Water in San Francisco'\n",
    "\n",
    "# convert dict into list list\n",
    "data = list()\n",
    "for month in all_temp_rain_dict:\n",
    "    one_month = list()\n",
    "    one_month.append(month)\n",
    "    one_month = one_month + all_temp_rain_dict[month]\n",
    "    data.append(one_month)\n",
    "\n",
    "data = np.asarray(data)\n",
    "print(data)\n",
    "\n",
    "plt.ion()\n",
    "plt.clf()\n",
    "fig = plt.figure(1)\n",
    "fig.subplots_adjust(hspace=.20)\n",
    "gs = gridspec.GridSpec(2, 1, height_ratios=[1.75, 1])\n",
    "ax0 = plt.subplot(gs[0])\n",
    "ax1 = plt.subplot(gs[1], sharex=ax0)\n",
    "plt.setp(ax0.get_xticklabels(), visible=False) # disable upper axis label\n",
    "\n",
    "ax0.patch.set_facecolor('None')\n",
    "ax1.patch.set_facecolor('None')\n",
    "\n",
    "plt.suptitle(first_line, fontsize=14)\n",
    "\n",
    "if k2c(data[:, 1]).min() < 5:\n",
    "    y = 0\n",
    "    if not use_c:\n",
    "        y = c2f(0)\n",
    "    ax0.plot([0, data[:, 1].max() + 1], [y, y], zorder=-1, color='#888888',\n",
    "            alpha=.75, dashes=(8, 2))\n",
    "\n",
    "# <month-num>  <high-temp>  <low-temp>  <avg-temp>  <avg-precip>  \n",
    "\n",
    "rects0 = ax0.bar(.35 + data[:, 0], data[:, 2] - data[:, 1], bottom=data[:, 1],\n",
    "        width=.6, color='#df3c3c', edgecolor='#731515')\n",
    "\n",
    "rects1 = ax1.bar(.35 + data[:, 0], data[:, 4], color='#1b7edb', width=.6,\n",
    "        edgecolor='#1d4871')\n",
    "\n",
    "##################\n",
    "plt.xticks(np.arange(0,12) + 1.4, ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun',\n",
    "    'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'],\n",
    "    rotation=30)\n",
    "\n",
    "disable_spines(ax0)\n",
    "disable_spines(ax1)\n",
    "ax0.spines['left'].set_visible(True)\n",
    "ax1.spines['left'].set_visible(True)\n",
    "\n",
    "for tic in ax0.xaxis.get_major_ticks():\n",
    "    tic.tick1On = tic.tick2On = False\n",
    "\n",
    "for tic in ax0.yaxis.get_major_ticks():\n",
    "    tic.tick2On = False\n",
    "\n",
    "for tic in ax1.xaxis.get_major_ticks():\n",
    "    tic.tick1On = tic.tick2On = False\n",
    "\n",
    "for tic in ax1.yaxis.get_major_ticks():\n",
    "    tic.tick2On = False\n",
    "\n",
    "for rect in rects1:\n",
    "    height = rect.get_height()\n",
    "    ax1.text(rect.get_x() + rect.get_width()/2., 1.08*height,\n",
    "        '%.1f' % (height), ha='center', va='bottom', color='#1d4871')\n",
    "\n",
    "for r, rect in enumerate(rects0):\n",
    "    height = rect.get_height()\n",
    "    ax0.text(rect.get_x() + rect.get_width()/2., rect.get_y() + 1.08*height,\n",
    "        '%d' % int(height + rect.get_y()), ha='center', va='bottom',\n",
    "        color='#731515')\n",
    "    ax0.text(rect.get_x() + rect.get_width()/2., rect.get_y() - 2,\n",
    "        '%d' % int(rect.get_y()), ha='center', va='top', color='#731515')\n",
    "    ax0.plot([rect.get_x() + .05, rect.get_x() + rect.get_width() - .05],\n",
    "            [data[r, 3], data[r, 3]], color='#731515')\n",
    "\n",
    "if use_c:\n",
    "    ax0.set_ylabel('Temperature (C)')\n",
    "    ax1.set_ylabel('Precipitation (cm)')\n",
    "else:\n",
    "    ax0.set_ylabel('Temperature (F)')\n",
    "    ax1.set_ylabel('Precipitation (in)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q8. [2 pt] Influencers: Determine how features influence each other using Pearson’s correlation coefficient (PCC). The output for this job should include (1) feature pairs sorted by absolute correlation coefficient, and (2) a correlation matrix visualization (heatmaps are a good option)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.stat import Correlation\n",
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.stat import Statistics\n",
    "import numpy as np\n",
    "\n",
    "started_at = datetime.now()\n",
    "\n",
    "seriesX = sc.parallelize([1.0, 2.0, 3.0, 3.0, 5.0])  # a series\n",
    "# seriesY must have the same number of partitions and cardinality as seriesX\n",
    "seriesY = sc.parallelize([11.0, 22.0, 33.0, 33.0, 555.0])\n",
    "\n",
    "print(seriesX)\n",
    "print(seriesY)\n",
    "\n",
    "# Compute the correlation using Pearson's method. Enter \"spearman\" for Spearman's method.\n",
    "# If a method is not specified, Pearson's method will be used by default.\n",
    "print(\"Correlation is: \" + str(Statistics.corr(seriesX, seriesY, method=\"pearson\")))\n",
    "\n",
    "data = sc.parallelize(\n",
    "    [np.array([1.0, 10.0, 100.0]), np.array([2.0, 20.0, 200.0]), np.array([5.0, 33.0, 366.0])]\n",
    ")  # an RDD of Vectors\n",
    "\n",
    "# calculate the correlation matrix using Pearson's method. Use \"spearman\" for Spearman's method.\n",
    "# If a method is not specified, Pearson's method will be used by default.\n",
    "print(Statistics.corr(data, method=\"pearson\"))\n",
    "\n",
    "print(\"Finished. it's been \" + str((datetime.now()-started_at).seconds) + \" seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q9. [2 pt] Prediction/Classification: Using what you learned above as your guide, choose a feature to predict or classify via machine learning models in MLlib. You will need to explain:\n",
    "* The feature you will predict/classify\n",
    "* Features used to train the model\n",
    "* How you partitioned your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "def prepare_data(dframe, predictors, target):\n",
    "    assembler = VectorAssembler(inputCols=predictors, outputCol=\"features\")\n",
    "    output = assembler.transform(dframe)\n",
    "    return output.select(\"features\", target).withColumnRenamed(target, \"label\")\n",
    "\n",
    "\n",
    "prepped = prepare_data(df,\n",
    "    [\"lightning_surface\", \n",
    "         \"categorical_rain_yes1_no0_surface\", \n",
    "         \"soil_type_as_in_zobler_surface\", \n",
    "         \"plant_canopy_surface_water_surface\",\n",
    "         \"geopotential_height_zerodegc_isotherm\",\n",
    "         \"v-component_of_wind_maximum_wind\",\n",
    "         \"u-component_of_wind_maximum_wind\",\n",
    "         \"temperature_surface\"],\n",
    "    \"vegetation_surface\")\n",
    "\n",
    "prepped.show()\n",
    "(trainingData, testData) = prepped.randomSplit([0.9, 0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "rf = RandomForestRegressor(numTrees=100, maxDepth=5, maxBins=32)\n",
    "model = rf.fit(trainingData)\n",
    "predictions = model.transform(testData)\n",
    "\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "p_df = predictions.select(\"label\", \"prediction\").toPandas()\n",
    "\n",
    "plt.suptitle('Random Forest Regressor', fontsize=16)\n",
    "\n",
    "minval = p_df[['label', 'prediction']].min().min()\n",
    "maxval = p_df[['label', 'prediction']].max().max()\n",
    "plt.axis([minval, maxval, minval, maxval])\n",
    "\n",
    "plt.plot(p_df['label'], p_df['prediction'], '.', color='#2ba5f1')\n",
    "plt.plot(range(int(minval), int(maxval)), range(int(minval), int(maxval)), lw=3, dashes=(10, 3), color='#000000', alpha=0.25, label='Ideal Predictor')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Option 1: Advanced Analysis\n",
    "## You’ve had the opportunity to analyze two datasets thus far; now it’s time to analyze a dataset of your own. Find a dataset online and use Spark (or Hadoop) to analyze it. You should:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. [0.5 pt] Describe the dataset\n",
    "* Police Department Incident Reports: Historical 2003 to May 2018 in San Francisco   [source from]: https://data.sfgov.org/Public-Safety/Police-Department-Incident-Reports-Historical-2003/tmnf-yvry\n",
    "* What's in this Dataset?\n",
    "    * The .csv file(462.9 MB) contains:   \n",
    "        * Rows: 2.21M \n",
    "        * Columns: 13 \n",
    "        * Each row is a: Incident Report\n",
    "* Here are features in the dataset:\n",
    "    * IncidntNum\n",
    "    * Category\n",
    "    * Descript\n",
    "    * DayOfWeek\n",
    "    * Date\n",
    "    * Time\n",
    "    * PdDistrict\n",
    "    * Resolution\n",
    "    * Address\n",
    "    * X\n",
    "    * Y\n",
    "    * Location\n",
    "    * PdId\n",
    "* Preview\n",
    "![](./images/PDIR_dataset.png)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. [0.5 pt] Outline the types of insights you hope to gain from it\n",
    "* Where is the most dangerous area in San Francisco ?\n",
    "* What kind of incident happened most in San Francisco ?\n",
    "* How many incidents were finally resolved ?\n",
    "* Are there any reasons that may raise/drop the crime rate? Time? Place? Resolution rate？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. [1 pt] Make hypotheses about what you might find\n",
    "* Break in car (VANDALISM) happened a lot in San Francisco.\n",
    "* None resolution may take somehow proportion.\n",
    "* Annual crime rate should be dropped from 2003 to 2018.\n",
    "* List some high incidence areas for 24 hours a day in order to remind people to avoid/pay more attention in those areas in a specific time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_option = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").csv('hdfs://orion11:40910/option_dataset/Police_Department_Incident_Reports__Historical_2003_to_May_2018.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "194694\n"
     ]
    }
   ],
   "source": [
    "notComplete = df_option.filter(df_option.Category == \"ASSAULT\").count()\n",
    "print(notComplete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+--------+---------+----------+-----+----------+--------------+--------------------+----------------+---------------+--------------------+--------------+\n",
      "|IncidntNum|Category|Descript|DayOfWeek|      Date| Time|PdDistrict|    Resolution|             Address|               X|              Y|            Location|          PdId|\n",
      "+----------+--------+--------+---------+----------+-----+----------+--------------+--------------------+----------------+---------------+--------------------+--------------+\n",
      "| 140646669| ASSAULT| BATTERY|   Monday|08/04/2014|08:56|   MISSION|ARREST, BOOKED|500 Block of SOUT...|-122.41747701285|37.764357751686|(37.764357751686,...|14064666904134|\n",
      "+----------+--------+--------+---------+----------+-----+----------+--------------+--------------------+----------------+---------------+--------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_option.createOrReplaceTempView(\"POLICE_DF\")\n",
    "spark.sql(\"select * from POLICE_DF limit 1\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1. Since we are super rich, we hire the Batman to take justice in San Francisco for 5 hours. But the Batman could only take justice for one place for an hour. So list the time table for the Batman, that is when happened the most incidents in an hour which needs the Batman’s help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(hour='00', timecount=113096)\n",
      "Row(hour='01', timecount=65182)\n",
      "Row(hour='02', timecount=54550)\n",
      "Row(hour='03', timecount=35596)\n",
      "Row(hour='04', timecount=25285)\n",
      "Row(hour='05', timecount=22413)\n",
      "Row(hour='06', timecount=33494)\n",
      "Row(hour='07', timecount=55551)\n",
      "Row(hour='08', timecount=82459)\n",
      "Row(hour='09', timecount=89303)\n",
      "Row(hour='10', timecount=95469)\n",
      "Row(hour='11', timecount=97620)\n",
      "Row(hour='12', timecount=132631)\n",
      "Row(hour='13', timecount=108540)\n",
      "Row(hour='14', timecount=112078)\n",
      "Row(hour='15', timecount=120190)\n",
      "Row(hour='16', timecount=125548)\n",
      "Row(hour='17', timecount=135481)\n",
      "Row(hour='18', timecount=140918)\n",
      "Row(hour='19', timecount=126404)\n",
      "Row(hour='20', timecount=115010)\n",
      "Row(hour='21', timecount=109559)\n",
      "Row(hour='22', timecount=113915)\n",
      "Row(hour='23', timecount=104732)\n",
      "24\n",
      "Finished. it's been 1 seconds\n"
     ]
    }
   ],
   "source": [
    "started_at = datetime.now()\n",
    "df_option.createOrReplaceTempView(\"POLICE_DF\")\n",
    "batman = spark.sql(\"SELECT substring(time, 1, 2 ) hour, count(*) as timecount FROM POLICE_DF  group by hour order by timecount DESC limit 5\").collect()\n",
    "for x in batman:\n",
    "    print(x)\n",
    "print(len(batman))\n",
    "print(\"Finished. it's been \" + str((datetime.now()-started_at).seconds) + \" seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2. Break in car (VANDALISM) when and where"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(PdDistrict='TENDERLOIN', c=4214)\n",
      "Row(PdDistrict='PARK', c=6554)\n",
      "Row(PdDistrict='RICHMOND', c=7837)\n",
      "Row(PdDistrict='TARAVAL', c=11607)\n",
      "Row(PdDistrict='CENTRAL', c=12586)\n",
      "Row(PdDistrict='INGLESIDE', c=13126)\n",
      "Row(PdDistrict='MISSION', c=14050)\n",
      "Row(PdDistrict='BAYVIEW', c=14103)\n",
      "Row(PdDistrict='NORTHERN', c=14533)\n",
      "Row(PdDistrict='SOUTHERN', c=17449)\n",
      "10\n",
      "There are totally 116059vandalism\n",
      "TENDERLOIN\n",
      "0.03630911863793415\n",
      "PARK\n",
      "0.05647127753987196\n",
      "RICHMOND\n",
      "0.06752599970704555\n",
      "TARAVAL\n",
      "0.10000947793794536\n",
      "CENTRAL\n",
      "0.10844484270931164\n",
      "INGLESIDE\n",
      "0.11309764860975884\n",
      "MISSION\n",
      "0.12105911648385735\n",
      "BAYVIEW\n",
      "0.12151578076667902\n",
      "NORTHERN\n",
      "0.12522079287259066\n",
      "SOUTHERN\n",
      "0.15034594473500548\n",
      "Finished. it's been 1 seconds\n"
     ]
    }
   ],
   "source": [
    "started_at = datetime.now()\n",
    "vandalism = spark.sql(\"SELECT PdDistrict, count(*) c FROM POLICE_DF WHERE Category ='VANDALISM' group by PdDistrict order by c\").collect()\n",
    "count = 0\n",
    "for x in vandalism:\n",
    "    count = count + x.c\n",
    "    print(x)\n",
    "print(len(vandalism))\n",
    "print('There are totally ' + str(count) + ' vandalisms')\n",
    "for y in vandalism:\n",
    "    print(y.PdDistrict)\n",
    "    print(y.c/count)\n",
    "print(\"Finished. it's been \" + str((datetime.now()-started_at).seconds) + \" seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(hour='00', timecount=6534)\n",
      "Row(hour='01', timecount=4562)\n",
      "Row(hour='02', timecount=4303)\n",
      "Row(hour='03', timecount=2898)\n",
      "Row(hour='04', timecount=1942)\n",
      "Row(hour='05', timecount=1611)\n",
      "Row(hour='06', timecount=1856)\n",
      "Row(hour='07', timecount=2518)\n",
      "Row(hour='08', timecount=3684)\n",
      "Row(hour='09', timecount=3474)\n",
      "Row(hour='10', timecount=3587)\n",
      "Row(hour='11', timecount=3395)\n",
      "Row(hour='12', timecount=4640)\n",
      "Row(hour='13', timecount=3600)\n",
      "Row(hour='14', timecount=4077)\n",
      "Row(hour='15', timecount=4801)\n",
      "Row(hour='16', timecount=5316)\n",
      "Row(hour='17', timecount=7077)\n",
      "Row(hour='18', timecount=8402)\n",
      "Row(hour='19', timecount=7619)\n",
      "Row(hour='20', timecount=7549)\n",
      "Row(hour='21', timecount=7683)\n",
      "Row(hour='22', timecount=7829)\n",
      "Row(hour='23', timecount=7102)\n",
      "24\n",
      "Finished. it's been 1 seconds\n"
     ]
    }
   ],
   "source": [
    "started_at = datetime.now()\n",
    "vandalism = spark.sql(\"SELECT substring(time, 1, 2 ) hour, count(*) as timecount FROM POLICE_DF WHERE Category ='VANDALISM' group by hour order by hour\").collect()\n",
    "for x in vandalism:\n",
    "    print(x)\n",
    "print(len(vandalism))\n",
    "\n",
    "print(\"Finished. it's been \" + str((datetime.now()-started_at).seconds) + \" seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "看上去多发生在半夜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(dates='01', timecount=9982)\n",
      "Row(dates='02', timecount=8958)\n",
      "Row(dates='03', timecount=10407)\n",
      "Row(dates='04', timecount=10173)\n",
      "Row(dates='05', timecount=9752)\n",
      "Row(dates='06', timecount=9336)\n",
      "Row(dates='07', timecount=10065)\n",
      "Row(dates='08', timecount=9718)\n",
      "Row(dates='09', timecount=9501)\n",
      "Row(dates='10', timecount=10112)\n",
      "Row(dates='11', timecount=9002)\n",
      "Row(dates='12', timecount=9053)\n",
      "12\n",
      "Finished. it's been 1 seconds\n"
     ]
    }
   ],
   "source": [
    "started_at = datetime.now()\n",
    "vandalism = spark.sql(\"SELECT substring(date, 1, 2 ) dates, count(*) as timecount FROM POLICE_DF WHERE Category ='VANDALISM' group by dates order by dates\").collect()\n",
    "for x in vandalism:\n",
    "    print(x)\n",
    "print(len(vandalism))\n",
    "\n",
    "print(\"Finished. it's been \" + str((datetime.now()-started_at).seconds) + \" seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "月份上看起来挺平均的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(year='2003', timecount=6448)\n",
      "Row(year='2004', timecount=6496)\n",
      "Row(year='2005', timecount=7013)\n",
      "Row(year='2006', timecount=7688)\n",
      "Row(year='2007', timecount=7566)\n",
      "Row(year='2008', timecount=7342)\n",
      "Row(year='2009', timecount=7604)\n",
      "Row(year='2010', timecount=7934)\n",
      "Row(year='2011', timecount=7243)\n",
      "Row(year='2012', timecount=7808)\n",
      "Row(year='2013', timecount=6921)\n",
      "Row(year='2014', timecount=7165)\n",
      "Row(year='2015', timecount=7675)\n",
      "Row(year='2016', timecount=8595)\n",
      "Row(year='2017', timecount=9765)\n",
      "Row(year='2018', timecount=2796)\n",
      "16\n",
      "Finished. it's been 1 seconds\n"
     ]
    }
   ],
   "source": [
    "started_at = datetime.now()\n",
    "vandalism = spark.sql(\"SELECT substring(date, 7, 4 ) year, count(*) as timecount FROM POLICE_DF WHERE Category ='VANDALISM' group by year order by year\").collect()\n",
    "# vandalism = spark.sql(\"SELECT substring(time, 1, 2 ) hour, date, PdDistrict FROM POLICE_DF WHERE Category ='VANDALISM' order by hour\").collect()\n",
    "for x in vandalism:\n",
    "    print(x)\n",
    "print(len(vandalism))\n",
    "\n",
    "print(\"Finished. it's been \" + str((datetime.now()-started_at).seconds) + \" seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "年份上也很平均"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3. \n",
    "### a. Top 10 Category and resolution rate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(Category='LARCENY/THEFT', totalCount=480448, resolveCount=42521)\n",
      "Row(Category='OTHER OFFENSES', totalCount=309358, resolveCount=221514)\n",
      "Row(Category='NON-CRIMINAL', totalCount=238323, resolveCount=53465)\n",
      "Row(Category='ASSAULT', totalCount=194694, resolveCount=80947)\n",
      "Row(Category='VEHICLE THEFT', totalCount=126602, resolveCount=10622)\n",
      "Row(Category='DRUG/NARCOTIC', totalCount=119628, resolveCount=109357)\n",
      "Row(Category='VANDALISM', totalCount=116059, resolveCount=14169)\n",
      "Row(Category='WARRANTS', totalCount=101379, resolveCount=95897)\n",
      "Row(Category='BURGLARY', totalCount=91543, resolveCount=14890)\n",
      "Row(Category='SUSPICIOUS OCC', totalCount=80444, resolveCount=9458)\n",
      "10\n",
      "Finished. it's been 2 seconds\n"
     ]
    }
   ],
   "source": [
    "started_at = datetime.now()\n",
    "category = spark.sql(\"SELECT a.Category, b.totalCount, a.resolveCount from (SELECT distinct Category, count(*) as resolveCount FROM POLICE_DF WHERE not Resolution ='NONE' group by Category) a inner join (SELECT distinct Category, count(*) as totalCount FROM POLICE_DF group by Category order by totalCount DESC limit 10) b on a.Category = b.Category order by b.totalCount DESC\").collect()\n",
    "for x in category:\n",
    "    print(x)\n",
    "print(len(category))\n",
    "\n",
    "print(\"Finished. it's been \" + str((datetime.now()-started_at).seconds) + \" seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top 10 category, and their resolution rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(Category='OTHER OFFENSES', count=221514)\n",
      "Row(Category='DRUG/NARCOTIC', count=109357)\n",
      "Row(Category='WARRANTS', count=95897)\n",
      "Row(Category='ASSAULT', count=80947)\n",
      "Row(Category='NON-CRIMINAL', count=53465)\n",
      "Row(Category='LARCENY/THEFT', count=42521)\n",
      "Row(Category='MISSING PERSON', count=34672)\n",
      "Row(Category='WEAPON LAWS', count=16164)\n",
      "Row(Category='PROSTITUTION', count=15851)\n",
      "Row(Category='BURGLARY', count=14890)\n",
      "10\n",
      "Finished. it's been 1 seconds\n"
     ]
    }
   ],
   "source": [
    "started_at = datetime.now()\n",
    "category = spark.sql(\"SELECT distinct Category, count(*) as count FROM POLICE_DF WHERE not Resolution ='NONE' group by Category order by count DESC limit 10\").collect()\n",
    "for x in category:\n",
    "    print(x)\n",
    "print(len(category))\n",
    "\n",
    "print(\"Finished. it's been \" + str((datetime.now()-started_at).seconds) + \" seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Relationship between criminal increase rate and solved criminal rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(Category='LARCENY/THEFT', totalCount=480448)\n",
      "Row(Category='OTHER OFFENSES', totalCount=309358)\n",
      "Row(Category='NON-CRIMINAL', totalCount=238323)\n",
      "Row(Category='ASSAULT', totalCount=194694)\n",
      "Row(Category='VEHICLE THEFT', totalCount=126602)\n",
      "Row(Category='DRUG/NARCOTIC', totalCount=119628)\n",
      "Row(Category='VANDALISM', totalCount=116059)\n",
      "Row(Category='WARRANTS', totalCount=101379)\n",
      "Row(Category='BURGLARY', totalCount=91543)\n",
      "Row(Category='SUSPICIOUS OCC', totalCount=80444)\n",
      "10\n",
      "Finished. it's been 1 seconds\n"
     ]
    }
   ],
   "source": [
    "started_at = datetime.now()\n",
    "category = spark.sql(\"SELECT distinct Category, count(*) as totalCount FROM POLICE_DF group by Category order by totalCount DESC limit 10\").collect()\n",
    "for x in category:\n",
    "    print(x)\n",
    "print(len(category))\n",
    "\n",
    "print(\"Finished. it's been \" + str((datetime.now()-started_at).seconds) + \" seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LARCENY/THEFT\n",
      "2003  total = 26393  resolution = 3844\n",
      "2004  total = 24505  resolution = 3389\n",
      "2005  total = 25319  resolution = 3346\n",
      "2006  total = 27352  resolution = 3345\n",
      "2007  total = 25770  resolution = 2879\n",
      "2008  total = 25807  resolution = 3099\n",
      "2009  total = 25585  resolution = 3014\n",
      "2010  total = 24446  resolution = 2973\n",
      "2011  total = 25905  resolution = 2729\n",
      "2012  total = 30976  resolution = 2358\n",
      "2013  total = 36412  resolution = 2777\n",
      "2014  total = 38003  resolution = 2367\n",
      "2015  total = 42068  resolution = 2135\n",
      "2016  total = 40449  resolution = 1837\n",
      "2017  total = 47826  resolution = 1882\n",
      "2018  total = 13632  resolution = 547\n",
      "OTHER OFFENSES\n",
      "2003  total = 21232  resolution = 16295\n",
      "2004  total = 20710  resolution = 15511\n",
      "2005  total = 17834  resolution = 12901\n",
      "2006  total = 18306  resolution = 12894\n",
      "2007  total = 19763  resolution = 14910\n",
      "2008  total = 23457  resolution = 18447\n",
      "2009  total = 24693  resolution = 19452\n",
      "2010  total = 20990  resolution = 15295\n",
      "2011  total = 19552  resolution = 14167\n",
      "2012  total = 18646  resolution = 12436\n",
      "2013  total = 19480  resolution = 13917\n",
      "2014  total = 20740  resolution = 14548\n",
      "2015  total = 20382  resolution = 13409\n",
      "2016  total = 19689  resolution = 12634\n",
      "2017  total = 18316  resolution = 11316\n",
      "2018  total = 5568  resolution = 3382\n",
      "NON-CRIMINAL\n",
      "2003  total = 13149  resolution = 1744\n",
      "2004  total = 13778  resolution = 2074\n",
      "2005  total = 14055  resolution = 2326\n",
      "2006  total = 13368  resolution = 2846\n",
      "2007  total = 12677  resolution = 2955\n",
      "2008  total = 12303  resolution = 2913\n",
      "2009  total = 12395  resolution = 2790\n",
      "2010  total = 13877  resolution = 3738\n",
      "2011  total = 15586  resolution = 4610\n",
      "2012  total = 16936  resolution = 4928\n",
      "2013  total = 21084  resolution = 8795\n",
      "2014  total = 19404  resolution = 6516\n",
      "2015  total = 19177  resolution = 2978\n",
      "2016  total = 17919  resolution = 2029\n",
      "2017  total = 17368  resolution = 1667\n",
      "2018  total = 5247  resolution = 556\n",
      "ASSAULT\n",
      "2003  total = 13461  resolution = 5551\n",
      "2004  total = 12899  resolution = 5034\n",
      "2005  total = 11601  resolution = 4142\n",
      "2006  total = 12449  resolution = 4606\n",
      "2007  total = 12518  resolution = 4846\n",
      "2008  total = 12681  resolution = 5141\n",
      "2009  total = 12284  resolution = 5529\n",
      "2010  total = 12387  resolution = 5660\n",
      "2011  total = 12279  resolution = 5605\n",
      "2012  total = 12181  resolution = 4997\n",
      "2013  total = 12580  resolution = 6419\n",
      "2014  total = 12402  resolution = 5253\n",
      "2015  total = 13115  resolution = 5316\n",
      "2016  total = 13603  resolution = 5647\n",
      "2017  total = 13655  resolution = 5388\n",
      "2018  total = 4599  resolution = 1813\n",
      "VEHICLE THEFT\n",
      "2003  total = 15325  resolution = 1339\n",
      "2004  total = 17884  resolution = 1454\n",
      "2005  total = 18194  resolution = 1561\n",
      "2006  total = 7291  resolution = 673\n",
      "2007  total = 6460  resolution = 537\n",
      "2008  total = 6053  resolution = 508\n",
      "2009  total = 5183  resolution = 431\n",
      "2010  total = 4346  resolution = 352\n",
      "2011  total = 4762  resolution = 398\n",
      "2012  total = 6183  resolution = 408\n",
      "2013  total = 6241  resolution = 521\n",
      "2014  total = 7108  resolution = 579\n",
      "2015  total = 7943  resolution = 575\n",
      "2016  total = 6422  resolution = 575\n",
      "2017  total = 5732  resolution = 563\n",
      "2018  total = 1475  resolution = 148\n",
      "DRUG/NARCOTIC\n",
      "2003  total = 9917  resolution = 9334\n",
      "2004  total = 9897  resolution = 9040\n",
      "2005  total = 8533  resolution = 7585\n",
      "2006  total = 9069  resolution = 8157\n",
      "2007  total = 10560  resolution = 9900\n",
      "2008  total = 11648  resolution = 10844\n",
      "2009  total = 11950  resolution = 11275\n",
      "2010  total = 9205  resolution = 8218\n",
      "2011  total = 6935  resolution = 6240\n",
      "2012  total = 6444  resolution = 5539\n",
      "2013  total = 6775  resolution = 6204\n",
      "2014  total = 5408  resolution = 4969\n",
      "2015  total = 4251  resolution = 3853\n",
      "2016  total = 4246  resolution = 3865\n",
      "2017  total = 3308  resolution = 3022\n",
      "2018  total = 1482  resolution = 1312\n",
      "VANDALISM\n",
      "2003  total = 6448  resolution = 765\n",
      "2004  total = 6496  resolution = 748\n",
      "2005  total = 7013  resolution = 644\n",
      "2006  total = 7688  resolution = 701\n",
      "2007  total = 7566  resolution = 893\n",
      "2008  total = 7342  resolution = 932\n",
      "2009  total = 7604  resolution = 961\n",
      "2010  total = 7934  resolution = 1152\n",
      "2011  total = 7243  resolution = 1045\n",
      "2012  total = 7808  resolution = 903\n",
      "2013  total = 6921  resolution = 1035\n",
      "2014  total = 7165  resolution = 978\n",
      "2015  total = 7675  resolution = 1003\n",
      "2016  total = 8595  resolution = 1022\n",
      "2017  total = 9765  resolution = 1054\n",
      "2018  total = 2796  resolution = 333\n",
      "WARRANTS\n",
      "2003  total = 9079  resolution = 8902\n",
      "2004  total = 8114  resolution = 7870\n",
      "2005  total = 6708  resolution = 6165\n",
      "2006  total = 6498  resolution = 6047\n",
      "2007  total = 7105  resolution = 6903\n",
      "2008  total = 5798  resolution = 5602\n",
      "2009  total = 5764  resolution = 5619\n",
      "2010  total = 6187  resolution = 5786\n",
      "2011  total = 6311  resolution = 5888\n",
      "2012  total = 6300  resolution = 5670\n",
      "2013  total = 7362  resolution = 6986\n",
      "2014  total = 6726  resolution = 6339\n",
      "2015  total = 6815  resolution = 6287\n",
      "2016  total = 5974  resolution = 5615\n",
      "2017  total = 5020  resolution = 4742\n",
      "2018  total = 1618  resolution = 1476\n",
      "BURGLARY\n",
      "2003  total = 6047  resolution = 840\n",
      "2004  total = 6753  resolution = 857\n",
      "2005  total = 7071  resolution = 895\n",
      "2006  total = 7004  resolution = 833\n",
      "2007  total = 5454  resolution = 778\n",
      "2008  total = 5679  resolution = 775\n",
      "2009  total = 5379  resolution = 844\n",
      "2010  total = 4966  resolution = 931\n",
      "2011  total = 4987  resolution = 1108\n",
      "2012  total = 6243  resolution = 1078\n",
      "2013  total = 6195  resolution = 1319\n",
      "2014  total = 6066  resolution = 1219\n",
      "2015  total = 5931  resolution = 1016\n",
      "2016  total = 5813  resolution = 1049\n",
      "2017  total = 5857  resolution = 994\n",
      "2018  total = 2098  resolution = 354\n",
      "SUSPICIOUS OCC\n",
      "2003  total = 4196  resolution = 370\n",
      "2004  total = 4489  resolution = 360\n",
      "2005  total = 4693  resolution = 550\n",
      "2006  total = 4775  resolution = 482\n",
      "2007  total = 4800  resolution = 518\n",
      "2008  total = 4751  resolution = 469\n",
      "2009  total = 4627  resolution = 512\n",
      "2010  total = 6004  resolution = 907\n",
      "2011  total = 6207  resolution = 868\n",
      "2012  total = 5860  resolution = 778\n",
      "2013  total = 5677  resolution = 989\n",
      "2014  total = 5230  resolution = 735\n",
      "2015  total = 5500  resolution = 731\n",
      "2016  total = 5802  resolution = 576\n",
      "2017  total = 6119  resolution = 476\n",
      "2018  total = 1714  resolution = 137\n",
      "Finished. it's been 22 seconds\n"
     ]
    }
   ],
   "source": [
    "started_at = datetime.now()\n",
    "lists = list()\n",
    "for x in category:\n",
    "    types = x.Category\n",
    "    yearResolution = spark.sql(\"SELECT a.year, a.totalCount, b.resolveCount from (SELECT substring(date, 7, 4 ) year, count(*) as totalCount FROM POLICE_DF WHERE Category = '%s' group by year) a inner join (SELECT substring(date, 7, 4 ) year, count(*) as resolveCount FROM POLICE_DF WHERE (not Resolution ='NONE') and Category = '%s' group by year )b on a.year = b.year order by a.year \" %(types,types)).collect()\n",
    "    print(types)\n",
    "    lists.append(yearResolution)\n",
    "    for x in yearResolution:\n",
    "        print(str(x.year) +'  total = ' + str(x.totalCount) + '  resolution = '+str(x.resolveCount))\n",
    "# for x in category:\n",
    "#     print(x)\n",
    "# print(len(category))\n",
    "\n",
    "print(\"Finished. it's been \" + str((datetime.now()-started_at).seconds) + \" seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2003  0.14564467851324214\n",
      "2004  0.13829830646806773\n",
      "2005  0.1321537185512856\n",
      "2006  0.12229453056449255\n",
      "2007  0.11171905316259216\n",
      "2008  0.1200836982214128\n",
      "2009  0.11780340042993942\n",
      "2010  0.12161498813711855\n",
      "2011  0.10534645821270025\n",
      "2012  0.07612345041322315\n",
      "2013  0.07626606613204438\n",
      "2014  0.06228455648238297\n",
      "2015  0.05075116478083103\n",
      "2016  0.045415214220376275\n",
      "2017  0.03935098063814661\n",
      "2018  0.04012617370892019\n",
      "*****************\n",
      "2003  0.7674736247174077\n",
      "2004  0.7489618541767262\n",
      "2005  0.7233935179993272\n",
      "2006  0.7043592264831203\n",
      "2007  0.7544401153671001\n",
      "2008  0.7864177004732062\n",
      "2009  0.7877536143846434\n",
      "2010  0.7286803239637922\n",
      "2011  0.7245806055646481\n",
      "2012  0.6669526976295184\n",
      "2013  0.7144250513347022\n",
      "2014  0.7014464802314369\n",
      "2015  0.6578844078108135\n",
      "2016  0.6416780943674133\n",
      "2017  0.6178204848220136\n",
      "2018  0.6073994252873564\n",
      "*****************\n",
      "2003  0.13263366035439958\n",
      "2004  0.1505298301640296\n",
      "2005  0.16549270722162931\n",
      "2006  0.21289646918013166\n",
      "2007  0.23309931371775658\n",
      "2008  0.23677151914167277\n",
      "2009  0.22509076240419523\n",
      "2010  0.26936657779058876\n",
      "2011  0.2957782625433081\n",
      "2012  0.29097779877184693\n",
      "2013  0.41714095996964523\n",
      "2014  0.3358070500927644\n",
      "2015  0.15529019137508474\n",
      "2016  0.11323176516546682\n",
      "2017  0.09598111469368954\n",
      "2018  0.10596531351248333\n",
      "*****************\n",
      "2003  0.4123764950598024\n",
      "2004  0.3902628110706256\n",
      "2005  0.3570381863632445\n",
      "2006  0.3699895573941682\n",
      "2007  0.3871225435373063\n",
      "2008  0.405409668007255\n",
      "2009  0.4500976880494953\n",
      "2010  0.4569306531040607\n",
      "2011  0.456470396612102\n",
      "2012  0.4102290452343814\n",
      "2013  0.5102543720190779\n",
      "2014  0.4235607160135462\n",
      "2015  0.40533739992375145\n",
      "2016  0.4151290156583107\n",
      "2017  0.39458073965580376\n",
      "2018  0.3942161339421613\n",
      "*****************\n",
      "2003  0.08737357259380098\n",
      "2004  0.08130172220979646\n",
      "2005  0.08579751566450478\n",
      "2006  0.09230558222466054\n",
      "2007  0.08312693498452012\n",
      "2008  0.08392532628448703\n",
      "2009  0.08315647308508586\n",
      "2010  0.08099401748734468\n",
      "2011  0.08357832843343133\n",
      "2012  0.06598738476467735\n",
      "2013  0.08348021150456658\n",
      "2014  0.08145751266178954\n",
      "2015  0.07239078433841117\n",
      "2016  0.08953597010277173\n",
      "2017  0.0982205163991626\n",
      "2018  0.10033898305084746\n",
      "*****************\n",
      "2003  0.9412120600988202\n",
      "2004  0.9134081034656967\n",
      "2005  0.8889019102308684\n",
      "2006  0.8994376447237843\n",
      "2007  0.9375\n",
      "2008  0.9309752747252747\n",
      "2009  0.9435146443514645\n",
      "2010  0.8927756653992396\n",
      "2011  0.8997837058399423\n",
      "2012  0.8595592799503414\n",
      "2013  0.915719557195572\n",
      "2014  0.9188239644970414\n",
      "2015  0.906374970595154\n",
      "2016  0.9102684879886952\n",
      "2017  0.9135429262394196\n",
      "2018  0.8852901484480432\n",
      "*****************\n",
      "2003  0.11864143920595534\n",
      "2004  0.11514778325123153\n",
      "2005  0.09182945957507486\n",
      "2006  0.09118106139438086\n",
      "2007  0.11802802008987576\n",
      "2008  0.1269408880414056\n",
      "2009  0.12638085218306155\n",
      "2010  0.14519788253087976\n",
      "2011  0.14427723319066685\n",
      "2012  0.11565061475409837\n",
      "2013  0.14954486345903772\n",
      "2014  0.13649685973482206\n",
      "2015  0.13068403908794787\n",
      "2016  0.1189063408958697\n",
      "2017  0.10793650793650794\n",
      "2018  0.11909871244635194\n",
      "*****************\n",
      "2003  0.9805044608437052\n",
      "2004  0.9699285186098102\n",
      "2005  0.919051878354204\n",
      "2006  0.9305940289319791\n",
      "2007  0.9715693173821253\n",
      "2008  0.9661952397378406\n",
      "2009  0.9748438584316447\n",
      "2010  0.9351866817520608\n",
      "2011  0.9329741720804944\n",
      "2012  0.9\n",
      "2013  0.9489269220320565\n",
      "2014  0.9424620874219447\n",
      "2015  0.9225238444607483\n",
      "2016  0.939906260462002\n",
      "2017  0.9446215139442231\n",
      "2018  0.9122373300370828\n",
      "*****************\n",
      "2003  0.13891185711923268\n",
      "2004  0.12690656004738635\n",
      "2005  0.12657332767642485\n",
      "2006  0.11893203883495146\n",
      "2007  0.14264759809314265\n",
      "2008  0.13646768797323472\n",
      "2009  0.15690648819483174\n",
      "2010  0.18747482883608538\n",
      "2011  0.22217766192099458\n",
      "2012  0.17267339420150568\n",
      "2013  0.2129136400322841\n",
      "2014  0.20095614902736564\n",
      "2015  0.17130332153093913\n",
      "2016  0.18045759504558748\n",
      "2017  0.16971145637698482\n",
      "2018  0.16873212583412775\n",
      "*****************\n",
      "2003  0.08817921830314586\n",
      "2004  0.08019603475161506\n",
      "2005  0.1171958235670147\n",
      "2006  0.10094240837696335\n",
      "2007  0.10791666666666666\n",
      "2008  0.09871605977688908\n",
      "2009  0.11065485195591096\n",
      "2010  0.1510659560293138\n",
      "2011  0.13984211374254873\n",
      "2012  0.13276450511945392\n",
      "2013  0.174211731548353\n",
      "2014  0.14053537284894838\n",
      "2015  0.13290909090909092\n",
      "2016  0.09927611168562564\n",
      "2017  0.07779048864193495\n",
      "2018  0.07992998833138856\n",
      "*****************\n"
     ]
    }
   ],
   "source": [
    "for x in lists:\n",
    "    for y in x:\n",
    "        print(str(y.year) + '  ' + str(y.resolveCount/ y.totalCount))\n",
    "    print('*****************')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
