{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StructType(List(StructField(Timestamp,LongType,true),StructField(Geohash,StringType,true),StructField(geopotential_height_lltw,FloatType,true),StructField(water_equiv_of_accum_snow_depth_surface,FloatType,true),StructField(drag_coefficient_surface,FloatType,true),StructField(sensible_heat_net_flux_surface,FloatType,true),StructField(categorical_ice_pellets_yes1_no0_surface,FloatType,true),StructField(visibility_surface,FloatType,true),StructField(number_of_soil_layers_in_root_zone_surface,FloatType,true),StructField(categorical_freezing_rain_yes1_no0_surface,FloatType,true),StructField(pressure_reduced_to_msl_msl,FloatType,true),StructField(upward_short_wave_rad_flux_surface,FloatType,true),StructField(relative_humidity_zerodegc_isotherm,FloatType,true),StructField(categorical_snow_yes1_no0_surface,FloatType,true),StructField(u-component_of_wind_tropopause,FloatType,true),StructField(surface_wind_gust_surface,FloatType,true),StructField(total_cloud_cover_entire_atmosphere,FloatType,true),StructField(upward_long_wave_rad_flux_surface,FloatType,true),StructField(land_cover_land1_sea0_surface,FloatType,true),StructField(vegitation_type_as_in_sib_surface,FloatType,true),StructField(v-component_of_wind_pblri,FloatType,true),StructField(albedo_surface,FloatType,true),StructField(lightning_surface,FloatType,true),StructField(ice_cover_ice1_no_ice0_surface,FloatType,true),StructField(convective_inhibition_surface,FloatType,true),StructField(pressure_surface,FloatType,true),StructField(transpiration_stress-onset_soil_moisture_surface,FloatType,true),StructField(soil_porosity_surface,FloatType,true),StructField(vegetation_surface,FloatType,true),StructField(categorical_rain_yes1_no0_surface,FloatType,true),StructField(downward_long_wave_rad_flux_surface,FloatType,true),StructField(planetary_boundary_layer_height_surface,FloatType,true),StructField(soil_type_as_in_zobler_surface,FloatType,true),StructField(geopotential_height_cloud_base,FloatType,true),StructField(friction_velocity_surface,FloatType,true),StructField(maximumcomposite_radar_reflectivity_entire_atmosphere,FloatType,true),StructField(plant_canopy_surface_water_surface,FloatType,true),StructField(v-component_of_wind_maximum_wind,FloatType,true),StructField(geopotential_height_zerodegc_isotherm,FloatType,true),StructField(mean_sea_level_pressure_nam_model_reduction_msl,FloatType,true),StructField(temperature_surface,FloatType,true),StructField(snow_cover_surface,FloatType,true),StructField(geopotential_height_surface,FloatType,true),StructField(convective_available_potential_energy_surface,FloatType,true),StructField(latent_heat_net_flux_surface,FloatType,true),StructField(surface_roughness_surface,FloatType,true),StructField(pressure_maximum_wind,FloatType,true),StructField(temperature_tropopause,FloatType,true),StructField(geopotential_height_pblri,FloatType,true),StructField(pressure_tropopause,FloatType,true),StructField(snow_depth_surface,FloatType,true),StructField(v-component_of_wind_tropopause,FloatType,true),StructField(downward_short_wave_rad_flux_surface,FloatType,true),StructField(u-component_of_wind_maximum_wind,FloatType,true),StructField(wilting_point_surface,FloatType,true),StructField(precipitable_water_entire_atmosphere,FloatType,true),StructField(u-component_of_wind_pblri,FloatType,true),StructField(direct_evaporation_cease_soil_moisture_surface,FloatType,true)))\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, FloatType, LongType, StringType\n",
    "\n",
    "feats = []\n",
    "f = open('features.txt')\n",
    "for line_num, line in enumerate(f):\n",
    "    if line_num == 0:\n",
    "        # Timestamp\n",
    "        feats.append(StructField(line.strip(), LongType(), True))\n",
    "    elif line_num == 1:\n",
    "        # Geohash\n",
    "        feats.append(StructField(line.strip(), StringType(), True))\n",
    "    else:\n",
    "        # Other features\n",
    "        feats.append(StructField(line.strip(), FloatType(), True))\n",
    "    \n",
    "schema = StructType(feats)\n",
    "\n",
    "print(schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = spark.read.format('csv').option('sep', '\\t').schema(schema).load('/Volumes/evo/Datasets/NAM_2015_S/*')\n",
    "# df = spark.read.format('csv').option('sep', '\\t').schema(schema).load('hdfs://orion11:40910/nam_tiny.tdv')\n",
    "# df = spark.read.format('csv').option('sep', '\\t').schema(schema).load('hdfs://orion11:40910/datasets/nam_201501.tdv.gz')\n",
    "\n",
    "df = spark.read.format('csv').option('sep', '\\t').schema(schema).load('hdfs://orion11:37000/data/nam/*')\n",
    "# df_large = spark.read.format('csv').option('sep', '\\t').schema(schema).load('hdfs://orion11:37000/data/nam/nam_201509.tdv.gz')\n",
    "# df_entire = spark.read.format('csv').option('sep', '\\t').schema(schema).load('hdfs://orion11:37000/data/nam/*')\n",
    "\n",
    "# df.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_large = spark.read.format('csv').option('sep', '\\t').schema(schema).load('hdfs://orion11:40910/datasets/nam_201509.tdv.gz')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_entire = spark.read.format('csv').option('sep', '\\t').schema(schema).load('hdfs://orion11:40910/datasets/*')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Timestamp=1426377600000, Geohash='cf7ecr4h2ps0', geopotential_height_lltw=136.53125, water_equiv_of_accum_snow_depth_surface=77.0, drag_coefficient_surface=0.0, sensible_heat_net_flux_surface=-39.57763671875, categorical_ice_pellets_yes1_no0_surface=0.0, visibility_surface=24221.587890625, number_of_soil_layers_in_root_zone_surface=3.0, categorical_freezing_rain_yes1_no0_surface=0.0, pressure_reduced_to_msl_msl=99602.0, upward_short_wave_rad_flux_surface=6.625, relative_humidity_zerodegc_isotherm=34.0, categorical_snow_yes1_no0_surface=0.0, u-component_of_wind_tropopause=27.527877807617188, surface_wind_gust_surface=16.158788681030273, total_cloud_cover_entire_atmosphere=100.0, upward_long_wave_rad_flux_surface=314.0560302734375, land_cover_land1_sea0_surface=1.0, vegitation_type_as_in_sib_surface=18.0, v-component_of_wind_pblri=12.31915283203125, albedo_surface=38.75, lightning_surface=0.0, ice_cover_ice1_no_ice0_surface=0.0, convective_inhibition_surface=-0.65234375, pressure_surface=97221.0, transpiration_stress-onset_soil_moisture_surface=0.3125, soil_porosity_surface=0.5, vegetation_surface=1.0, categorical_rain_yes1_no0_surface=0.0, downward_long_wave_rad_flux_surface=274.9662780761719, planetary_boundary_layer_height_surface=1905.5, soil_type_as_in_zobler_surface=3.0, geopotential_height_cloud_base=6696.0, friction_velocity_surface=0.6008660197257996, maximumcomposite_radar_reflectivity_entire_atmosphere=-20.0, plant_canopy_surface_water_surface=0.10999999940395355, v-component_of_wind_maximum_wind=-7.388824462890625, geopotential_height_zerodegc_isotherm=2380.0, mean_sea_level_pressure_nam_model_reduction_msl=99607.0, temperature_surface=272.7480163574219, snow_cover_surface=100.0, geopotential_height_surface=195.8200225830078, convective_available_potential_energy_surface=0.0, latent_heat_net_flux_surface=-0.0329132080078125, surface_roughness_surface=0.1750158965587616, pressure_maximum_wind=20726.736328125, temperature_tropopause=206.35687255859375, geopotential_height_pblri=300.42779541015625, pressure_tropopause=20582.23828125, snow_depth_surface=0.4211999773979187, v-component_of_wind_tropopause=-7.8085479736328125, downward_short_wave_rad_flux_surface=17.0, u-component_of_wind_maximum_wind=28.66766357421875, wilting_point_surface=0.04749999940395355, precipitable_water_entire_atmosphere=9.207815170288086, u-component_of_wind_pblri=4.711578369140625, direct_evaporation_cease_soil_moisture_surface=0.04749999940395355),\n",
       " Row(Timestamp=1426377600000, Geohash='ccn3u17cxe80', geopotential_height_lltw=4.28125, water_equiv_of_accum_snow_depth_surface=26.0, drag_coefficient_surface=0.0, sensible_heat_net_flux_surface=-12.95263671875, categorical_ice_pellets_yes1_no0_surface=0.0, visibility_surface=24221.587890625, number_of_soil_layers_in_root_zone_surface=4.0, categorical_freezing_rain_yes1_no0_surface=0.0, pressure_reduced_to_msl_msl=101712.0, upward_short_wave_rad_flux_surface=1.75, relative_humidity_zerodegc_isotherm=10.0, categorical_snow_yes1_no0_surface=0.0, u-component_of_wind_tropopause=16.527877807617188, surface_wind_gust_surface=6.90878963470459, total_cloud_cover_entire_atmosphere=0.0, upward_long_wave_rad_flux_surface=303.1810302734375, land_cover_land1_sea0_surface=1.0, vegitation_type_as_in_sib_surface=1.0, v-component_of_wind_pblri=5.63165283203125, albedo_surface=24.5, lightning_surface=0.0, ice_cover_ice1_no_ice0_surface=0.0, convective_inhibition_surface=-0.65234375, pressure_surface=96528.0, transpiration_stress-onset_soil_moisture_surface=0.3125, soil_porosity_surface=0.5, vegetation_surface=15.75, categorical_rain_yes1_no0_surface=0.0, downward_long_wave_rad_flux_surface=231.71627807617188, planetary_boundary_layer_height_surface=407.0, soil_type_as_in_zobler_surface=3.0, geopotential_height_cloud_base=-5000.0, friction_velocity_surface=0.3508659601211548, maximumcomposite_radar_reflectivity_entire_atmosphere=-20.0, plant_canopy_surface_water_surface=0.5, v-component_of_wind_maximum_wind=-34.263824462890625, geopotential_height_zerodegc_isotherm=2440.0, mean_sea_level_pressure_nam_model_reduction_msl=101723.0, temperature_surface=270.3730163574219, snow_cover_surface=100.0, geopotential_height_surface=426.32000732421875, convective_available_potential_energy_surface=0.0, latent_heat_net_flux_surface=-0.0329132080078125, surface_roughness_surface=1.9000158309936523, pressure_maximum_wind=22326.736328125, temperature_tropopause=209.85687255859375, geopotential_height_pblri=328.42779541015625, pressure_tropopause=21382.23828125, snow_depth_surface=0.052799999713897705, v-component_of_wind_tropopause=-33.18354797363281, downward_short_wave_rad_flux_surface=6.875, u-component_of_wind_maximum_wind=15.91766357421875, wilting_point_surface=0.04749999940395355, precipitable_water_entire_atmosphere=5.457814693450928, u-component_of_wind_pblri=2.274078369140625, direct_evaporation_cease_soil_moisture_surface=0.04749999940395355),\n",
       " Row(Timestamp=1426377600000, Geohash='f2fgf6usbe2p', geopotential_height_lltw=-472.96875, water_equiv_of_accum_snow_depth_surface=142.0, drag_coefficient_surface=0.0, sensible_heat_net_flux_surface=-7.07763671875, categorical_ice_pellets_yes1_no0_surface=0.0, visibility_surface=7621.5888671875, number_of_soil_layers_in_root_zone_surface=4.0, categorical_freezing_rain_yes1_no0_surface=0.0, pressure_reduced_to_msl_msl=101610.0, upward_short_wave_rad_flux_surface=0.0, relative_humidity_zerodegc_isotherm=77.0, categorical_snow_yes1_no0_surface=0.0, u-component_of_wind_tropopause=20.277877807617188, surface_wind_gust_surface=7.28378963470459, total_cloud_cover_entire_atmosphere=100.0, upward_long_wave_rad_flux_surface=291.5560302734375, land_cover_land1_sea0_surface=1.0, vegitation_type_as_in_sib_surface=1.0, v-component_of_wind_pblri=-1.68084716796875, albedo_surface=39.5, lightning_surface=0.0, ice_cover_ice1_no_ice0_surface=0.0, convective_inhibition_surface=-0.65234375, pressure_surface=97035.0, transpiration_stress-onset_soil_moisture_surface=0.3125, soil_porosity_surface=0.5, vegetation_surface=3.0, categorical_rain_yes1_no0_surface=0.0, downward_long_wave_rad_flux_surface=280.2162780761719, planetary_boundary_layer_height_surface=798.5, soil_type_as_in_zobler_surface=3.0, geopotential_height_cloud_base=1541.0, friction_velocity_surface=0.6258659958839417, maximumcomposite_radar_reflectivity_entire_atmosphere=11.25, plant_canopy_surface_water_surface=0.25999999046325684, v-component_of_wind_maximum_wind=-1.513824462890625, geopotential_height_zerodegc_isotherm=0.0, mean_sea_level_pressure_nam_model_reduction_msl=101625.0, temperature_surface=267.7480163574219, snow_cover_surface=100.0, geopotential_height_surface=362.32000732421875, convective_available_potential_energy_surface=0.0, latent_heat_net_flux_surface=12.717086791992188, surface_roughness_surface=1.9000158309936523, pressure_maximum_wind=12126.736328125, temperature_tropopause=219.73187255859375, geopotential_height_pblri=193.92779541015625, pressure_tropopause=25782.23828125, snow_depth_surface=0.5703999996185303, v-component_of_wind_tropopause=10.816452026367188, downward_short_wave_rad_flux_surface=0.0, u-component_of_wind_maximum_wind=30.91766357421875, wilting_point_surface=0.04749999940395355, precipitable_water_entire_atmosphere=9.082815170288086, u-component_of_wind_pblri=-5.475921630859375, direct_evaporation_cease_soil_moisture_surface=0.04749999940395355),\n",
       " Row(Timestamp=1426377600000, Geohash='9x786rfxxpzz', geopotential_height_lltw=3072.03125, water_equiv_of_accum_snow_depth_surface=0.0, drag_coefficient_surface=0.0, sensible_heat_net_flux_surface=-18.82763671875, categorical_ice_pellets_yes1_no0_surface=0.0, visibility_surface=24221.587890625, number_of_soil_layers_in_root_zone_surface=3.0, categorical_freezing_rain_yes1_no0_surface=0.0, pressure_reduced_to_msl_msl=102109.0, upward_short_wave_rad_flux_surface=29.25, relative_humidity_zerodegc_isotherm=49.0, categorical_snow_yes1_no0_surface=0.0, u-component_of_wind_tropopause=9.652877807617188, surface_wind_gust_surface=5.90878963470459, total_cloud_cover_entire_atmosphere=70.0, upward_long_wave_rad_flux_surface=344.6810302734375, land_cover_land1_sea0_surface=1.0, vegitation_type_as_in_sib_surface=10.0, v-component_of_wind_pblri=5.06915283203125, albedo_surface=19.25, lightning_surface=0.0, ice_cover_ice1_no_ice0_surface=0.0, convective_inhibition_surface=-0.65234375, pressure_surface=78016.0, transpiration_stress-onset_soil_moisture_surface=0.32999998331069946, soil_porosity_surface=0.5, vegetation_surface=3.0, categorical_rain_yes1_no0_surface=0.0, downward_long_wave_rad_flux_surface=262.8412780761719, planetary_boundary_layer_height_surface=2201.0, soil_type_as_in_zobler_surface=6.0, geopotential_height_cloud_base=8044.0, friction_velocity_surface=0.3258659839630127, maximumcomposite_radar_reflectivity_entire_atmosphere=-8.3125, plant_canopy_surface_water_surface=0.47999998927116394, v-component_of_wind_maximum_wind=-44.763824462890625, geopotential_height_zerodegc_isotherm=3440.0, mean_sea_level_pressure_nam_model_reduction_msl=101683.0, temperature_surface=279.1230163574219, snow_cover_surface=100.0, geopotential_height_surface=2258.570068359375, convective_available_potential_energy_surface=0.0, latent_heat_net_flux_surface=25.217086791992188, surface_roughness_surface=0.1750158965587616, pressure_maximum_wind=20326.736328125, temperature_tropopause=209.85687255859375, geopotential_height_pblri=193.67779541015625, pressure_tropopause=17782.23828125, snow_depth_surface=0.0007999999797903001, v-component_of_wind_tropopause=-40.30854797363281, downward_short_wave_rad_flux_surface=152.375, u-component_of_wind_maximum_wind=5.04266357421875, wilting_point_surface=0.06624999642372131, precipitable_water_entire_atmosphere=6.082814693450928, u-component_of_wind_pblri=5.836578369140625, direct_evaporation_cease_soil_moisture_surface=0.06624999642372131),\n",
       " Row(Timestamp=1426377600000, Geohash='cf36gb2z345b', geopotential_height_lltw=315.28125, water_equiv_of_accum_snow_depth_surface=82.0, drag_coefficient_surface=0.0, sensible_heat_net_flux_surface=-39.32763671875, categorical_ice_pellets_yes1_no0_surface=0.0, visibility_surface=24221.587890625, number_of_soil_layers_in_root_zone_surface=3.0, categorical_freezing_rain_yes1_no0_surface=0.0, pressure_reduced_to_msl_msl=99229.0, upward_short_wave_rad_flux_surface=7.875, relative_humidity_zerodegc_isotherm=23.0, categorical_snow_yes1_no0_surface=0.0, u-component_of_wind_tropopause=24.402877807617188, surface_wind_gust_surface=14.65878963470459, total_cloud_cover_entire_atmosphere=100.0, upward_long_wave_rad_flux_surface=315.6810302734375, land_cover_land1_sea0_surface=1.0, vegitation_type_as_in_sib_surface=18.0, v-component_of_wind_pblri=11.38165283203125, albedo_surface=35.0, lightning_surface=0.0, ice_cover_ice1_no_ice0_surface=0.0, convective_inhibition_surface=-0.65234375, pressure_surface=95390.0, transpiration_stress-onset_soil_moisture_surface=0.3125, soil_porosity_surface=0.5, vegetation_surface=1.0, categorical_rain_yes1_no0_surface=0.0, downward_long_wave_rad_flux_surface=310.3412780761719, planetary_boundary_layer_height_surface=1036.0, soil_type_as_in_zobler_surface=3.0, geopotential_height_cloud_base=1977.0, friction_velocity_surface=0.6758660078048706, maximumcomposite_radar_reflectivity_entire_atmosphere=8.8125, plant_canopy_surface_water_surface=0.13499999046325684, v-component_of_wind_maximum_wind=-11.763824462890625, geopotential_height_zerodegc_isotherm=2680.0, mean_sea_level_pressure_nam_model_reduction_msl=99250.0, temperature_surface=273.1230163574219, snow_cover_surface=100.0, geopotential_height_surface=321.32000732421875, convective_available_potential_energy_surface=0.0, latent_heat_net_flux_surface=9.092086791992188, surface_roughness_surface=0.1750158965587616, pressure_maximum_wind=11126.736328125, temperature_tropopause=208.10687255859375, geopotential_height_pblri=258.67779541015625, pressure_tropopause=20782.23828125, snow_depth_surface=0.3779999911785126, v-component_of_wind_tropopause=6.4414520263671875, downward_short_wave_rad_flux_surface=22.75, u-component_of_wind_maximum_wind=27.04266357421875, wilting_point_surface=0.04749999940395355, precipitable_water_entire_atmosphere=10.332815170288086, u-component_of_wind_pblri=0.274078369140625, direct_evaporation_cease_soil_moisture_surface=0.04749999940395355)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df.filter(df.snow_depth_surface != 0).take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1. [1 pt] Strangely Snowy: Find a location that contains snow while its surroundings do not. Why does this occur? Is it a high mountain peak in a desert?Â¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(Geohash='c1gxsefbz52p', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c43hd76g0npb', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c43g3722yrzz', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c1nt970j5b5b', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c1grzzxwt1bp', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c436f3k4p8xb', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c43jhzrrsbrz', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c44hk1hy9u2p', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c46sf3s53p80', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c43gkzxxn6bp', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c41tugrtwhzz', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c458r20zx080', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c43gb4ywdgrz', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c438x5esgf00', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c4459s8rz1xb', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c43c1zbmntbp', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c43e1f64v9bp', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c41z4k0q1v00', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c43uwe7z04zz', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c44n20bvf7pb', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c41uxkww12rz', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c439bnrmtb80', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c1q28yjg8080', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c1nt41ctyzeb', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c432rdwup080', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c43esqrrbgpb', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c1gr1wvsqzrz', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c44nftm8u1zz', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c1gnx42ddfpb', avg(snow_cover_surface)=99.91358024691358)\n",
      "Row(Geohash='c43ejwc60zzz', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c41z9hnc43rz', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c452rpbu2mbp', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c1nmmz1m37rz', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c1nrnydywy00', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c44pt3ddqnpb', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c43cukjs4q2p', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c304zyed5j5b', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c43brsccqvzz', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c460e30rt8xb', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c43m2snbpzbp', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c43hwpfhs9zz', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c1gyvgbcd7rz', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c43kp9tv6krz', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c1grm74n9f00', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c43ddfshx9zz', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c1gzku493krz', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c1gx2zw2p3bp', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c43upev6nkzz', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c44pbsgr0r2p', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c43d5fyjw72p', avg(snow_cover_surface)=99.90123456790124)\n",
      "Row(Geohash='c41t9qfhk9rz', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c433u9bub3bp', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c44jutrxvc2p', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c1q0t6n70g00', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c41xbfbfkp80', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c436yjg4v2rz', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c438fqgmsm00', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c1pcwbxkywrz', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c1gxje99en80', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c43fx06cyhrz', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c437b0xke580', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c439n53vsxzz', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c43dwx55yr2p', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c4591sqdmz2p', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c1nqxts9reeb', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c43cmkr5gvzz', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c1nkytgn25h0', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c1p5fmbjmkrz', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c460wvwuzxzz', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c43673wec2pb', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c43sdmg10700', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c1gzb0p913rz', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c1p89bngnb00', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c4645ugrspzz', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c41vbxvq6k2p', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c3k8v0znz280', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c43k6uu1egxb', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c45b6umxp9xb', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c41uhb4r5n00', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c3hzccj7b75b', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c41ueb1jyypb', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c3048znkdueb', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c1pe3dez7y5b', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c41w0qb2mxxb', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c445v1jx5z2p', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c4387rjkb0rz', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c3k0yw8m6us0', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c3k3419762xb', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c43b05v7222p', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c41wrxgfefpb', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c436rnjk6zxb', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c43hr0kbbvpb', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c460h33t7dpb', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c43u6zh1sg80', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c4585eur3580', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c307564rcz7z', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c458dsjt0xxb', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c1pfn4p9hy5b', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c437gvnt3d00', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c464r96ksbxb', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c44h0kxx1rxb', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c36u4wfm3jup', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c41gtb5rubpb', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c44p3thkr700', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c1uhcd9k1n2p', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c44hyv9kjj00', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c1gyqex11wpb', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c43ggzmuxn00', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c3k2d8k3h7pb', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c46190vf50rz', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c41xurr50ypb', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c41zubt02prz', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c3041ty0ftpb', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c1nuq5290jup', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c41yek3dwk2p', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c1gy5p6c9n2p', avg(snow_cover_surface)=99.98765432098766)\n",
      "Row(Geohash='c41zmbwmedpb', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c41w7gq1ggpb', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c30154vrv1xb', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c36uq45f8f00', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c1snbcp98xzz', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c43j14zyhszz', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c44hg15vqhxb', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c44wfrncngpb', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c43kcu3t702p', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c46400s6m9xb', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c1gqdwstr52p', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c1gwp7n9ecbp', avg(snow_cover_surface)=99.98765432098766)\n",
      "Row(Geohash='c301dkmu8u2p', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c4658sph98xb', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c44n7tqsw5xb', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c43e8f003n00', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c43ff7d4xz2p', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c4393q9myw00', avg(snow_cover_surface)=99.90123456790124)\n",
      "Row(Geohash='c1nkrkswv2xb', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c1gzguk0t9bp', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c439t50n482p', avg(snow_cover_surface)=99.90123456790124)\n",
      "Row(Geohash='c1gru77j5fzz', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c41v48pupf00', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c43s5qjhy0xb', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c43h57tx92xb', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c1snut3f7mbp', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c41yybzdh1zz', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c1gqy6bzqnpb', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c41v98n9w0xb', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c43f77ggy1rz', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c4r5m1vqf07z', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c1p48r7t4qh0', avg(snow_cover_surface)=100.0)\n",
      "Row(Geohash='c1gz1py1burz', avg(snow_cover_surface)=100.0)\n",
      "149\n"
     ]
    }
   ],
   "source": [
    "df_large.createOrReplaceTempView(\"TEMP_DF\")\n",
    "\n",
    "started_at = datetime.now()\n",
    "\n",
    "not_snow_all_year = spark.sql(\"SELECT  Geohash,avg(snow_cover_surface) FROM TEMP_DF group by Geohash having avg(snow_cover_surface) >= 99\").collect()\n",
    "\n",
    "for x in not_snow_all_year:\n",
    "    print(x)\n",
    "print(len(not_snow_all_year))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c1gx\n",
      "c43h\n",
      "c43g\n",
      "c1nt\n",
      "c1gr\n",
      "c436\n",
      "c43j\n",
      "c44h\n",
      "c46s\n",
      "c43g\n",
      "c41t\n",
      "c458\n",
      "c43g\n",
      "c438\n",
      "c445\n",
      "c43c\n",
      "c43e\n",
      "c41z\n",
      "c43u\n",
      "c44n\n",
      "c41u\n",
      "c439\n",
      "c1q2\n",
      "c1nt\n",
      "c432\n",
      "c43e\n",
      "c1gr\n",
      "c44n\n",
      "c1gn\n",
      "c43e\n",
      "c41z\n",
      "c452\n",
      "c1nm\n",
      "c1nr\n",
      "c44p\n",
      "c43c\n",
      "c304\n",
      "c43b\n",
      "c460\n",
      "c43m\n",
      "c43h\n",
      "c1gy\n",
      "c43k\n",
      "c1gr\n",
      "c43d\n",
      "c1gz\n",
      "c1gx\n",
      "c43u\n",
      "c44p\n",
      "c43d\n",
      "c41t\n",
      "c433\n",
      "c44j\n",
      "c1q0\n",
      "c41x\n",
      "c436\n",
      "c438\n",
      "c1pc\n",
      "c1gx\n",
      "c43f\n",
      "c437\n",
      "c439\n",
      "c43d\n",
      "c459\n",
      "c1nq\n",
      "c43c\n",
      "c1nk\n",
      "c1p5\n",
      "c460\n",
      "c436\n",
      "c43s\n",
      "c1gz\n",
      "c1p8\n",
      "c464\n",
      "c41v\n",
      "c3k8\n",
      "c43k\n",
      "c45b\n",
      "c41u\n",
      "c3hz\n",
      "c41u\n",
      "c304\n",
      "c1pe\n",
      "c41w\n",
      "c445\n",
      "c438\n",
      "c3k0\n",
      "c3k3\n",
      "c43b\n",
      "c41w\n",
      "c436\n",
      "c43h\n",
      "c460\n",
      "c43u\n",
      "c458\n",
      "c307\n",
      "c458\n",
      "c1pf\n",
      "c437\n",
      "c464\n",
      "c44h\n",
      "c36u\n",
      "c41g\n",
      "c44p\n",
      "c1uh\n",
      "c44h\n",
      "c1gy\n",
      "c43g\n",
      "c3k2\n",
      "c461\n",
      "c41x\n",
      "c41z\n",
      "c304\n",
      "c1nu\n",
      "c41y\n",
      "c1gy\n",
      "c41z\n",
      "c41w\n",
      "c301\n",
      "c36u\n",
      "c1sn\n",
      "c43j\n",
      "c44h\n",
      "c44w\n",
      "c43k\n",
      "c464\n",
      "c1gq\n",
      "c1gw\n",
      "c301\n",
      "c465\n",
      "c44n\n",
      "c43e\n",
      "c43f\n",
      "c439\n",
      "c1nk\n",
      "c1gz\n",
      "c439\n",
      "c1gr\n",
      "c41v\n",
      "c43s\n",
      "c43h\n",
      "c1sn\n",
      "c41y\n",
      "c1gq\n",
      "c41v\n",
      "c43f\n",
      "c4r5\n",
      "c1p4\n",
      "c1gz\n",
      "149\n",
      "74\n"
     ]
    }
   ],
   "source": [
    "four_set = set()\n",
    "for x in not_snow_all_year:\n",
    "    print(x.Geohash[0:4])\n",
    "    four_set.add(x.Geohash[0:4])\n",
    "print(len(not_snow_all_year))\n",
    "print(len(four_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c3hz%\n",
      "4\n",
      "c43g%\n",
      "5\n",
      "c43m%\n",
      "3\n",
      "c1p4%\n",
      "5\n",
      "c41z%\n",
      "4\n",
      "c36u%\n",
      "4\n",
      "c1nr%\n",
      "4\n",
      "c460%\n",
      "3\n",
      "c1p8%\n",
      "3\n",
      "c41g%\n",
      "4\n",
      "c436%\n",
      "4\n",
      "c1nt%\n",
      "4\n",
      "c433%\n",
      "4\n",
      "c41x%\n",
      "4\n",
      "c452%\n",
      "3\n",
      "c1q0%\n",
      "4\n",
      "c3k8%\n",
      "4\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-b2bffffb2a20>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfour_set\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mparam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'%'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mgroup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"SELECT distinct Geohash,avg(snow_cover_surface) as avgSnow FROM TEMP_DF WHERE Geohash like '%s' group by Geohash\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0mparam\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark-2.3.1-bin-hadoop2.7/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    464\u001b[0m         \"\"\"\n\u001b[1;32m    465\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectToPython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    467\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBatchedSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPickleSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark-2.3.1-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1253\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1255\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1257\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m/usr/local/spark-2.3.1-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m    983\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 985\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark-2.3.1-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1152\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1153\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home2/anaconda3/lib/python3.6/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# df.createOrReplaceTempView(\"TEMP_DF\")\n",
    "groupList = list()\n",
    "for y in four_set:\n",
    "        param = y + '%'\n",
    "        group = spark.sql(\"SELECT distinct Geohash,avg(snow_cover_surface) as avgSnow FROM TEMP_DF WHERE Geohash like '%s' group by Geohash\" %param ).collect()\n",
    "        print(param ) \n",
    "        print(len(group))\n",
    "        groupList.append(group)\n",
    "        \n",
    "print(len(groupList))\n",
    "print('Complete')\n",
    "print(\"Finished. it's been \" + str((datetime.now()-started_at).seconds) + \" seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(Geohash='c1uhvnv5p8pb', avgSnow=7.283950617283951), Row(Geohash='c1uh66r1r02p', avgSnow=9.74074074074074), Row(Geohash='c1uhcd9k1n2p', avgSnow=100.0), Row(Geohash='c1uhqne01400', avgSnow=2.9135802469135803)]\n",
      "[Row(Geohash='c3k0fgxqugeb', avgSnow=10.790123456790123), Row(Geohash='c3k0yw8m6us0', avgSnow=100.0), Row(Geohash='c3k0rj7e0zs0', avgSnow=21.506172839506174), Row(Geohash='c3k07d6swzkp', avgSnow=4.08641975308642), Row(Geohash='c3k00p3vw3gz', avgSnow=4.08641975308642)]\n",
      "Finished. it's been 29050 seconds\n"
     ]
    }
   ],
   "source": [
    "for x in groupList:\n",
    "    avg=0\n",
    "    for y in x:\n",
    "        avg= avg + y.avgSnow\n",
    "    avg= avg/len(x)\n",
    "    if avg<30:\n",
    "        print(x)\n",
    "#     print(avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c1uhcd9k1n2p is a mountain peak(lavender peak)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2. [1 pt] Lightning rod: Where are you most likely to be struck by lightning? Use a precision of at least 4 Geohash characters and provide the top 3 locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(Geohash='9eqepuxk7x20', avgLighting=0.3952922077922078)\n",
      "Row(Geohash='9g3h968ygj7z', avgLighting=0.3871753246753247)\n",
      "Row(Geohash='9g3y52sgeceb', avgLighting=0.3758116883116883)\n",
      "Row(Geohash='9g3ug8ckk4hp', avgLighting=0.3741883116883117)\n",
      "Row(Geohash='9g3v7kxpuhh0', avgLighting=0.3741883116883117)\n",
      "Row(Geohash='9g3m79nrf3zb', avgLighting=0.37337662337662336)\n",
      "Row(Geohash='9g3mq3f7y6eb', avgLighting=0.3685064935064935)\n",
      "Row(Geohash='9g2vntcg8c2p', avgLighting=0.35957792207792205)\n",
      "Row(Geohash='9g3hegkvh55z', avgLighting=0.35714285714285715)\n",
      "Row(Geohash='9g35cqggdn5z', avgLighting=0.35714285714285715)\n",
      "Finished. it's been 780 seconds\n"
     ]
    }
   ],
   "source": [
    "df_entire.createOrReplaceTempView(\"TEMP_DF\")\n",
    "started_at = datetime.now()\n",
    "lightning = spark.sql(\"SELECT  Geohash,avg(lightning_surface) as avgLighting FROM TEMP_DF group by Geohash order by avgLighting DESC Limit 10 \").collect()\n",
    "\n",
    "for x in lightning:\n",
    "    print(x)\n",
    "print(\"Finished. it's been \" + str((datetime.now()-started_at).seconds) + \" seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I calculate the average lightning chances. The top 3 location is 9eqe, 9g3h, 9g3y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3. [1 pt] Drying out: Choose a region in North America (defined by one or more Geohashes) and determine when its driest month is. This should include a histogram with data from each month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "dfList = list()\n",
    "for x in range(1,13):\n",
    "    if x<10:\n",
    "        x = '0'+str(x)\n",
    "    path = \"hdfs://orion11:40910/datasets/nam_2015%s.tdv.gz\" %x\n",
    "#     print(path)\n",
    "    df_1 = spark.read.format('csv').option('sep', '\\t').schema(schema).load(path)\n",
    "    dfList.append(df_1)\n",
    "print(len(dfList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(subGeo='9q9n', avgRain=0.0)\n",
      "Row(subGeo='9q9n', avgRain=0.0603448275862069)\n",
      "Row(subGeo='9q9n', avgRain=0.0)\n",
      "Row(subGeo='9q9n', avgRain=0.0777027027027027)\n",
      "Row(subGeo='9q9n', avgRain=0.020161290322580645)\n",
      "Row(subGeo='9q9n', avgRain=0.0022935779816513763)\n",
      "Row(subGeo='9q9n', avgRain=0.01639344262295082)\n",
      "Row(subGeo='9q9n', avgRain=0.008333333333333333)\n",
      "Row(subGeo='9q9n', avgRain=0.009259259259259259)\n",
      "Row(subGeo='9q9n', avgRain=0.0021551724137931034)\n",
      "Row(subGeo='9q9n', avgRain=0.0546218487394958)\n",
      "Row(subGeo='9q9n', avgRain=0.0872093023255814)\n",
      "Finished. it's been 3060 seconds\n"
     ]
    }
   ],
   "source": [
    "#9q9n categorical_rain_yes1_no0_surface\n",
    "from pyspark.sql.functions import substring\n",
    "started_at = datetime.now()\n",
    "for df_1 in dfList:  \n",
    "    df_1.createOrReplaceTempView(\"TEMP_DF\")\n",
    "    dry_place = spark.sql(\"SELECT  substring(Geohash, 1, 4) as subGeo, AVG(categorical_rain_yes1_no0_surface) as avgRain FROM TEMP_DF group by subGeo having subGeo = '9q9n'\").collect()\n",
    "    for x in dry_place:\n",
    "        print(x)\n",
    "print(\"Finished. it's been \" + str((datetime.now()-started_at).seconds) + \" seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the avgerage rain chances for Hayward. January, March do not rain at all. \n",
    "<br>Bay Area<br>\n",
    "![](./images/bayArea.jpg)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4. [2 pt] Travel Startup: After graduating from USF, you found a startup that aims to provide personalized travel itineraries using big data analysis. Given your own personal preferences, build a plan for a year of travel across 5 locations. Or, in other words: pick 5 regions. What is the best time of year to visit them based on the dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "dfList = list()\n",
    "for x in range(1,13):\n",
    "    if x<10:\n",
    "        x = '0'+str(x)\n",
    "    path = \"hdfs://orion11:37000/data/nam/nam_2015%s.tdv.gz\" %x\n",
    "#     print(path)\n",
    "    df_1 = spark.read.format('csv').option('sep', '\\t').schema(schema).load(path)\n",
    "    dfList.append(df_1)\n",
    "print(len(dfList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "month: 1\n",
      "Row(Timestamp=1420178400000, Geohash='8vkurbww01b0', avgtemper=293.17681884765625, avghum=48.0)\n",
      "THI1 value is: 65.07032812500003\n",
      "Row(Timestamp=1420178400000, Geohash='9usrkvqdgzez', avgtemper=295.30181884765625, avghum=15.0)\n",
      "THI1 value is: 65.19922790527346\n",
      "Row(Timestamp=1420178400000, Geohash='8uxh1ktytx00', avgtemper=294.80181884765625, avghum=21.0)\n",
      "THI1 value is: 65.16533703613283\n",
      "Row(Timestamp=1420178400000, Geohash='8uz0geczqhpz', avgtemper=294.42681884765625, avghum=27.0)\n",
      "THI1 value is: 65.20519616699221\n",
      "Row(Timestamp=1420178400000, Geohash='9kt3kq4je1kp', avgtemper=293.80181884765625, avghum=43.0)\n",
      "THI1 value is: 65.55273718261722\n",
      "month: 2\n",
      "Row(Timestamp=1422748800000, Geohash='dw11jd7dct5b', avgtemper=294.15985107421875, avghum=89.0)\n",
      "THI1 value is: 69.07964831542972\n",
      "Row(Timestamp=1422748800000, Geohash='dw4ss4qugy6p', avgtemper=293.78485107421875, avghum=91.0)\n",
      "THI1 value is: 68.5725953369141\n",
      "Row(Timestamp=1422748800000, Geohash='9uf5eftyjhpb', avgtemper=294.65985107421875, avghum=81.0)\n",
      "THI1 value is: 69.34786022949223\n",
      "Row(Timestamp=1422748800000, Geohash='9g78j99nhsb0', avgtemper=294.65985107421875, avghum=41.0)\n",
      "THI1 value is: 66.46391979980473\n",
      "Row(Timestamp=1422748800000, Geohash='dtc5yrdffgeb', avgtemper=294.28485107421875, avghum=85.0)\n",
      "THI1 value is: 69.01750427246097\n",
      "month: 3\n",
      "Row(Timestamp=1425362400000, Geohash='95ftk3et6uzz', avgtemper=295.13739013671875, avghum=16.0)\n",
      "THI1 value is: 65.11989453125003\n",
      "Row(Timestamp=1425362400000, Geohash='95gubzsyxy20', avgtemper=294.88739013671875, avghum=18.0)\n",
      "THI1 value is: 65.0286423339844\n",
      "Row(Timestamp=1425362400000, Geohash='95ugg6qeuk8p', avgtemper=295.13739013671875, avghum=16.0)\n",
      "THI1 value is: 65.11989453125003\n",
      "Row(Timestamp=1425362400000, Geohash='95upeftn1wrb', avgtemper=294.63739013671875, avghum=22.0)\n",
      "THI1 value is: 65.07113793945315\n",
      "Row(Timestamp=1425362400000, Geohash='95vjj0zts8b0', avgtemper=295.13739013671875, avghum=20.0)\n",
      "THI1 value is: 65.42739013671877\n",
      "month: 4\n",
      "Row(Timestamp=1427846400000, Geohash='9ezg1krg8kk0', avgtemper=294.2497863769531, avghum=97.0)\n",
      "THI1 value is: 69.77562188720708\n",
      "Row(Timestamp=1427846400000, Geohash='9g0kfvzevc5b', avgtemper=294.6247863769531, avghum=94.0)\n",
      "THI1 value is: 70.22412829589848\n",
      "Row(Timestamp=1427846400000, Geohash='9gb7qgmxbu5b', avgtemper=294.4997863769531, avghum=87.0)\n",
      "THI1 value is: 69.51314324951176\n",
      "Row(Timestamp=1427846400000, Geohash='9mpxfu4kdhhp', avgtemper=294.9997863769531, avghum=45.0)\n",
      "THI1 value is: 67.17723297119144\n",
      "Row(Timestamp=1427846400000, Geohash='9mrbdzwx9eu0', avgtemper=295.1247863769531, avghum=49.0)\n",
      "THI1 value is: 67.64047442626956\n",
      "month: 5\n",
      "Row(Timestamp=1430438400000, Geohash='9spgvj4xmmzb', avgtemper=293.888916015625, avghum=76.0)\n",
      "THI1 value is: 67.78470898437504\n",
      "Row(Timestamp=1430438400000, Geohash='c292jgr2bwxb', avgtemper=294.388916015625, avghum=43.0)\n",
      "THI1 value is: 66.27486669921878\n",
      "Row(Timestamp=1430438400000, Geohash='djzrgxuc1cs0', avgtemper=293.638916015625, avghum=100.0)\n",
      "THI1 value is: 68.88004882812504\n",
      "Row(Timestamp=1430438400000, Geohash='dq17k94jfh80', avgtemper=294.263916015625, avghum=39.0)\n",
      "THI1 value is: 65.84856005859378\n",
      "Row(Timestamp=1430438400000, Geohash='9g88y4cf507b', avgtemper=294.138916015625, avghum=85.0)\n",
      "THI1 value is: 68.77671142578129\n",
      "month: 6\n",
      "Row(Timestamp=1433116800000, Geohash='dnseh0g4w45b', avgtemper=293.62646484375, avghum=88.0)\n",
      "THI1 value is: 68.11646093750005\n",
      "Row(Timestamp=1433116800000, Geohash='c37331193ss0', avgtemper=295.25146484375, avghum=74.0)\n",
      "THI1 value is: 69.75425585937504\n",
      "Row(Timestamp=1433116800000, Geohash='9xyjr7462xrz', avgtemper=294.37646484375, avghum=55.0)\n",
      "THI1 value is: 67.09072753906254\n",
      "Row(Timestamp=1433116800000, Geohash='9wsc2wehk17z', avgtemper=294.50146484375, avghum=93.0)\n",
      "THI1 value is: 69.93903417968755\n",
      "Row(Timestamp=1433116800000, Geohash='9g897mcuzp5b', avgtemper=293.62646484375, avghum=83.0)\n",
      "THI1 value is: 67.80763769531254\n",
      "month: 7\n",
      "Row(Timestamp=1435708800000, Geohash='f2d5v1jeyp7z', avgtemper=293.533203125, avghum=89.0)\n",
      "THI1 value is: 68.02061328125004\n",
      "Row(Timestamp=1435708800000, Geohash='8ypk4ncq5n2p', avgtemper=295.033203125, avghum=28.0)\n",
      "THI1 value is: 65.92985937500004\n",
      "Row(Timestamp=1435708800000, Geohash='9hwuw5zg2g8p', avgtemper=293.783203125, avghum=61.0)\n",
      "THI1 value is: 66.66981640625005\n",
      "Row(Timestamp=1435708800000, Geohash='dnz99pet0b5b', avgtemper=294.533203125, avghum=65.0)\n",
      "THI1 value is: 68.01064453125004\n",
      "Row(Timestamp=1435708800000, Geohash='8yxzk3r2d72p', avgtemper=294.408203125, avghum=38.0)\n",
      "THI1 value is: 65.95067968750004\n",
      "month: 8\n",
      "Row(Timestamp=1438387200000, Geohash='9exc0vhkpw20', avgtemper=293.905029296875, avghum=89.0)\n",
      "THI1 value is: 68.64899951171878\n",
      "Row(Timestamp=1438387200000, Geohash='9mjebk6g6d0p', avgtemper=295.030029296875, avghum=35.0)\n",
      "THI1 value is: 66.45703369140628\n",
      "Row(Timestamp=1438387200000, Geohash='cbbz2cbczckp', avgtemper=293.905029296875, avghum=43.0)\n",
      "THI1 value is: 65.67968603515628\n",
      "Row(Timestamp=1438387200000, Geohash='cdmfx6ntsnkp', avgtemper=294.155029296875, avghum=75.0)\n",
      "THI1 value is: 68.13279541015629\n",
      "Row(Timestamp=1438387200000, Geohash='8ykecuk67y00', avgtemper=295.030029296875, avghum=25.0)\n",
      "THI1 value is: 65.69903076171877\n",
      "month: 9\n",
      "Row(Timestamp=1441130400000, Geohash='8ygq6w53dgxb', avgtemper=294.3955078125, avghum=26.0)\n",
      "THI1 value is: 65.10223828125002\n",
      "Row(Timestamp=1441130400000, Geohash='8ygy73gtpu80', avgtemper=294.3955078125, avghum=28.0)\n",
      "THI1 value is: 65.24114843750003\n",
      "Row(Timestamp=1441130400000, Geohash='8yxn1pgzyhxb', avgtemper=294.5205078125, avghum=27.0)\n",
      "THI1 value is: 65.30544335937503\n",
      "Row(Timestamp=1441130400000, Geohash='8yxqccmws22p', avgtemper=294.5205078125, avghum=27.0)\n",
      "THI1 value is: 65.30544335937503\n",
      "Row(Timestamp=1441130400000, Geohash='8zm43e9md4pb', avgtemper=293.3955078125, avghum=45.0)\n",
      "THI1 value is: 65.17188476562504\n",
      "month: 10\n",
      "Row(Timestamp=1443657600000, Geohash='c8xgk8w8b7rz', avgtemper=293.69580078125, avghum=43.0)\n",
      "THI1 value is: 65.42233496093753\n",
      "Row(Timestamp=1443657600000, Geohash='9g32k67xvd2p', avgtemper=294.69580078125, avghum=87.0)\n",
      "THI1 value is: 69.84048730468754\n",
      "Row(Timestamp=1443657600000, Geohash='8vyzfxs1rcbp', avgtemper=295.07080078125, avghum=29.0)\n",
      "THI1 value is: 66.04667285156253\n",
      "Row(Timestamp=1443657600000, Geohash='8y79q3k8s4pb', avgtemper=295.07080078125, avghum=40.0)\n",
      "THI1 value is: 66.88496093750003\n",
      "Row(Timestamp=1443657600000, Geohash='8y7bw160kc2p', avgtemper=295.07080078125, avghum=39.0)\n",
      "THI1 value is: 66.80875292968753\n",
      "month: 11\n",
      "Row(Timestamp=1446336000000, Geohash='djdwyzu2gvh0', avgtemper=294.8983459472656, avghum=74.0)\n",
      "THI1 value is: 69.2104527587891\n",
      "Row(Timestamp=1446336000000, Geohash='djt12vdtexb0', avgtemper=294.7733459472656, avghum=40.0)\n",
      "THI1 value is: 66.52801513671878\n",
      "Row(Timestamp=1446336000000, Geohash='8vtzbep14krz', avgtemper=295.2733459472656, avghum=20.0)\n",
      "THI1 value is: 65.56334594726565\n",
      "Row(Timestamp=1446336000000, Geohash='8vvje26qqs80', avgtemper=294.7733459472656, avghum=41.0)\n",
      "THI1 value is: 66.60124859619144\n",
      "Row(Timestamp=1446336000000, Geohash='8y78bsphg3bp', avgtemper=294.3983459472656, avghum=35.0)\n",
      "THI1 value is: 65.7305978393555\n",
      "month: 12\n",
      "Row(Timestamp=1448928000000, Geohash='9h86peybrz20', avgtemper=295.3572998046875, avghum=23.0)\n",
      "THI1 value is: 65.88451879882815\n",
      "Row(Timestamp=1448928000000, Geohash='8vgushutk3bp', avgtemper=293.2322998046875, avghum=76.0)\n",
      "THI1 value is: 66.76038769531253\n",
      "Row(Timestamp=1448928000000, Geohash='8vsvuupqvybp', avgtemper=293.9822998046875, avghum=35.0)\n",
      "THI1 value is: 65.25214477539066\n",
      "Row(Timestamp=1448928000000, Geohash='8vy43hh7cp2p', avgtemper=293.4822998046875, avghum=47.0)\n",
      "THI1 value is: 65.40102075195315\n",
      "Row(Timestamp=1448928000000, Geohash='9g76dbr175ez', avgtemper=294.4822998046875, avghum=35.0)\n",
      "THI1 value is: 65.82714477539065\n",
      "Finished. it's been 3760 seconds\n"
     ]
    }
   ],
   "source": [
    "started_at = datetime.now()\n",
    "\n",
    "month = 1\n",
    "for df_1 in dfList:  \n",
    "    df_1.createOrReplaceTempView(\"TRAVEL_DF\")\n",
    "\n",
    "    # F = 9/5(K â 273.15) + 32  68 to 72 Â°F, so K should be 293.15 to 295.37\n",
    "    best_time = spark.sql(\"SELECT Timestamp, Geohash, avg(temperature_surface) as avgtemper, \\\n",
    "                             avg(relative_humidity_zerodegc_isotherm) as avghum FROM TRAVEL_DF \\\n",
    "                                group by Timestamp, Geohash \\\n",
    "                                  having avg(temperature_surface) > 293.15 \\\n",
    "                                     and avg(temperature_surface) < 295.37\").collect()\n",
    "    print('month:', month)\n",
    "    month = month + 1\n",
    "    \n",
    "    count = 0\n",
    "    for x in best_time:\n",
    "        f = 9/5 * (x.avgtemper - 273.15) + 32\n",
    "        c = (f - 32) * 5/9\n",
    "        THI1 = 1.8 * c - (1 - x.avghum / 100) * (c - 14.3) + 32\n",
    "    #     print(THI1)\n",
    "        if count > 4 :\n",
    "            break\n",
    "        if THI1 >= 65 and THI1 < 75:\n",
    "            count = count + 1\n",
    "            print(x)\n",
    "            print('THI1 value is:', THI1)\n",
    "\n",
    "print(\"Finished. it's been \" + str((datetime.now()-started_at).seconds) + \" seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Pick the temperature_surface and relative_humidity_zerodegc_isotherm feature\n",
    "* The Temperature-Humidity Index (THI) was calculated for each month using the formula developed by Kibler (1964):<br> \n",
    "from: https://www.researchgate.net/publication/304717353_An_Estimate_of_Thermal_Comfort_in_North-Central_Region_of_Nigeria <br>\n",
    "THI1 = 1.8 Ã Ta - (1-RH)(Ta-14.3) + 32\n",
    "    * Where Ta = average ambient monthly temperature in Â°C \n",
    "    * RH = average monthly relative humidity as a fraction of the unit\n",
    "![](./images/THI1_index_table.png)<br>\n",
    "* Most people will feel comfortable at colloquially a range of temperatures around 20 to 22 Â°C (68 to 72 Â°F). Thermal comfort: https://en.wikipedia.org/wiki/Thermal_comfort\n",
    "* The recommended range of indoor relative humidity in air conditioned buildings is generally 30-60%. Relative humidity: https://en.wikipedia.org/wiki/Relative_humidity\n",
    "* The formula converts a temperature from kelvin K to degrees Fahrenheit F: F = 9/5(K â 273.15) + 32\n",
    "* The formula converts a temperature from Fahrenheit (Â°F) to Celsius (Â°C): C = (F - 32) * 5/9\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "| Best time(month) |    Geohash    | Average Temperature Â°F|  Average Humidity  |        THI1       |\n",
    "|:---------------- |:--------------|:---------------------:|:------------------:|:-----------------:|\n",
    "| April            | 9mpxfu4kdhhp  | 71.3                  | 45.0               | 67.17723297119144 |\n",
    "| May              | c292jgr2bwxb  | 70.2                  | 43.0               | 66.27486669921878 |\n",
    "| June             | 9xyjr7462xrz  | 70.2                  | 55.0               | 67.09072753906254 | \n",
    "| November         | djt12vdtexb0  | 70.9                  | 40.0               | 66.52801513671878 |\n",
    "| December         | 9g76dbr175ez  | 70.4                  | 35.0               | 65.82714477539065 |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### April, Geohash: 9mpxfu4kdhhp, Average Temperature Â°F: 71.3, Average Humidity: 45.0,  THI1: 67.17723297119144\n",
    "![](./images/travel_01.png)<br>\n",
    "#### May, Geohash: c292jgr2bwxb, Average Temperature Â°F: 70.2, Average Humidity: 43.0,  THI1: 66.27486669921878\n",
    "![](./images/travel_02.png)<br>\n",
    "#### June, Geohash: 9xyjr7462xrz, Average Temperature Â°F: 70.2, Average Humidity: 55.0,  THI1: 67.09072753906254\n",
    "![](./images/travel_03.png)<br>\n",
    "#### November, Geohash: djt12vdtexb0, Average Temperature Â°F: 70.9, Average Humidity: 40.0,  THI1: 66.52801513671878\n",
    "![](./images/travel_04.png)<br>\n",
    "#### December, Geohash: 9g76dbr175ez, Average Temperature Â°F: 70.4, Average Humidity: 35.0,  THI1: 65.82714477539065\n",
    "![](./images/travel_05.png)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q5. [1 pt] Escaping the fog: After becoming rich from your startup, you are looking for the perfect location to build your Bay Area mansion with unobstructed views. Find the locations that are the least foggy and show them on a map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(Geohash='9q9jnpfx5rxb', avgVisibility=23811.522961255705)\n",
      "Row(Geohash='9q9jv7p453zz', avgVisibility=23703.065985376827)\n",
      "Row(Geohash='9q9m35ffeg7z', avgVisibility=23698.021476396112)\n",
      "Row(Geohash='9q9kc8wprss0', avgVisibility=23675.469547196735)\n",
      "Row(Geohash='9q9jdrny12rz', avgVisibility=23616.419106960297)\n",
      "Row(Geohash='9q8zuzx3hzeb', avgVisibility=23600.692098283627)\n",
      "Row(Geohash='9q9k4m9efqh0', avgVisibility=23497.428004600173)\n",
      "Row(Geohash='9q9hp3c2xy5b', avgVisibility=23449.653520560052)\n",
      "Row(Geohash='9q9hwsyh477z', avgVisibility=23436.152038374712)\n",
      "Row(Geohash='9q9mbqzmqxrz', avgVisibility=23435.8553033342)\n",
      "Row(Geohash='9q9pesk2gbs0', avgVisibility=23400.098629552464)\n",
      "Row(Geohash='9q9j5h47k07z', avgVisibility=23329.623851402575)\n",
      "Row(Geohash='9q9py8hurvup', avgVisibility=23277.991815833026)\n",
      "Row(Geohash='9q9nky5sd52p', avgVisibility=23164.93542607814)\n",
      "Row(Geohash='9q9nxfh005kp', avgVisibility=23088.525926889935)\n",
      "Row(Geohash='9q9p887u91up', avgVisibility=22978.882014243467)\n",
      "Row(Geohash='9q8zms7pxw7z', avgVisibility=22889.71287378147)\n",
      "Row(Geohash='9q8zcgwg9y7z', avgVisibility=22882.88794112842)\n",
      "Row(Geohash='9q9n3f70mpeb', avgVisibility=22881.849376101167)\n",
      "Row(Geohash='9q9ppm0d2vgz', avgVisibility=22866.27073556238)\n",
      "Row(Geohash='9q9mkpejm7rz', avgVisibility=22846.537790437127)\n",
      "Row(Geohash='9q8vx7q925s0', avgVisibility=22698.1698534142)\n",
      "Row(Geohash='9q9mx57x2ps0', avgVisibility=22678.882005442498)\n",
      "Row(Geohash='9q9ph1pyzf5b', avgVisibility=22557.517036797384)\n",
      "Row(Geohash='9q9kusw6n7kp', avgVisibility=22483.184677927715)\n",
      "Row(Geohash='9q9km33ww4h0', avgVisibility=22449.653523746514)\n",
      "Row(Geohash='9q9j004nwjgz', avgVisibility=22337.042257518373)\n",
      "Row(Geohash='9q8yny6gc2pb', avgVisibility=22245.202497623795)\n",
      "Row(Geohash='9q9mp8qsnqrz', avgVisibility=22244.163904211287)\n",
      "Row(Geohash='9q8yy1rjchrz', avgVisibility=22123.540761658984)\n",
      "Row(Geohash='9q9he9jbjws0', avgVisibility=21851.87905278192)\n",
      "Row(Geohash='9q8z68ddprbp', avgVisibility=21642.383494119615)\n",
      "Row(Geohash='9q9ncjr6xueb', avgVisibility=21488.377561846544)\n",
      "Row(Geohash='9q8yejw8eb7z', avgVisibility=21146.982903568256)\n",
      "Row(Geohash='9q8v37qn62h0', avgVisibility=20924.28260454297)\n",
      "Row(Geohash='9q9h2tjgdy7z', avgVisibility=20923.095670479343)\n",
      "Row(Geohash='9q8vkrqk0g2p', avgVisibility=20726.95322760064)\n",
      "Row(Geohash='9q8vbyd0t880', avgVisibility=20611.226223915903)\n",
      "Row(Geohash='9q8y81w4x87z', avgVisibility=20583.48141743946)\n",
      "Row(Geohash='9q8y5f6qqy00', avgVisibility=20559.594181275865)\n",
      "40\n",
      "Finished. it's been 317 seconds\n"
     ]
    }
   ],
   "source": [
    "started_at = datetime.now()\n",
    "df_entire.createOrReplaceTempView(\"TEMP_DF\")\n",
    "visibility = spark.sql(\"SELECT  Geohash,avg(visibility_surface) as avgVisibility FROM TEMP_DF WHERE Geohash like '9q9p%' or Geohash like '9q9n%' or Geohash like '9q9j%' or Geohash like '9q9m%' or Geohash like '9q9k%' or Geohash like '9q9h%' or Geohash like '9q8v%' or Geohash like '9q8y%' or Geohash like '9q8z%' group by Geohash order by avgVisibility DESC \").collect()\n",
    "for x in visibility:\n",
    "    print(x)\n",
    "print(len(visibility))\n",
    "print(\"Finished. it's been \" + str((datetime.now()-started_at).seconds) + \" seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The list above has the best visibility in the bay area. \n",
    "Such as 9q9m35 is a good place to build a mansion. The address is S Grimmer Blvd:Fremont Blvd, Fremont, CA\n",
    "<br>\n",
    "![](./images/Grimmer.jpg)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q6. [2 pt] SolarWind, Inc.: You get bored enjoying the amazing views from your mansion, so you start a new company; here, you want to help power companies plan out the locations of solar and wind farms across North America. Locate the top 3 places for solar and wind farms, as well as a combination of both (solar + wind farm). You will report a total of 9 Geohashes as well as their relevant attributes (for example, cloud cover and wind speeds)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 3 places for solar:\n",
      "Row(Geohash='9mtz1cve0n5b', avgcloud=13.301871440195281, avgwind=2.864942249701797)\n",
      "Row(Geohash='9mwj3j02xw0p', avgcloud=13.459723352318958, avgwind=3.1962075028795844)\n",
      "Row(Geohash='9mtzhuf4cbrz', avgcloud=13.545972335231896, avgwind=2.6938682030719696)\n",
      "Top 3 places for wind farms:\n",
      "Row(Geohash='f9pz8wckcbw0', avgcloud=72.79739625711962, avgwind=9.304476408392457)\n",
      "Row(Geohash='f9pbdppft8d0', avgcloud=82.01952807160293, avgwind=9.286219695969459)\n",
      "Row(Geohash='f9rb1xc31tzz', avgcloud=73.3393002441009, avgwind=9.269183482989924)\n",
      "Top 3 places for solar and wind farms:\n",
      "Row(Geohash='d7jjffc3g57b', avgcloud=19.093572009764035, avgwind=7.262419868791074) comb_solar_wind: -11.83115214097296\n",
      "Row(Geohash='9mq6w3hry67z', avgcloud=22.372660699755897, avgwind=6.57151263059883) comb_solar_wind: -15.801148069157067\n",
      "Row(Geohash='d7jjtr7ksvh0', avgcloud=23.687550854353134, avgwind=7.485111079679649) comb_solar_wind: -16.202439774673486\n",
      "Finished. it's been 2182 seconds\n"
     ]
    }
   ],
   "source": [
    "started_at = datetime.now()\n",
    "\n",
    "df.createOrReplaceTempView(\"SOLAR_WIND_DF\")\n",
    "\n",
    "print('Top 3 places for solar:')\n",
    "cloud = spark.sql(\"SELECT Geohash, avg(total_cloud_cover_entire_atmosphere) as avgcloud, \\\n",
    "                     avg(surface_wind_gust_surface) as avgwind \\\n",
    "                       FROM SOLAR_WIND_DF WHERE land_cover_land1_sea0_surface = 1 \\\n",
    "                         and total_cloud_cover_entire_atmosphere >= 0 \\\n",
    "                           group by Geohash order by avgcloud ASC limit 3\").collect()\n",
    "\n",
    "for c in cloud:\n",
    "    print(c)\n",
    "\n",
    "print('Top 3 places for wind farms:')    \n",
    "wind = spark.sql(\"SELECT Geohash, avg(total_cloud_cover_entire_atmosphere) as avgcloud, \\\n",
    "                    avg(surface_wind_gust_surface) as avgwind \\\n",
    "                      FROM SOLAR_WIND_DF WHERE land_cover_land1_sea0_surface = 1 \\\n",
    "                        and geopotential_height_surface > 80 \\\n",
    "                          and total_cloud_cover_entire_atmosphere >= 0 \\\n",
    "                            group by Geohash having avgwind > 6.5 order by avgwind DESC limit 3\").collect()\n",
    "\n",
    "for w in wind:\n",
    "    print(w)\n",
    "\n",
    "print('Top 3 places for solar and wind farms:')\n",
    "solar_wind = spark.sql(\"SELECT Geohash, avg(total_cloud_cover_entire_atmosphere) as avgcloud, \\\n",
    "                          avg(surface_wind_gust_surface) as avgwind \\\n",
    "                            FROM SOLAR_WIND_DF WHERE land_cover_land1_sea0_surface = 1 \\\n",
    "                              and geopotential_height_surface > 80 \\\n",
    "                                and total_cloud_cover_entire_atmosphere >= 0 \\\n",
    "                                  group by Geohash having avgwind > 6.5\").collect()\n",
    "\n",
    "\n",
    "def comparator(e):\n",
    "    return e.avgwind - e.avgcloud\n",
    "\n",
    "solar_wind.sort(reverse=True, key=comparator)\n",
    "\n",
    "count = 0\n",
    "for sw in solar_wind:\n",
    "    if count > 2:\n",
    "        break\n",
    "    count = count + 1\n",
    "    print(sw, 'comb_solar_wind:', sw.avgwind - sw.avgcloud)\n",
    "\n",
    "print(\"Finished. it's been \" + str((datetime.now()-started_at).seconds) + \" seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example\n",
    "* Top 1 place for solar: Geohash='9mtz1cve0n5b', avgcloud=13.301871440195281, avgwind=2.864942249701797\n",
    "![](./images/solar_01.png)<br>\n",
    "* Top 1 place for wind farm: Geohash='f9pz8wckcbw0', avgcloud=72.79739625711962, avgwind=9.304476408392457\n",
    "![](./images/wind_01.png)<br>\n",
    "* Top 1 place for solar and wind farm: Geohash='d7jjffc3g57b', avgcloud=19.093572009764035, avgwind=7.262419868791074, comb_solar_wind: -11.83115214097296\n",
    "![](./images/solar_wind_01.png)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q7. [2 pt] Climate Chart: Given a Geohash prefix, create a climate chart for the region. This includes high, low, and average temperatures, as well as monthly average rainfall (precipitation). Hereâs a (poor quality) script that will generate this for you.\n",
    "* Earn up to 1 point of extra credit for enhancing/improving this chart (or porting it to a more feature-rich visualization library)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prefix geohash is 9q94r cutted from 9q94rzdk9 and it is a location in San Francisco."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = spark.read.format('csv').option('sep', '\\t').schema(schema).load('/Volumes/evo/Datasets/NAM_2015_S/*')\n",
    "\n",
    "dfList = list()\n",
    "for x in range(1,13):\n",
    "    if x<10:\n",
    "        x = '0'+str(x)\n",
    "    path = \"hdfs://orion11:20910/datasets/nam_2015%s.tdv.gz\" %x\n",
    "    df_month = spark.read.format('csv').option('sep', '\\t').schema(schema).load(path)\n",
    "    dfList.append(df_month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: [321.3058776855469, 220.9600830078125, 278.4280017600881, 15.4007151189068], 2: [321.85589599609375, 218.9928436279297, 277.6198424566246, 13.73090510602658], 3: [325.72711181640625, 224.5123748779297, 282.37342680636027, 15.254122521463952], 4: [330.8544921875, 231.1042938232422, 285.6748562759392, 16.784410604823112], 5: [328.609619140625, 250.086181640625, 289.69650555733415, 21.304568438239144], 6: [328.758544921875, 263.453857421875, 293.3297806171615, 27.03816524232326], 7: [330.179931640625, 264.17919921875, 295.0883008215764, 30.93388773319382], 8: [331.390625, 265.593505859375, 295.26517995190824, 31.27203283472495], 9: [329.76318359375, 258.19873046875, 293.3856543802662, 28.357654699853768], 10: [326.630859375, 250.59478759765625, 289.5638638867643, 23.53296922862459], 11: [322.370849609375, 232.58804321289062, 285.06030274475773, 18.983783798604218], 12: [321.13922119140625, 226.92987060546875, 282.3588260704892, 16.793512732111274]}\n",
      "Finished. it's been 2932 seconds\n"
     ]
    }
   ],
   "source": [
    "started_at = datetime.now()\n",
    "\n",
    "# key: month\n",
    "# value: month_data_list max_temp, min_temp, avg_temp, avg_rain\n",
    "all_temp_rain_dict = dict()\n",
    "\n",
    "month_index = 1\n",
    "for df_month in dfList:\n",
    "    df_month.createOrReplaceTempView(\"temp_rain_table\")\n",
    "    max_temp = 0\n",
    "    min_temp = 0\n",
    "    avg_temp = 0\n",
    "    avg_rain = 0\n",
    "    month_data = list()\n",
    "    \n",
    "    temp_rain = spark.sql(\"select max(temperature_surface) as max_temp,\\\n",
    "    min(temperature_surface) as min_temp,\\\n",
    "    avg(temperature_surface) as avg_temp,\\\n",
    "    avg(precipitable_water_entire_atmosphere) as avg_rain\\\n",
    "    from temp_rain_table\").collect()\n",
    "\n",
    "    for ele in temp_rain:\n",
    "        month_data.append(ele.max_temp)\n",
    "        month_data.append(ele.min_temp)\n",
    "        month_data.append(ele.avg_temp)\n",
    "        month_data.append(ele.avg_rain)\n",
    "    \n",
    "    all_temp_rain_dict[month_index] = month_data\n",
    "    month_index += 1\n",
    "    \n",
    "print(all_temp_rain_dict)\n",
    "print(\"Finished. it's been \" + str((datetime.now()-started_at).seconds) + \" seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  1.         321.30587769 220.96008301 278.42800176  15.40071512]\n",
      " [  2.         321.855896   218.99284363 277.61984246  13.73090511]\n",
      " [  3.         325.72711182 224.51237488 282.37342681  15.25412252]\n",
      " [  4.         330.85449219 231.10429382 285.67485628  16.7844106 ]\n",
      " [  5.         328.60961914 250.08618164 289.69650556  21.30456844]\n",
      " [  6.         328.75854492 263.45385742 293.32978062  27.03816524]\n",
      " [  7.         330.17993164 264.17919922 295.08830082  30.93388773]\n",
      " [  8.         331.390625   265.59350586 295.26517995  31.27203283]\n",
      " [  9.         329.76318359 258.19873047 293.38565438  28.3576547 ]\n",
      " [ 10.         326.63085938 250.5947876  289.56386389  23.53296923]\n",
      " [ 11.         322.37084961 232.58804321 285.06030274  18.9837838 ]\n",
      " [ 12.         321.13922119 226.92987061 282.35882607  16.79351273]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home2/anaconda3/lib/python3.6/site-packages/matplotlib/font_manager.py:1328: UserWarning: findfont: Font family ['sans-serif'] not found. Falling back to DejaVu Sans\n",
      "  (prop.get_family(), self.defaultFamily[fontext]))\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.gridspec as gridspec\n",
    "import matplotlib\n",
    "from matplotlib import rc\n",
    "\n",
    "def c2f(t):\n",
    "    return (t*9/5.0)+32\n",
    "\n",
    "def k2c(t):\n",
    "    return t-273.15\n",
    "\n",
    "def k2f(t):\n",
    "    return (t*9/5.0)-459.67\n",
    "\n",
    "def disable_spines(ax):\n",
    "    for s in ax.spines:\n",
    "        ax.spines[s].set_visible(False)\n",
    "\n",
    "rc('font',**{'family':'sans-serif','sans-serif':['Arial']})\n",
    "use_c = False\n",
    "converter = k2f\n",
    "if use_c:\n",
    "    converter = k2c\n",
    "\n",
    "first_line = 'Temperature and Precipitable Water in San Francisco'\n",
    "\n",
    "# convert dict into list list\n",
    "data = list()\n",
    "for month in all_temp_rain_dict:\n",
    "    one_month = list()\n",
    "    one_month.append(month)\n",
    "    one_month = one_month + all_temp_rain_dict[month]\n",
    "    data.append(one_month)\n",
    "\n",
    "data = np.asarray(data)\n",
    "print(data)\n",
    "\n",
    "plt.ion()\n",
    "plt.clf()\n",
    "fig = plt.figure(1)\n",
    "fig.subplots_adjust(hspace=.20)\n",
    "gs = gridspec.GridSpec(2, 1, height_ratios=[1.75, 1])\n",
    "ax0 = plt.subplot(gs[0])\n",
    "ax1 = plt.subplot(gs[1], sharex=ax0)\n",
    "plt.setp(ax0.get_xticklabels(), visible=False) # disable upper axis label\n",
    "\n",
    "ax0.patch.set_facecolor('None')\n",
    "ax1.patch.set_facecolor('None')\n",
    "\n",
    "plt.suptitle(first_line, fontsize=14)\n",
    "\n",
    "if k2c(data[:, 1]).min() < 5:\n",
    "    y = 0\n",
    "    if not use_c:\n",
    "        y = c2f(0)\n",
    "    ax0.plot([0, data[:, 1].max() + 1], [y, y], zorder=-1, color='#888888',\n",
    "            alpha=.75, dashes=(8, 2))\n",
    "\n",
    "# <month-num>  <high-temp>  <low-temp>  <avg-temp>  <avg-precip>  \n",
    "\n",
    "rects0 = ax0.bar(.35 + data[:, 0], data[:, 2] - data[:, 1], bottom=data[:, 1],\n",
    "        width=.6, color='#df3c3c', edgecolor='#731515')\n",
    "\n",
    "rects1 = ax1.bar(.35 + data[:, 0], data[:, 4], color='#1b7edb', width=.6,\n",
    "        edgecolor='#1d4871')\n",
    "\n",
    "##################\n",
    "plt.xticks(np.arange(0,12) + 1.4, ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun',\n",
    "    'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'],\n",
    "    rotation=30)\n",
    "\n",
    "disable_spines(ax0)\n",
    "disable_spines(ax1)\n",
    "ax0.spines['left'].set_visible(True)\n",
    "ax1.spines['left'].set_visible(True)\n",
    "\n",
    "for tic in ax0.xaxis.get_major_ticks():\n",
    "    tic.tick1On = tic.tick2On = False\n",
    "\n",
    "for tic in ax0.yaxis.get_major_ticks():\n",
    "    tic.tick2On = False\n",
    "\n",
    "for tic in ax1.xaxis.get_major_ticks():\n",
    "    tic.tick1On = tic.tick2On = False\n",
    "\n",
    "for tic in ax1.yaxis.get_major_ticks():\n",
    "    tic.tick2On = False\n",
    "\n",
    "for rect in rects1:\n",
    "    height = rect.get_height()\n",
    "    ax1.text(rect.get_x() + rect.get_width()/2., 1.08*height,\n",
    "        '%.1f' % (height), ha='center', va='bottom', color='#1d4871')\n",
    "\n",
    "for r, rect in enumerate(rects0):\n",
    "    height = rect.get_height()\n",
    "    ax0.text(rect.get_x() + rect.get_width()/2., rect.get_y() + 1.08*height,\n",
    "        '%d' % int(height + rect.get_y()), ha='center', va='bottom',\n",
    "        color='#731515')\n",
    "    ax0.text(rect.get_x() + rect.get_width()/2., rect.get_y() - 2,\n",
    "        '%d' % int(rect.get_y()), ha='center', va='top', color='#731515')\n",
    "    ax0.plot([rect.get_x() + .05, rect.get_x() + rect.get_width() - .05],\n",
    "            [data[r, 3], data[r, 3]], color='#731515')\n",
    "\n",
    "if use_c:\n",
    "    ax0.set_ylabel('Temperature (C)')\n",
    "    ax1.set_ylabel('Precipitation (cm)')\n",
    "else:\n",
    "    ax0.set_ylabel('Temperature (F)')\n",
    "    ax1.set_ylabel('Precipitation (in)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./images/climate_chart.jpeg)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q8. [2 pt] Influencers: Determine how features influence each other using Pearsonâs correlation coefficient (PCC). The output for this job should include (1) feature pairs sorted by absolute correlation coefficient, and (2) a correlation matrix visualization (heatmaps are a good option)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.stat import Correlation\n",
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = df.select([attr for attr in df.columns if attr != 'Geohash' and attr != '' and attr != None])\n",
    "new_df.show(10)\n",
    "# print(type(new_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ParallelCollectionRDD[72] at parallelize at PythonRDD.scala:184\n",
      "ParallelCollectionRDD[73] at parallelize at PythonRDD.scala:184\n",
      "Correlation is: 0.8500286768773007\n",
      "[[1.         0.97888347 0.99038957]\n",
      " [0.97888347 1.         0.99774832]\n",
      " [0.99038957 0.99774832 1.        ]]\n",
      "Finished. it's been 2 seconds\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.stat import Correlation\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "started_at = datetime.now()\n",
    "\n",
    "# convert to vector column first\n",
    "vector_col = \"corr_features\"\n",
    "assembler = VectorAssembler(inputCols=new_df.columns, outputCol=vector_col)\n",
    "df_vector = assembler.transform(new_df).select(vector_col)\n",
    "\n",
    "# get correlation matrix\n",
    "matrix = Correlation.corr(df_vector, vector_col)\n",
    "big_array = matrix.collect()[0][\"pearson({})\".format(vector_col)].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########debug part##########\n",
    "print(big_array)\n",
    "print(len(new_df.columns))\n",
    "new_df_size = new_df.count()\n",
    "print(new_df_size)\n",
    "print(type(matrix))\n",
    "print(matrix.columns)\n",
    "print(matrix.count())\n",
    "########debug part##########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "new_array_shape = np.array(big_array).reshape(57,57)\n",
    "print(new_array_shape)\n",
    "\n",
    "print(\"Finished. it's been \" + str((datetime.now()-started_at).seconds) + \" seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from matplotlib import rc\n",
    "# rc('font',**{'family':'sans-serif','sans-serif':['Arial']})\n",
    "\n",
    "plt.suptitle('Correlation Heatmap', fontsize=16)\n",
    "plt.xlabel('Dimension ID', fontsize=14)\n",
    "plt.ylabel('Dimension ID', fontsize=14)\n",
    "\n",
    "plt.pcolor(new_array_shape, cmap='RdBu_r')\n",
    "cb = plt.colorbar()\n",
    "cb.set_label('Correlation Coefficient', fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./images/influencer.jpeg)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(new_df.columns)):\n",
    "    for j in range(len(new_df.columns)):\n",
    "        if i != j:\n",
    "            print(f\"{new_df.columns[i]}, {new_df.columns[j]}   {abs(new_array_shape[i][j])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q9. [2 pt] Prediction/Classification: Using what you learned above as your guide, choose a feature to predict or classify via machine learning models in MLlib. You will need to explain:\n",
    "* The feature you will predict/classify\n",
    "* Features used to train the model\n",
    "* How you partitioned your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "started_at = datetime.now()\n",
    "\n",
    "def prepare_data(dframe, predictors, target):\n",
    "    assembler = VectorAssembler(inputCols=predictors, outputCol=\"features\")\n",
    "    output = assembler.transform(dframe)\n",
    "    return output.select(\"features\", target).withColumnRenamed(target, \"label\")\n",
    "\n",
    "\n",
    "prepped = prepare_data(df,\n",
    "    [\"lightning_surface\", \n",
    "         \"categorical_rain_yes1_no0_surface\", \n",
    "         \"soil_type_as_in_zobler_surface\", \n",
    "         \"plant_canopy_surface_water_surface\",\n",
    "         \"geopotential_height_zerodegc_isotherm\",\n",
    "         \"v-component_of_wind_maximum_wind\",\n",
    "         \"u-component_of_wind_maximum_wind\",\n",
    "         \"temperature_surface\"],\n",
    "    \"vegetation_surface\")\n",
    "\n",
    "prepped.show()\n",
    "(trainingData, testData) = prepped.randomSplit([0.9, 0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "rf = RandomForestRegressor(numTrees=100, maxDepth=5, maxBins=32)\n",
    "model = rf.fit(trainingData)\n",
    "predictions = model.transform(testData)\n",
    "\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "p_df = predictions.select(\"label\", \"prediction\").toPandas()\n",
    "\n",
    "plt.suptitle('Random Forest Regressor', fontsize=16)\n",
    "\n",
    "minval = p_df[['label', 'prediction']].min().min()\n",
    "maxval = p_df[['label', 'prediction']].max().max()\n",
    "plt.axis([minval, maxval, minval, maxval])\n",
    "\n",
    "plt.plot(p_df['label'], p_df['prediction'], '.', color='#2ba5f1')\n",
    "plt.plot(range(int(minval), int(maxval)), range(int(minval), int(maxval)), lw=3, dashes=(10, 3), color='#000000', alpha=0.25, label='Ideal Predictor')\n",
    "plt.show()\n",
    "\n",
    "print(\"Finished. it's been \" + str((datetime.now()-started_at).seconds) + \" seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Option 1: Advanced Analysis\n",
    "## Youâve had the opportunity to analyze two datasets thus far; now itâs time to analyze a dataset of your own. Find a dataset online and use Spark (or Hadoop) to analyze it. You should:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. [0.5 pt] Describe the dataset\n",
    "* Police Department Incident Reports: Historical 2003 to May 2018 in San Francisco <br>   [source from]: https://data.sfgov.org/Public-Safety/Police-Department-Incident-Reports-Historical-2003/tmnf-yvry\n",
    "* What's in this Dataset?\n",
    "    * The .csv file(462.9 MB) contains:   \n",
    "        * Rows: 2.21M \n",
    "        * Columns: 13 \n",
    "        * Each row is a: Incident Report\n",
    "* Here are features in the dataset:\n",
    "    * IncidntNum\n",
    "    * Category\n",
    "    * Descript\n",
    "    * DayOfWeek\n",
    "    * Date\n",
    "    * Time\n",
    "    * PdDistrict\n",
    "    * Resolution\n",
    "    * Address\n",
    "    * X\n",
    "    * Y\n",
    "    * Location\n",
    "    * PdId\n",
    "* Preview\n",
    "![](./images/PDIR_dataset.png)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. [0.5 pt] Outline the types of insights you hope to gain from it\n",
    "* Where is the most dangerous area in San Francisco ?\n",
    "* What kind of incident happened most in San Francisco ?\n",
    "* How many incidents were finally resolved ?\n",
    "* Are there any reasons that may raise/drop the crime rate? Time? Place? Resolution rateï¼"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. [1 pt] Make hypotheses about what you might find\n",
    "* Break in car (VANDALISM) happened a lot in San Francisco.\n",
    "* None resolution may take somehow proportion.\n",
    "* Annual crime rate should be dropped from 2003 to 2018.\n",
    "* List some high incidence areas for 24 hours a day in order to remind people to avoid/pay more attention in those areas in a specific time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_option = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").csv('hdfs://orion11:40910/option_dataset/Police_Department_Incident_Reports__Historical_2003_to_May_2018.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "194694\n"
     ]
    }
   ],
   "source": [
    "notComplete = df_option.filter(df_option.Category == \"ASSAULT\").count()\n",
    "print(notComplete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+--------+---------+----------+-----+----------+--------------+--------------------+----------------+---------------+--------------------+--------------+\n",
      "|IncidntNum|Category|Descript|DayOfWeek|      Date| Time|PdDistrict|    Resolution|             Address|               X|              Y|            Location|          PdId|\n",
      "+----------+--------+--------+---------+----------+-----+----------+--------------+--------------------+----------------+---------------+--------------------+--------------+\n",
      "| 140646669| ASSAULT| BATTERY|   Monday|08/04/2014|08:56|   MISSION|ARREST, BOOKED|500 Block of SOUT...|-122.41747701285|37.764357751686|(37.764357751686,...|14064666904134|\n",
      "+----------+--------+--------+---------+----------+-----+----------+--------------+--------------------+----------------+---------------+--------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_option.createOrReplaceTempView(\"POLICE_DF\")\n",
    "spark.sql(\"select * from POLICE_DF limit 1\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1. Since we are super rich, we hire the Batman to take justice in San Francisco for 5 hours. But the Batman could only take justice for one place for an hour. So list the time table for the Batman, that is when happened the most incidents in an hour which needs the Batmanâs help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(hour='00', timecount=113096)\n",
      "Row(hour='01', timecount=65182)\n",
      "Row(hour='02', timecount=54550)\n",
      "Row(hour='03', timecount=35596)\n",
      "Row(hour='04', timecount=25285)\n",
      "Row(hour='05', timecount=22413)\n",
      "Row(hour='06', timecount=33494)\n",
      "Row(hour='07', timecount=55551)\n",
      "Row(hour='08', timecount=82459)\n",
      "Row(hour='09', timecount=89303)\n",
      "Row(hour='10', timecount=95469)\n",
      "Row(hour='11', timecount=97620)\n",
      "Row(hour='12', timecount=132631)\n",
      "Row(hour='13', timecount=108540)\n",
      "Row(hour='14', timecount=112078)\n",
      "Row(hour='15', timecount=120190)\n",
      "Row(hour='16', timecount=125548)\n",
      "Row(hour='17', timecount=135481)\n",
      "Row(hour='18', timecount=140918)\n",
      "Row(hour='19', timecount=126404)\n",
      "Row(hour='20', timecount=115010)\n",
      "Row(hour='21', timecount=109559)\n",
      "Row(hour='22', timecount=113915)\n",
      "Row(hour='23', timecount=104732)\n",
      "24\n",
      "Finished. it's been 1 seconds\n"
     ]
    }
   ],
   "source": [
    "started_at = datetime.now()\n",
    "df_option.createOrReplaceTempView(\"POLICE_DF\")\n",
    "batman = spark.sql(\"SELECT substring(time, 1, 2 ) hour, count(*) as timecount FROM POLICE_DF  group by hour order by timecount DESC limit 5\").collect()\n",
    "for x in batman:\n",
    "    print(x)\n",
    "print(len(batman))\n",
    "print(\"Finished. it's been \" + str((datetime.now()-started_at).seconds) + \" seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2. Break in car (VANDALISM) when and where"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(PdDistrict='TENDERLOIN', c=4214)\n",
      "Row(PdDistrict='PARK', c=6554)\n",
      "Row(PdDistrict='RICHMOND', c=7837)\n",
      "Row(PdDistrict='TARAVAL', c=11607)\n",
      "Row(PdDistrict='CENTRAL', c=12586)\n",
      "Row(PdDistrict='INGLESIDE', c=13126)\n",
      "Row(PdDistrict='MISSION', c=14050)\n",
      "Row(PdDistrict='BAYVIEW', c=14103)\n",
      "Row(PdDistrict='NORTHERN', c=14533)\n",
      "Row(PdDistrict='SOUTHERN', c=17449)\n",
      "10\n",
      "There are totally 116059vandalism\n",
      "TENDERLOIN\n",
      "0.03630911863793415\n",
      "PARK\n",
      "0.05647127753987196\n",
      "RICHMOND\n",
      "0.06752599970704555\n",
      "TARAVAL\n",
      "0.10000947793794536\n",
      "CENTRAL\n",
      "0.10844484270931164\n",
      "INGLESIDE\n",
      "0.11309764860975884\n",
      "MISSION\n",
      "0.12105911648385735\n",
      "BAYVIEW\n",
      "0.12151578076667902\n",
      "NORTHERN\n",
      "0.12522079287259066\n",
      "SOUTHERN\n",
      "0.15034594473500548\n",
      "Finished. it's been 1 seconds\n"
     ]
    }
   ],
   "source": [
    "started_at = datetime.now()\n",
    "vandalism = spark.sql(\"SELECT PdDistrict, count(*) c FROM POLICE_DF WHERE Category ='VANDALISM' group by PdDistrict order by c\").collect()\n",
    "count = 0\n",
    "for x in vandalism:\n",
    "    count = count + x.c\n",
    "    print(x)\n",
    "print(len(vandalism))\n",
    "print('There are totally ' + str(count) + ' vandalisms')\n",
    "for y in vandalism:\n",
    "    print(y.PdDistrict)\n",
    "    print(y.c/count)\n",
    "print(\"Finished. it's been \" + str((datetime.now()-started_at).seconds) + \" seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(hour='00', timecount=6534)\n",
      "Row(hour='01', timecount=4562)\n",
      "Row(hour='02', timecount=4303)\n",
      "Row(hour='03', timecount=2898)\n",
      "Row(hour='04', timecount=1942)\n",
      "Row(hour='05', timecount=1611)\n",
      "Row(hour='06', timecount=1856)\n",
      "Row(hour='07', timecount=2518)\n",
      "Row(hour='08', timecount=3684)\n",
      "Row(hour='09', timecount=3474)\n",
      "Row(hour='10', timecount=3587)\n",
      "Row(hour='11', timecount=3395)\n",
      "Row(hour='12', timecount=4640)\n",
      "Row(hour='13', timecount=3600)\n",
      "Row(hour='14', timecount=4077)\n",
      "Row(hour='15', timecount=4801)\n",
      "Row(hour='16', timecount=5316)\n",
      "Row(hour='17', timecount=7077)\n",
      "Row(hour='18', timecount=8402)\n",
      "Row(hour='19', timecount=7619)\n",
      "Row(hour='20', timecount=7549)\n",
      "Row(hour='21', timecount=7683)\n",
      "Row(hour='22', timecount=7829)\n",
      "Row(hour='23', timecount=7102)\n",
      "24\n",
      "Finished. it's been 1 seconds\n"
     ]
    }
   ],
   "source": [
    "started_at = datetime.now()\n",
    "vandalism = spark.sql(\"SELECT substring(time, 1, 2 ) hour, count(*) as timecount FROM POLICE_DF WHERE Category ='VANDALISM' group by hour order by hour\").collect()\n",
    "for x in vandalism:\n",
    "    print(x)\n",
    "print(len(vandalism))\n",
    "\n",
    "print(\"Finished. it's been \" + str((datetime.now()-started_at).seconds) + \" seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "çä¸å»å¤åçå¨åå¤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(dates='01', timecount=9982)\n",
      "Row(dates='02', timecount=8958)\n",
      "Row(dates='03', timecount=10407)\n",
      "Row(dates='04', timecount=10173)\n",
      "Row(dates='05', timecount=9752)\n",
      "Row(dates='06', timecount=9336)\n",
      "Row(dates='07', timecount=10065)\n",
      "Row(dates='08', timecount=9718)\n",
      "Row(dates='09', timecount=9501)\n",
      "Row(dates='10', timecount=10112)\n",
      "Row(dates='11', timecount=9002)\n",
      "Row(dates='12', timecount=9053)\n",
      "12\n",
      "Finished. it's been 1 seconds\n"
     ]
    }
   ],
   "source": [
    "started_at = datetime.now()\n",
    "vandalism = spark.sql(\"SELECT substring(date, 1, 2 ) dates, count(*) as timecount FROM POLICE_DF WHERE Category ='VANDALISM' group by dates order by dates\").collect()\n",
    "for x in vandalism:\n",
    "    print(x)\n",
    "print(len(vandalism))\n",
    "\n",
    "print(\"Finished. it's been \" + str((datetime.now()-started_at).seconds) + \" seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æä»½ä¸çèµ·æ¥æºå¹³åç"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(year='2003', timecount=6448)\n",
      "Row(year='2004', timecount=6496)\n",
      "Row(year='2005', timecount=7013)\n",
      "Row(year='2006', timecount=7688)\n",
      "Row(year='2007', timecount=7566)\n",
      "Row(year='2008', timecount=7342)\n",
      "Row(year='2009', timecount=7604)\n",
      "Row(year='2010', timecount=7934)\n",
      "Row(year='2011', timecount=7243)\n",
      "Row(year='2012', timecount=7808)\n",
      "Row(year='2013', timecount=6921)\n",
      "Row(year='2014', timecount=7165)\n",
      "Row(year='2015', timecount=7675)\n",
      "Row(year='2016', timecount=8595)\n",
      "Row(year='2017', timecount=9765)\n",
      "Row(year='2018', timecount=2796)\n",
      "16\n",
      "Finished. it's been 1 seconds\n"
     ]
    }
   ],
   "source": [
    "started_at = datetime.now()\n",
    "vandalism = spark.sql(\"SELECT substring(date, 7, 4 ) year, count(*) as timecount FROM POLICE_DF WHERE Category ='VANDALISM' group by year order by year\").collect()\n",
    "# vandalism = spark.sql(\"SELECT substring(time, 1, 2 ) hour, date, PdDistrict FROM POLICE_DF WHERE Category ='VANDALISM' order by hour\").collect()\n",
    "for x in vandalism:\n",
    "    print(x)\n",
    "print(len(vandalism))\n",
    "\n",
    "print(\"Finished. it's been \" + str((datetime.now()-started_at).seconds) + \" seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å¹´ä»½ä¸ä¹å¾å¹³å"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3. \n",
    "### a. Top 10 Category and resolution rate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(Category='LARCENY/THEFT', totalCount=480448, resolveCount=42521)\n",
      "Row(Category='OTHER OFFENSES', totalCount=309358, resolveCount=221514)\n",
      "Row(Category='NON-CRIMINAL', totalCount=238323, resolveCount=53465)\n",
      "Row(Category='ASSAULT', totalCount=194694, resolveCount=80947)\n",
      "Row(Category='VEHICLE THEFT', totalCount=126602, resolveCount=10622)\n",
      "Row(Category='DRUG/NARCOTIC', totalCount=119628, resolveCount=109357)\n",
      "Row(Category='VANDALISM', totalCount=116059, resolveCount=14169)\n",
      "Row(Category='WARRANTS', totalCount=101379, resolveCount=95897)\n",
      "Row(Category='BURGLARY', totalCount=91543, resolveCount=14890)\n",
      "Row(Category='SUSPICIOUS OCC', totalCount=80444, resolveCount=9458)\n",
      "10\n",
      "Finished. it's been 2 seconds\n"
     ]
    }
   ],
   "source": [
    "started_at = datetime.now()\n",
    "category = spark.sql(\"SELECT a.Category, b.totalCount, a.resolveCount from (SELECT distinct Category, count(*) as resolveCount FROM POLICE_DF WHERE not Resolution ='NONE' group by Category) a inner join (SELECT distinct Category, count(*) as totalCount FROM POLICE_DF group by Category order by totalCount DESC limit 10) b on a.Category = b.Category order by b.totalCount DESC\").collect()\n",
    "for x in category:\n",
    "    print(x)\n",
    "print(len(category))\n",
    "\n",
    "print(\"Finished. it's been \" + str((datetime.now()-started_at).seconds) + \" seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top 10 category, and their resolution rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(Category='OTHER OFFENSES', count=221514)\n",
      "Row(Category='DRUG/NARCOTIC', count=109357)\n",
      "Row(Category='WARRANTS', count=95897)\n",
      "Row(Category='ASSAULT', count=80947)\n",
      "Row(Category='NON-CRIMINAL', count=53465)\n",
      "Row(Category='LARCENY/THEFT', count=42521)\n",
      "Row(Category='MISSING PERSON', count=34672)\n",
      "Row(Category='WEAPON LAWS', count=16164)\n",
      "Row(Category='PROSTITUTION', count=15851)\n",
      "Row(Category='BURGLARY', count=14890)\n",
      "10\n",
      "Finished. it's been 1 seconds\n"
     ]
    }
   ],
   "source": [
    "started_at = datetime.now()\n",
    "category = spark.sql(\"SELECT distinct Category, count(*) as count FROM POLICE_DF WHERE not Resolution ='NONE' group by Category order by count DESC limit 10\").collect()\n",
    "for x in category:\n",
    "    print(x)\n",
    "print(len(category))\n",
    "\n",
    "print(\"Finished. it's been \" + str((datetime.now()-started_at).seconds) + \" seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Relationship between criminal increase rate and solved criminal rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(Category='LARCENY/THEFT', totalCount=480448)\n",
      "Row(Category='OTHER OFFENSES', totalCount=309358)\n",
      "Row(Category='NON-CRIMINAL', totalCount=238323)\n",
      "Row(Category='ASSAULT', totalCount=194694)\n",
      "Row(Category='VEHICLE THEFT', totalCount=126602)\n",
      "Row(Category='DRUG/NARCOTIC', totalCount=119628)\n",
      "Row(Category='VANDALISM', totalCount=116059)\n",
      "Row(Category='WARRANTS', totalCount=101379)\n",
      "Row(Category='BURGLARY', totalCount=91543)\n",
      "Row(Category='SUSPICIOUS OCC', totalCount=80444)\n",
      "10\n",
      "Finished. it's been 1 seconds\n"
     ]
    }
   ],
   "source": [
    "started_at = datetime.now()\n",
    "category = spark.sql(\"SELECT distinct Category, count(*) as totalCount FROM POLICE_DF group by Category order by totalCount DESC limit 10\").collect()\n",
    "for x in category:\n",
    "    print(x)\n",
    "print(len(category))\n",
    "\n",
    "print(\"Finished. it's been \" + str((datetime.now()-started_at).seconds) + \" seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LARCENY/THEFT\n",
      "2003  total = 26393  resolution = 3844\n",
      "2004  total = 24505  resolution = 3389\n",
      "2005  total = 25319  resolution = 3346\n",
      "2006  total = 27352  resolution = 3345\n",
      "2007  total = 25770  resolution = 2879\n",
      "2008  total = 25807  resolution = 3099\n",
      "2009  total = 25585  resolution = 3014\n",
      "2010  total = 24446  resolution = 2973\n",
      "2011  total = 25905  resolution = 2729\n",
      "2012  total = 30976  resolution = 2358\n",
      "2013  total = 36412  resolution = 2777\n",
      "2014  total = 38003  resolution = 2367\n",
      "2015  total = 42068  resolution = 2135\n",
      "2016  total = 40449  resolution = 1837\n",
      "2017  total = 47826  resolution = 1882\n",
      "2018  total = 13632  resolution = 547\n",
      "OTHER OFFENSES\n",
      "2003  total = 21232  resolution = 16295\n",
      "2004  total = 20710  resolution = 15511\n",
      "2005  total = 17834  resolution = 12901\n",
      "2006  total = 18306  resolution = 12894\n",
      "2007  total = 19763  resolution = 14910\n",
      "2008  total = 23457  resolution = 18447\n",
      "2009  total = 24693  resolution = 19452\n",
      "2010  total = 20990  resolution = 15295\n",
      "2011  total = 19552  resolution = 14167\n",
      "2012  total = 18646  resolution = 12436\n",
      "2013  total = 19480  resolution = 13917\n",
      "2014  total = 20740  resolution = 14548\n",
      "2015  total = 20382  resolution = 13409\n",
      "2016  total = 19689  resolution = 12634\n",
      "2017  total = 18316  resolution = 11316\n",
      "2018  total = 5568  resolution = 3382\n",
      "NON-CRIMINAL\n",
      "2003  total = 13149  resolution = 1744\n",
      "2004  total = 13778  resolution = 2074\n",
      "2005  total = 14055  resolution = 2326\n",
      "2006  total = 13368  resolution = 2846\n",
      "2007  total = 12677  resolution = 2955\n",
      "2008  total = 12303  resolution = 2913\n",
      "2009  total = 12395  resolution = 2790\n",
      "2010  total = 13877  resolution = 3738\n",
      "2011  total = 15586  resolution = 4610\n",
      "2012  total = 16936  resolution = 4928\n",
      "2013  total = 21084  resolution = 8795\n",
      "2014  total = 19404  resolution = 6516\n",
      "2015  total = 19177  resolution = 2978\n",
      "2016  total = 17919  resolution = 2029\n",
      "2017  total = 17368  resolution = 1667\n",
      "2018  total = 5247  resolution = 556\n",
      "ASSAULT\n",
      "2003  total = 13461  resolution = 5551\n",
      "2004  total = 12899  resolution = 5034\n",
      "2005  total = 11601  resolution = 4142\n",
      "2006  total = 12449  resolution = 4606\n",
      "2007  total = 12518  resolution = 4846\n",
      "2008  total = 12681  resolution = 5141\n",
      "2009  total = 12284  resolution = 5529\n",
      "2010  total = 12387  resolution = 5660\n",
      "2011  total = 12279  resolution = 5605\n",
      "2012  total = 12181  resolution = 4997\n",
      "2013  total = 12580  resolution = 6419\n",
      "2014  total = 12402  resolution = 5253\n",
      "2015  total = 13115  resolution = 5316\n",
      "2016  total = 13603  resolution = 5647\n",
      "2017  total = 13655  resolution = 5388\n",
      "2018  total = 4599  resolution = 1813\n",
      "VEHICLE THEFT\n",
      "2003  total = 15325  resolution = 1339\n",
      "2004  total = 17884  resolution = 1454\n",
      "2005  total = 18194  resolution = 1561\n",
      "2006  total = 7291  resolution = 673\n",
      "2007  total = 6460  resolution = 537\n",
      "2008  total = 6053  resolution = 508\n",
      "2009  total = 5183  resolution = 431\n",
      "2010  total = 4346  resolution = 352\n",
      "2011  total = 4762  resolution = 398\n",
      "2012  total = 6183  resolution = 408\n",
      "2013  total = 6241  resolution = 521\n",
      "2014  total = 7108  resolution = 579\n",
      "2015  total = 7943  resolution = 575\n",
      "2016  total = 6422  resolution = 575\n",
      "2017  total = 5732  resolution = 563\n",
      "2018  total = 1475  resolution = 148\n",
      "DRUG/NARCOTIC\n",
      "2003  total = 9917  resolution = 9334\n",
      "2004  total = 9897  resolution = 9040\n",
      "2005  total = 8533  resolution = 7585\n",
      "2006  total = 9069  resolution = 8157\n",
      "2007  total = 10560  resolution = 9900\n",
      "2008  total = 11648  resolution = 10844\n",
      "2009  total = 11950  resolution = 11275\n",
      "2010  total = 9205  resolution = 8218\n",
      "2011  total = 6935  resolution = 6240\n",
      "2012  total = 6444  resolution = 5539\n",
      "2013  total = 6775  resolution = 6204\n",
      "2014  total = 5408  resolution = 4969\n",
      "2015  total = 4251  resolution = 3853\n",
      "2016  total = 4246  resolution = 3865\n",
      "2017  total = 3308  resolution = 3022\n",
      "2018  total = 1482  resolution = 1312\n",
      "VANDALISM\n",
      "2003  total = 6448  resolution = 765\n",
      "2004  total = 6496  resolution = 748\n",
      "2005  total = 7013  resolution = 644\n",
      "2006  total = 7688  resolution = 701\n",
      "2007  total = 7566  resolution = 893\n",
      "2008  total = 7342  resolution = 932\n",
      "2009  total = 7604  resolution = 961\n",
      "2010  total = 7934  resolution = 1152\n",
      "2011  total = 7243  resolution = 1045\n",
      "2012  total = 7808  resolution = 903\n",
      "2013  total = 6921  resolution = 1035\n",
      "2014  total = 7165  resolution = 978\n",
      "2015  total = 7675  resolution = 1003\n",
      "2016  total = 8595  resolution = 1022\n",
      "2017  total = 9765  resolution = 1054\n",
      "2018  total = 2796  resolution = 333\n",
      "WARRANTS\n",
      "2003  total = 9079  resolution = 8902\n",
      "2004  total = 8114  resolution = 7870\n",
      "2005  total = 6708  resolution = 6165\n",
      "2006  total = 6498  resolution = 6047\n",
      "2007  total = 7105  resolution = 6903\n",
      "2008  total = 5798  resolution = 5602\n",
      "2009  total = 5764  resolution = 5619\n",
      "2010  total = 6187  resolution = 5786\n",
      "2011  total = 6311  resolution = 5888\n",
      "2012  total = 6300  resolution = 5670\n",
      "2013  total = 7362  resolution = 6986\n",
      "2014  total = 6726  resolution = 6339\n",
      "2015  total = 6815  resolution = 6287\n",
      "2016  total = 5974  resolution = 5615\n",
      "2017  total = 5020  resolution = 4742\n",
      "2018  total = 1618  resolution = 1476\n",
      "BURGLARY\n",
      "2003  total = 6047  resolution = 840\n",
      "2004  total = 6753  resolution = 857\n",
      "2005  total = 7071  resolution = 895\n",
      "2006  total = 7004  resolution = 833\n",
      "2007  total = 5454  resolution = 778\n",
      "2008  total = 5679  resolution = 775\n",
      "2009  total = 5379  resolution = 844\n",
      "2010  total = 4966  resolution = 931\n",
      "2011  total = 4987  resolution = 1108\n",
      "2012  total = 6243  resolution = 1078\n",
      "2013  total = 6195  resolution = 1319\n",
      "2014  total = 6066  resolution = 1219\n",
      "2015  total = 5931  resolution = 1016\n",
      "2016  total = 5813  resolution = 1049\n",
      "2017  total = 5857  resolution = 994\n",
      "2018  total = 2098  resolution = 354\n",
      "SUSPICIOUS OCC\n",
      "2003  total = 4196  resolution = 370\n",
      "2004  total = 4489  resolution = 360\n",
      "2005  total = 4693  resolution = 550\n",
      "2006  total = 4775  resolution = 482\n",
      "2007  total = 4800  resolution = 518\n",
      "2008  total = 4751  resolution = 469\n",
      "2009  total = 4627  resolution = 512\n",
      "2010  total = 6004  resolution = 907\n",
      "2011  total = 6207  resolution = 868\n",
      "2012  total = 5860  resolution = 778\n",
      "2013  total = 5677  resolution = 989\n",
      "2014  total = 5230  resolution = 735\n",
      "2015  total = 5500  resolution = 731\n",
      "2016  total = 5802  resolution = 576\n",
      "2017  total = 6119  resolution = 476\n",
      "2018  total = 1714  resolution = 137\n",
      "Finished. it's been 22 seconds\n"
     ]
    }
   ],
   "source": [
    "started_at = datetime.now()\n",
    "lists = list()\n",
    "for x in category:\n",
    "    types = x.Category\n",
    "    yearResolution = spark.sql(\"SELECT a.year, a.totalCount, b.resolveCount from (SELECT substring(date, 7, 4 ) year, count(*) as totalCount FROM POLICE_DF WHERE Category = '%s' group by year) a inner join (SELECT substring(date, 7, 4 ) year, count(*) as resolveCount FROM POLICE_DF WHERE (not Resolution ='NONE') and Category = '%s' group by year )b on a.year = b.year order by a.year \" %(types,types)).collect()\n",
    "    print(types)\n",
    "    lists.append(yearResolution)\n",
    "    for x in yearResolution:\n",
    "        print(str(x.year) +'  total = ' + str(x.totalCount) + '  resolution = '+str(x.resolveCount))\n",
    "# for x in category:\n",
    "#     print(x)\n",
    "# print(len(category))\n",
    "\n",
    "print(\"Finished. it's been \" + str((datetime.now()-started_at).seconds) + \" seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2003  0.14564467851324214\n",
      "2004  0.13829830646806773\n",
      "2005  0.1321537185512856\n",
      "2006  0.12229453056449255\n",
      "2007  0.11171905316259216\n",
      "2008  0.1200836982214128\n",
      "2009  0.11780340042993942\n",
      "2010  0.12161498813711855\n",
      "2011  0.10534645821270025\n",
      "2012  0.07612345041322315\n",
      "2013  0.07626606613204438\n",
      "2014  0.06228455648238297\n",
      "2015  0.05075116478083103\n",
      "2016  0.045415214220376275\n",
      "2017  0.03935098063814661\n",
      "2018  0.04012617370892019\n",
      "*****************\n",
      "2003  0.7674736247174077\n",
      "2004  0.7489618541767262\n",
      "2005  0.7233935179993272\n",
      "2006  0.7043592264831203\n",
      "2007  0.7544401153671001\n",
      "2008  0.7864177004732062\n",
      "2009  0.7877536143846434\n",
      "2010  0.7286803239637922\n",
      "2011  0.7245806055646481\n",
      "2012  0.6669526976295184\n",
      "2013  0.7144250513347022\n",
      "2014  0.7014464802314369\n",
      "2015  0.6578844078108135\n",
      "2016  0.6416780943674133\n",
      "2017  0.6178204848220136\n",
      "2018  0.6073994252873564\n",
      "*****************\n",
      "2003  0.13263366035439958\n",
      "2004  0.1505298301640296\n",
      "2005  0.16549270722162931\n",
      "2006  0.21289646918013166\n",
      "2007  0.23309931371775658\n",
      "2008  0.23677151914167277\n",
      "2009  0.22509076240419523\n",
      "2010  0.26936657779058876\n",
      "2011  0.2957782625433081\n",
      "2012  0.29097779877184693\n",
      "2013  0.41714095996964523\n",
      "2014  0.3358070500927644\n",
      "2015  0.15529019137508474\n",
      "2016  0.11323176516546682\n",
      "2017  0.09598111469368954\n",
      "2018  0.10596531351248333\n",
      "*****************\n",
      "2003  0.4123764950598024\n",
      "2004  0.3902628110706256\n",
      "2005  0.3570381863632445\n",
      "2006  0.3699895573941682\n",
      "2007  0.3871225435373063\n",
      "2008  0.405409668007255\n",
      "2009  0.4500976880494953\n",
      "2010  0.4569306531040607\n",
      "2011  0.456470396612102\n",
      "2012  0.4102290452343814\n",
      "2013  0.5102543720190779\n",
      "2014  0.4235607160135462\n",
      "2015  0.40533739992375145\n",
      "2016  0.4151290156583107\n",
      "2017  0.39458073965580376\n",
      "2018  0.3942161339421613\n",
      "*****************\n",
      "2003  0.08737357259380098\n",
      "2004  0.08130172220979646\n",
      "2005  0.08579751566450478\n",
      "2006  0.09230558222466054\n",
      "2007  0.08312693498452012\n",
      "2008  0.08392532628448703\n",
      "2009  0.08315647308508586\n",
      "2010  0.08099401748734468\n",
      "2011  0.08357832843343133\n",
      "2012  0.06598738476467735\n",
      "2013  0.08348021150456658\n",
      "2014  0.08145751266178954\n",
      "2015  0.07239078433841117\n",
      "2016  0.08953597010277173\n",
      "2017  0.0982205163991626\n",
      "2018  0.10033898305084746\n",
      "*****************\n",
      "2003  0.9412120600988202\n",
      "2004  0.9134081034656967\n",
      "2005  0.8889019102308684\n",
      "2006  0.8994376447237843\n",
      "2007  0.9375\n",
      "2008  0.9309752747252747\n",
      "2009  0.9435146443514645\n",
      "2010  0.8927756653992396\n",
      "2011  0.8997837058399423\n",
      "2012  0.8595592799503414\n",
      "2013  0.915719557195572\n",
      "2014  0.9188239644970414\n",
      "2015  0.906374970595154\n",
      "2016  0.9102684879886952\n",
      "2017  0.9135429262394196\n",
      "2018  0.8852901484480432\n",
      "*****************\n",
      "2003  0.11864143920595534\n",
      "2004  0.11514778325123153\n",
      "2005  0.09182945957507486\n",
      "2006  0.09118106139438086\n",
      "2007  0.11802802008987576\n",
      "2008  0.1269408880414056\n",
      "2009  0.12638085218306155\n",
      "2010  0.14519788253087976\n",
      "2011  0.14427723319066685\n",
      "2012  0.11565061475409837\n",
      "2013  0.14954486345903772\n",
      "2014  0.13649685973482206\n",
      "2015  0.13068403908794787\n",
      "2016  0.1189063408958697\n",
      "2017  0.10793650793650794\n",
      "2018  0.11909871244635194\n",
      "*****************\n",
      "2003  0.9805044608437052\n",
      "2004  0.9699285186098102\n",
      "2005  0.919051878354204\n",
      "2006  0.9305940289319791\n",
      "2007  0.9715693173821253\n",
      "2008  0.9661952397378406\n",
      "2009  0.9748438584316447\n",
      "2010  0.9351866817520608\n",
      "2011  0.9329741720804944\n",
      "2012  0.9\n",
      "2013  0.9489269220320565\n",
      "2014  0.9424620874219447\n",
      "2015  0.9225238444607483\n",
      "2016  0.939906260462002\n",
      "2017  0.9446215139442231\n",
      "2018  0.9122373300370828\n",
      "*****************\n",
      "2003  0.13891185711923268\n",
      "2004  0.12690656004738635\n",
      "2005  0.12657332767642485\n",
      "2006  0.11893203883495146\n",
      "2007  0.14264759809314265\n",
      "2008  0.13646768797323472\n",
      "2009  0.15690648819483174\n",
      "2010  0.18747482883608538\n",
      "2011  0.22217766192099458\n",
      "2012  0.17267339420150568\n",
      "2013  0.2129136400322841\n",
      "2014  0.20095614902736564\n",
      "2015  0.17130332153093913\n",
      "2016  0.18045759504558748\n",
      "2017  0.16971145637698482\n",
      "2018  0.16873212583412775\n",
      "*****************\n",
      "2003  0.08817921830314586\n",
      "2004  0.08019603475161506\n",
      "2005  0.1171958235670147\n",
      "2006  0.10094240837696335\n",
      "2007  0.10791666666666666\n",
      "2008  0.09871605977688908\n",
      "2009  0.11065485195591096\n",
      "2010  0.1510659560293138\n",
      "2011  0.13984211374254873\n",
      "2012  0.13276450511945392\n",
      "2013  0.174211731548353\n",
      "2014  0.14053537284894838\n",
      "2015  0.13290909090909092\n",
      "2016  0.09927611168562564\n",
      "2017  0.07779048864193495\n",
      "2018  0.07992998833138856\n",
      "*****************\n"
     ]
    }
   ],
   "source": [
    "for x in lists:\n",
    "    for y in x:\n",
    "        print(str(y.year) + '  ' + str(y.resolveCount/ y.totalCount))\n",
    "    print('*****************')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
