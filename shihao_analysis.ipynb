{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up our Schema\n",
    "\n",
    "Spark can automatically create a schema for CSV files, but ours don't have headings. Let's set this up here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StructType(List(StructField(Timestamp,LongType,true),StructField(Geohash,StringType,true),StructField(geopotential_height_lltw,FloatType,true),StructField(water_equiv_of_accum_snow_depth_surface,FloatType,true),StructField(drag_coefficient_surface,FloatType,true),StructField(sensible_heat_net_flux_surface,FloatType,true),StructField(categorical_ice_pellets_yes1_no0_surface,FloatType,true),StructField(visibility_surface,FloatType,true),StructField(number_of_soil_layers_in_root_zone_surface,FloatType,true),StructField(categorical_freezing_rain_yes1_no0_surface,FloatType,true),StructField(pressure_reduced_to_msl_msl,FloatType,true),StructField(upward_short_wave_rad_flux_surface,FloatType,true),StructField(relative_humidity_zerodegc_isotherm,FloatType,true),StructField(categorical_snow_yes1_no0_surface,FloatType,true),StructField(u-component_of_wind_tropopause,FloatType,true),StructField(surface_wind_gust_surface,FloatType,true),StructField(total_cloud_cover_entire_atmosphere,FloatType,true),StructField(upward_long_wave_rad_flux_surface,FloatType,true),StructField(land_cover_land1_sea0_surface,FloatType,true),StructField(vegitation_type_as_in_sib_surface,FloatType,true),StructField(v-component_of_wind_pblri,FloatType,true),StructField(albedo_surface,FloatType,true),StructField(lightning_surface,FloatType,true),StructField(ice_cover_ice1_no_ice0_surface,FloatType,true),StructField(convective_inhibition_surface,FloatType,true),StructField(pressure_surface,FloatType,true),StructField(transpiration_stress-onset_soil_moisture_surface,FloatType,true),StructField(soil_porosity_surface,FloatType,true),StructField(vegetation_surface,FloatType,true),StructField(categorical_rain_yes1_no0_surface,FloatType,true),StructField(downward_long_wave_rad_flux_surface,FloatType,true),StructField(planetary_boundary_layer_height_surface,FloatType,true),StructField(soil_type_as_in_zobler_surface,FloatType,true),StructField(geopotential_height_cloud_base,FloatType,true),StructField(friction_velocity_surface,FloatType,true),StructField(maximumcomposite_radar_reflectivity_entire_atmosphere,FloatType,true),StructField(plant_canopy_surface_water_surface,FloatType,true),StructField(v-component_of_wind_maximum_wind,FloatType,true),StructField(geopotential_height_zerodegc_isotherm,FloatType,true),StructField(mean_sea_level_pressure_nam_model_reduction_msl,FloatType,true),StructField(temperature_surface,FloatType,true),StructField(snow_cover_surface,FloatType,true),StructField(geopotential_height_surface,FloatType,true),StructField(convective_available_potential_energy_surface,FloatType,true),StructField(latent_heat_net_flux_surface,FloatType,true),StructField(surface_roughness_surface,FloatType,true),StructField(pressure_maximum_wind,FloatType,true),StructField(temperature_tropopause,FloatType,true),StructField(geopotential_height_pblri,FloatType,true),StructField(pressure_tropopause,FloatType,true),StructField(snow_depth_surface,FloatType,true),StructField(v-component_of_wind_tropopause,FloatType,true),StructField(downward_short_wave_rad_flux_surface,FloatType,true),StructField(u-component_of_wind_maximum_wind,FloatType,true),StructField(wilting_point_surface,FloatType,true),StructField(precipitable_water_entire_atmosphere,FloatType,true),StructField(u-component_of_wind_pblri,FloatType,true),StructField(direct_evaporation_cease_soil_moisture_surface,FloatType,true)))\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "from pyspark.sql.types import StructType, StructField, FloatType, LongType, StringType\n",
    "\n",
    "feats = []\n",
    "f = open('features.txt')\n",
    "for line_num, line in enumerate(f):\n",
    "    if line_num == 0:\n",
    "        # Timestamp\n",
    "        feats.append(StructField(line.strip(), LongType(), True))\n",
    "    elif line_num == 1:\n",
    "        # Geohash\n",
    "        feats.append(StructField(line.strip(), StringType(), True))\n",
    "    else:\n",
    "        # Other features\n",
    "        feats.append(StructField(line.strip(), FloatType(), True))\n",
    "    \n",
    "schema = StructType(feats)\n",
    "\n",
    "print(schema)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a Dataframe\n",
    "\n",
    "Let's load our CSV into a 'dataframe' - Spark's abstraction for working with tabular data (built on top of RDDs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Timestamp=1430438400000, Geohash='dndf9tz5r8eb', geopotential_height_lltw=1915.593994140625, water_equiv_of_accum_snow_depth_surface=0.0, drag_coefficient_surface=0.0, sensible_heat_net_flux_surface=-12.571273803710938, categorical_ice_pellets_yes1_no0_surface=0.0, visibility_surface=24220.529296875, number_of_soil_layers_in_root_zone_surface=3.0, categorical_freezing_rain_yes1_no0_surface=0.0, pressure_reduced_to_msl_msl=101235.0, upward_short_wave_rad_flux_surface=4.25, relative_humidity_zerodegc_isotherm=95.0, categorical_snow_yes1_no0_surface=0.0, u-component_of_wind_tropopause=20.28228759765625, surface_wind_gust_surface=3.9325132369995117, total_cloud_cover_entire_atmosphere=98.0, upward_long_wave_rad_flux_surface=371.25927734375, land_cover_land1_sea0_surface=1.0, vegitation_type_as_in_sib_surface=10.0, v-component_of_wind_pblri=-3.47259521484375, albedo_surface=17.25, lightning_surface=0.0, ice_cover_ice1_no_ice0_surface=0.0, convective_inhibition_surface=-12.582763671875, pressure_surface=98873.0, transpiration_stress-onset_soil_moisture_surface=0.35999998450279236, soil_porosity_surface=0.5, vegetation_surface=74.75, categorical_rain_yes1_no0_surface=0.0, downward_long_wave_rad_flux_surface=352.7455749511719, planetary_boundary_layer_height_surface=3799.75, soil_type_as_in_zobler_surface=4.0, geopotential_height_cloud_base=1795.5, friction_velocity_surface=0.12634363770484924, maximumcomposite_radar_reflectivity_entire_atmosphere=-5.625, plant_canopy_surface_water_surface=0.17249999940395355, v-component_of_wind_maximum_wind=-36.30204772949219, geopotential_height_zerodegc_isotherm=1940.0, mean_sea_level_pressure_nam_model_reduction_msl=101249.0, temperature_surface=284.513916015625, snow_cover_surface=0.0, geopotential_height_surface=199.8200225830078, convective_available_potential_energy_surface=320.0, latent_heat_net_flux_surface=6.516082763671875, surface_roughness_surface=0.1750158965587616, pressure_maximum_wind=36921.1640625, temperature_tropopause=211.69955444335938, geopotential_height_pblri=85.20362854003906, pressure_tropopause=10801.0634765625, snow_depth_surface=0.0, v-component_of_wind_tropopause=-10.068893432617188, downward_short_wave_rad_flux_surface=24.375, u-component_of_wind_maximum_wind=37.93487548828125, wilting_point_surface=0.08374999463558197, precipitable_water_entire_atmosphere=14.878089904785156, u-component_of_wind_pblri=-0.27671051025390625, direct_evaporation_cease_soil_moisture_surface=0.08374999463558197)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from pyspark.storagelevel import StorageLevel\n",
    "# spark.conf.set(\"spark.sql.broadcastTimeout\", 36000)\n",
    "# spark.conf.set(\"spark.sql.broadcastTimeout\", 1200)\n",
    "# df = spark.read.format('csv').option('sep', '\\t').schema(schema).load('/Volumes/evo/Datasets/NAM_2015_S/*')\n",
    "# df = spark.read.format('csv').option('sep', '\\t').schema(schema).load('hdfs://orion11:37000/nam_tiny.tdv')\n",
    "# df = spark.read.format('csv').option('sep', '\\t').schema(schema).load('hdfs://orion11:37000/data/nam/nam_201509.tdv.gz')\n",
    "# df = spark.read.format('csv').option('sep', '\\t').schema(schema).load('hdfs://orion11:37000/data/nam_s/*')\n",
    "df = spark.read.format('csv').option('sep', '\\t').schema(schema).load('hdfs://orion11:37000/data/nam/*')\n",
    "\n",
    "# df.cache()\n",
    "# df.persist(StorageLevel.DISK_ONLY)\n",
    "df.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [2 pt] Travel Startup: After graduating from USF, you found a startup that aims to provide personalized travel itineraries using big data analysis. Given your own personal preferences, build a plan for a year of travel across 5 locations. Or, in other words: pick 5 regions. What is the best time of year to visit them based on the dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "dfList = list()\n",
    "for x in range(1,13):\n",
    "    if x<10:\n",
    "        x = '0'+str(x)\n",
    "    path = \"hdfs://orion11:37000/data/nam/nam_2015%s.tdv.gz\" %x\n",
    "#     print(path)\n",
    "    df_1 = spark.read.format('csv').option('sep', '\\t').schema(schema).load(path)\n",
    "    dfList.append(df_1)\n",
    "print(len(dfList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "month: 1\n",
      "Row(Timestamp=1420178400000, Geohash='8vkurbww01b0', avgtemper=293.17681884765625, avghum=48.0)\n",
      "THI1 value is: 65.07032812500003\n",
      "Row(Timestamp=1420178400000, Geohash='9usrkvqdgzez', avgtemper=295.30181884765625, avghum=15.0)\n",
      "THI1 value is: 65.19922790527346\n",
      "Row(Timestamp=1420178400000, Geohash='8uxh1ktytx00', avgtemper=294.80181884765625, avghum=21.0)\n",
      "THI1 value is: 65.16533703613283\n",
      "Row(Timestamp=1420178400000, Geohash='8uz0geczqhpz', avgtemper=294.42681884765625, avghum=27.0)\n",
      "THI1 value is: 65.20519616699221\n",
      "Row(Timestamp=1420178400000, Geohash='9kt3kq4je1kp', avgtemper=293.80181884765625, avghum=43.0)\n",
      "THI1 value is: 65.55273718261722\n",
      "month: 2\n",
      "Row(Timestamp=1422748800000, Geohash='dw11jd7dct5b', avgtemper=294.15985107421875, avghum=89.0)\n",
      "THI1 value is: 69.07964831542972\n",
      "Row(Timestamp=1422748800000, Geohash='dw4ss4qugy6p', avgtemper=293.78485107421875, avghum=91.0)\n",
      "THI1 value is: 68.5725953369141\n",
      "Row(Timestamp=1422748800000, Geohash='9uf5eftyjhpb', avgtemper=294.65985107421875, avghum=81.0)\n",
      "THI1 value is: 69.34786022949223\n",
      "Row(Timestamp=1422748800000, Geohash='9g78j99nhsb0', avgtemper=294.65985107421875, avghum=41.0)\n",
      "THI1 value is: 66.46391979980473\n",
      "Row(Timestamp=1422748800000, Geohash='dtc5yrdffgeb', avgtemper=294.28485107421875, avghum=85.0)\n",
      "THI1 value is: 69.01750427246097\n",
      "month: 3\n",
      "Row(Timestamp=1425362400000, Geohash='95ftk3et6uzz', avgtemper=295.13739013671875, avghum=16.0)\n",
      "THI1 value is: 65.11989453125003\n",
      "Row(Timestamp=1425362400000, Geohash='95gubzsyxy20', avgtemper=294.88739013671875, avghum=18.0)\n",
      "THI1 value is: 65.0286423339844\n",
      "Row(Timestamp=1425362400000, Geohash='95ugg6qeuk8p', avgtemper=295.13739013671875, avghum=16.0)\n",
      "THI1 value is: 65.11989453125003\n",
      "Row(Timestamp=1425362400000, Geohash='95upeftn1wrb', avgtemper=294.63739013671875, avghum=22.0)\n",
      "THI1 value is: 65.07113793945315\n",
      "Row(Timestamp=1425362400000, Geohash='95vjj0zts8b0', avgtemper=295.13739013671875, avghum=20.0)\n",
      "THI1 value is: 65.42739013671877\n",
      "month: 4\n",
      "Row(Timestamp=1427846400000, Geohash='9ezg1krg8kk0', avgtemper=294.2497863769531, avghum=97.0)\n",
      "THI1 value is: 69.77562188720708\n",
      "Row(Timestamp=1427846400000, Geohash='9g0kfvzevc5b', avgtemper=294.6247863769531, avghum=94.0)\n",
      "THI1 value is: 70.22412829589848\n",
      "Row(Timestamp=1427846400000, Geohash='9gb7qgmxbu5b', avgtemper=294.4997863769531, avghum=87.0)\n",
      "THI1 value is: 69.51314324951176\n",
      "Row(Timestamp=1427846400000, Geohash='9mpxfu4kdhhp', avgtemper=294.9997863769531, avghum=45.0)\n",
      "THI1 value is: 67.17723297119144\n",
      "Row(Timestamp=1427846400000, Geohash='9mrbdzwx9eu0', avgtemper=295.1247863769531, avghum=49.0)\n",
      "THI1 value is: 67.64047442626956\n",
      "month: 5\n",
      "Row(Timestamp=1430438400000, Geohash='9spgvj4xmmzb', avgtemper=293.888916015625, avghum=76.0)\n",
      "THI1 value is: 67.78470898437504\n",
      "Row(Timestamp=1430438400000, Geohash='c292jgr2bwxb', avgtemper=294.388916015625, avghum=43.0)\n",
      "THI1 value is: 66.27486669921878\n",
      "Row(Timestamp=1430438400000, Geohash='djzrgxuc1cs0', avgtemper=293.638916015625, avghum=100.0)\n",
      "THI1 value is: 68.88004882812504\n",
      "Row(Timestamp=1430438400000, Geohash='dq17k94jfh80', avgtemper=294.263916015625, avghum=39.0)\n",
      "THI1 value is: 65.84856005859378\n",
      "Row(Timestamp=1430438400000, Geohash='9g88y4cf507b', avgtemper=294.138916015625, avghum=85.0)\n",
      "THI1 value is: 68.77671142578129\n",
      "month: 6\n",
      "Row(Timestamp=1433116800000, Geohash='dnseh0g4w45b', avgtemper=293.62646484375, avghum=88.0)\n",
      "THI1 value is: 68.11646093750005\n",
      "Row(Timestamp=1433116800000, Geohash='c37331193ss0', avgtemper=295.25146484375, avghum=74.0)\n",
      "THI1 value is: 69.75425585937504\n",
      "Row(Timestamp=1433116800000, Geohash='9xyjr7462xrz', avgtemper=294.37646484375, avghum=55.0)\n",
      "THI1 value is: 67.09072753906254\n",
      "Row(Timestamp=1433116800000, Geohash='9wsc2wehk17z', avgtemper=294.50146484375, avghum=93.0)\n",
      "THI1 value is: 69.93903417968755\n",
      "Row(Timestamp=1433116800000, Geohash='9g897mcuzp5b', avgtemper=293.62646484375, avghum=83.0)\n",
      "THI1 value is: 67.80763769531254\n",
      "month: 7\n",
      "Row(Timestamp=1435708800000, Geohash='f2d5v1jeyp7z', avgtemper=293.533203125, avghum=89.0)\n",
      "THI1 value is: 68.02061328125004\n",
      "Row(Timestamp=1435708800000, Geohash='8ypk4ncq5n2p', avgtemper=295.033203125, avghum=28.0)\n",
      "THI1 value is: 65.92985937500004\n",
      "Row(Timestamp=1435708800000, Geohash='9hwuw5zg2g8p', avgtemper=293.783203125, avghum=61.0)\n",
      "THI1 value is: 66.66981640625005\n",
      "Row(Timestamp=1435708800000, Geohash='dnz99pet0b5b', avgtemper=294.533203125, avghum=65.0)\n",
      "THI1 value is: 68.01064453125004\n",
      "Row(Timestamp=1435708800000, Geohash='8yxzk3r2d72p', avgtemper=294.408203125, avghum=38.0)\n",
      "THI1 value is: 65.95067968750004\n",
      "month: 8\n",
      "Row(Timestamp=1438387200000, Geohash='9exc0vhkpw20', avgtemper=293.905029296875, avghum=89.0)\n",
      "THI1 value is: 68.64899951171878\n",
      "Row(Timestamp=1438387200000, Geohash='9mjebk6g6d0p', avgtemper=295.030029296875, avghum=35.0)\n",
      "THI1 value is: 66.45703369140628\n",
      "Row(Timestamp=1438387200000, Geohash='cbbz2cbczckp', avgtemper=293.905029296875, avghum=43.0)\n",
      "THI1 value is: 65.67968603515628\n",
      "Row(Timestamp=1438387200000, Geohash='cdmfx6ntsnkp', avgtemper=294.155029296875, avghum=75.0)\n",
      "THI1 value is: 68.13279541015629\n",
      "Row(Timestamp=1438387200000, Geohash='8ykecuk67y00', avgtemper=295.030029296875, avghum=25.0)\n",
      "THI1 value is: 65.69903076171877\n",
      "month: 9\n",
      "Row(Timestamp=1441130400000, Geohash='8ygq6w53dgxb', avgtemper=294.3955078125, avghum=26.0)\n",
      "THI1 value is: 65.10223828125002\n",
      "Row(Timestamp=1441130400000, Geohash='8ygy73gtpu80', avgtemper=294.3955078125, avghum=28.0)\n",
      "THI1 value is: 65.24114843750003\n",
      "Row(Timestamp=1441130400000, Geohash='8yxn1pgzyhxb', avgtemper=294.5205078125, avghum=27.0)\n",
      "THI1 value is: 65.30544335937503\n",
      "Row(Timestamp=1441130400000, Geohash='8yxqccmws22p', avgtemper=294.5205078125, avghum=27.0)\n",
      "THI1 value is: 65.30544335937503\n",
      "Row(Timestamp=1441130400000, Geohash='8zm43e9md4pb', avgtemper=293.3955078125, avghum=45.0)\n",
      "THI1 value is: 65.17188476562504\n",
      "month: 10\n",
      "Row(Timestamp=1443657600000, Geohash='c8xgk8w8b7rz', avgtemper=293.69580078125, avghum=43.0)\n",
      "THI1 value is: 65.42233496093753\n",
      "Row(Timestamp=1443657600000, Geohash='9g32k67xvd2p', avgtemper=294.69580078125, avghum=87.0)\n",
      "THI1 value is: 69.84048730468754\n",
      "Row(Timestamp=1443657600000, Geohash='8vyzfxs1rcbp', avgtemper=295.07080078125, avghum=29.0)\n",
      "THI1 value is: 66.04667285156253\n",
      "Row(Timestamp=1443657600000, Geohash='8y79q3k8s4pb', avgtemper=295.07080078125, avghum=40.0)\n",
      "THI1 value is: 66.88496093750003\n",
      "Row(Timestamp=1443657600000, Geohash='8y7bw160kc2p', avgtemper=295.07080078125, avghum=39.0)\n",
      "THI1 value is: 66.80875292968753\n",
      "month: 11\n",
      "Row(Timestamp=1446336000000, Geohash='djdwyzu2gvh0', avgtemper=294.8983459472656, avghum=74.0)\n",
      "THI1 value is: 69.2104527587891\n",
      "Row(Timestamp=1446336000000, Geohash='djt12vdtexb0', avgtemper=294.7733459472656, avghum=40.0)\n",
      "THI1 value is: 66.52801513671878\n",
      "Row(Timestamp=1446336000000, Geohash='8vtzbep14krz', avgtemper=295.2733459472656, avghum=20.0)\n",
      "THI1 value is: 65.56334594726565\n",
      "Row(Timestamp=1446336000000, Geohash='8vvje26qqs80', avgtemper=294.7733459472656, avghum=41.0)\n",
      "THI1 value is: 66.60124859619144\n",
      "Row(Timestamp=1446336000000, Geohash='8y78bsphg3bp', avgtemper=294.3983459472656, avghum=35.0)\n",
      "THI1 value is: 65.7305978393555\n",
      "month: 12\n",
      "Row(Timestamp=1448928000000, Geohash='9h86peybrz20', avgtemper=295.3572998046875, avghum=23.0)\n",
      "THI1 value is: 65.88451879882815\n",
      "Row(Timestamp=1448928000000, Geohash='8vgushutk3bp', avgtemper=293.2322998046875, avghum=76.0)\n",
      "THI1 value is: 66.76038769531253\n",
      "Row(Timestamp=1448928000000, Geohash='8vsvuupqvybp', avgtemper=293.9822998046875, avghum=35.0)\n",
      "THI1 value is: 65.25214477539066\n",
      "Row(Timestamp=1448928000000, Geohash='8vy43hh7cp2p', avgtemper=293.4822998046875, avghum=47.0)\n",
      "THI1 value is: 65.40102075195315\n",
      "Row(Timestamp=1448928000000, Geohash='9g76dbr175ez', avgtemper=294.4822998046875, avghum=35.0)\n",
      "THI1 value is: 65.82714477539065\n",
      "Job running time: 1:02:56\n"
     ]
    }
   ],
   "source": [
    "start = datetime.datetime.now().replace(microsecond=0)\n",
    "\n",
    "month = 1\n",
    "for df_1 in dfList:  \n",
    "    df_1.createOrReplaceTempView(\"TRAVEL_DF\")\n",
    "\n",
    "    # F = 9/5(K − 273.15) + 32  68 to 72 °F, so K should be 293.15 to 295.37\n",
    "    best_time = spark.sql(\"SELECT Timestamp, Geohash, avg(temperature_surface) as avgtemper, \\\n",
    "                             avg(relative_humidity_zerodegc_isotherm) as avghum FROM TRAVEL_DF \\\n",
    "                                group by Timestamp, Geohash \\\n",
    "                                  having avg(temperature_surface) > 293.15 \\\n",
    "                                     and avg(temperature_surface) < 295.37\").collect()\n",
    "    print('month:', month)\n",
    "    month = month + 1\n",
    "    \n",
    "    count = 0\n",
    "    for x in best_time:\n",
    "        f = 9/5 * (x.avgtemper - 273.15) + 32\n",
    "        c = (f - 32) * 5/9\n",
    "        THI1 = 1.8 * c - (1 - x.avghum / 100) * (c - 14.3) + 32\n",
    "    #     print(THI1)\n",
    "        if count > 4 :\n",
    "            break\n",
    "        if THI1 >= 65 and THI1 < 75:\n",
    "            count = count + 1\n",
    "            print(x)\n",
    "            print('THI1 value is:', THI1)\n",
    "\n",
    "end = datetime.datetime.now().replace(microsecond=0)\n",
    "print('Job running time:', end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Pick the temperature_surface and relative_humidity_zerodegc_isotherm feature\n",
    "* The Temperature-Humidity Index (THI) was calculated for each month using the formula developed by Kibler (1964):<br> \n",
    "from: https://www.researchgate.net/publication/304717353_An_Estimate_of_Thermal_Comfort_in_North-Central_Region_of_Nigeria <br>\n",
    "THI1 = 1.8 × Ta - (1-RH)(Ta-14.3) + 32\n",
    "    * Where Ta = average ambient monthly temperature in °C \n",
    "    * RH = average monthly relative humidity as a fraction of the unit\n",
    "![](./images/THI1_index_table.png)<br>\n",
    "* Most people will feel comfortable at colloquially a range of temperatures around 20 to 22 °C (68 to 72 °F). Thermal comfort: https://en.wikipedia.org/wiki/Thermal_comfort\n",
    "* The recommended range of indoor relative humidity in air conditioned buildings is generally 30-60%. Relative humidity: https://en.wikipedia.org/wiki/Relative_humidity\n",
    "* The formula converts a temperature from kelvin K to degrees Fahrenheit F: F = 9/5(K − 273.15) + 32\n",
    "* The formula converts a temperature from Fahrenheit (°F) to Celsius (°C): C = (F - 32) * 5/9\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "| Best time(month) |    Geohash    | Average Temperature °F|  Average Humidity  |        THI1       |\n",
    "|:---------------- |:--------------|:---------------------:|:------------------:|:-----------------:|\n",
    "| April            | 9mpxfu4kdhhp  | 71.3                  | 45.0               | 67.17723297119144 |\n",
    "| May              | c292jgr2bwxb  | 70.2                  | 43.0               | 66.27486669921878 |\n",
    "| June             | 9xyjr7462xrz  | 70.2                  | 55.0               | 67.09072753906254 | \n",
    "| November         | djt12vdtexb0  | 70.9                  | 40.0               | 66.52801513671878 |\n",
    "| December         | 9g76dbr175ez  | 70.4                  | 35.0               | 65.82714477539065 |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### April, Geohash: 9mpxfu4kdhhp, Average Temperature °F: 71.3, Average Humidity: 45.0,  THI1: 67.17723297119144\n",
    "![](./images/travel_01.png)<br>\n",
    "#### May, Geohash: c292jgr2bwxb, Average Temperature °F: 70.2, Average Humidity: 43.0,  THI1: 66.27486669921878\n",
    "![](./images/travel_02.png)<br>\n",
    "#### June, Geohash: 9xyjr7462xrz, Average Temperature °F: 70.2, Average Humidity: 55.0,  THI1: 67.09072753906254\n",
    "![](./images/travel_03.png)<br>\n",
    "#### November, Geohash: djt12vdtexb0, Average Temperature °F: 70.9, Average Humidity: 40.0,  THI1: 66.52801513671878\n",
    "![](./images/travel_04.png)<br>\n",
    "#### December, Geohash: 9g76dbr175ez, Average Temperature °F: 70.4, Average Humidity: 35.0,  THI1: 65.82714477539065\n",
    "![](./images/travel_05.png)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [2 pt] SolarWind, Inc.: You get bored enjoying the amazing views from your mansion, so you start a new company; here, you want to help power companies plan out the locations of solar and wind farms across North America. Locate the top 3 places for solar and wind farms, as well as a combination of both (solar + wind farm). You will report a total of 9 Geohashes as well as their relevant attributes (for example, cloud cover and wind speeds)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 3 places for solar:\n",
      "Row(Geohash='9mtz1cve0n5b', avgcloud=13.301871440195281, avgwind=2.864942249701797)\n",
      "Row(Geohash='9mwj3j02xw0p', avgcloud=13.459723352318958, avgwind=3.1962075028795844)\n",
      "Row(Geohash='9mtzhuf4cbrz', avgcloud=13.545972335231896, avgwind=2.6938682030719696)\n",
      "Top 3 places for wind farms:\n",
      "Row(Geohash='f9pz8wckcbw0', avgcloud=72.79739625711962, avgwind=9.304476408392457)\n",
      "Row(Geohash='f9pbdppft8d0', avgcloud=82.01952807160293, avgwind=9.286219695969459)\n",
      "Row(Geohash='f9rb1xc31tzz', avgcloud=73.3393002441009, avgwind=9.269183482989924)\n",
      "Top 3 places for solar and wind farms:\n",
      "Row(Geohash='d7jjffc3g57b', avgcloud=19.093572009764035, avgwind=7.262419868791074) comb_solar_wind: -11.83115214097296\n",
      "Row(Geohash='9mq6w3hry67z', avgcloud=22.372660699755897, avgwind=6.57151263059883) comb_solar_wind: -15.801148069157067\n",
      "Row(Geohash='d7jjtr7ksvh0', avgcloud=23.687550854353134, avgwind=7.485111079679649) comb_solar_wind: -16.202439774673486\n",
      "Job running time: 0:35:45\n"
     ]
    }
   ],
   "source": [
    "start = datetime.datetime.now().replace(microsecond=0)\n",
    "\n",
    "df.createOrReplaceTempView(\"SOLAR_WIND_DF\")\n",
    "\n",
    "print('Top 3 places for solar:')\n",
    "cloud = spark.sql(\"SELECT Geohash, avg(total_cloud_cover_entire_atmosphere) as avgcloud, \\\n",
    "                     avg(surface_wind_gust_surface) as avgwind \\\n",
    "                       FROM SOLAR_WIND_DF WHERE land_cover_land1_sea0_surface = 1 \\\n",
    "                         and total_cloud_cover_entire_atmosphere >= 0 \\\n",
    "                           group by Geohash order by avgcloud ASC limit 3\").collect()\n",
    "\n",
    "for c in cloud:\n",
    "    print(c)\n",
    "\n",
    "print('Top 3 places for wind farms:')    \n",
    "wind = spark.sql(\"SELECT Geohash, avg(total_cloud_cover_entire_atmosphere) as avgcloud, \\\n",
    "                    avg(surface_wind_gust_surface) as avgwind \\\n",
    "                      FROM SOLAR_WIND_DF WHERE land_cover_land1_sea0_surface = 1 \\\n",
    "                        and geopotential_height_surface > 80 \\\n",
    "                          and total_cloud_cover_entire_atmosphere >= 0 \\\n",
    "                            group by Geohash having avgwind > 6.5 order by avgwind DESC limit 3\").collect()\n",
    "\n",
    "for w in wind:\n",
    "    print(w)\n",
    "\n",
    "print('Top 3 places for solar and wind farms:')\n",
    "solar_wind = spark.sql(\"SELECT Geohash, avg(total_cloud_cover_entire_atmosphere) as avgcloud, \\\n",
    "                          avg(surface_wind_gust_surface) as avgwind \\\n",
    "                            FROM SOLAR_WIND_DF WHERE land_cover_land1_sea0_surface = 1 \\\n",
    "                              and geopotential_height_surface > 80 \\\n",
    "                                and total_cloud_cover_entire_atmosphere >= 0 \\\n",
    "                                  group by Geohash having avgwind > 6.5\").collect()\n",
    "\n",
    "\n",
    "def comparator(e):\n",
    "    return e.avgwind - e.avgcloud\n",
    "\n",
    "solar_wind.sort(reverse=True, key=comparator)\n",
    "\n",
    "count = 0\n",
    "for sw in solar_wind:\n",
    "    if count > 2:\n",
    "        break\n",
    "    count = count + 1\n",
    "    print(sw, 'comb_solar_wind:', sw.avgwind - sw.avgcloud)\n",
    "\n",
    "end = datetime.datetime.now().replace(microsecond=0)\n",
    "print('Job running time:', end-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example\n",
    "* Top 1 place for solar: Geohash='9mtz1cve0n5b', avgcloud=13.301871440195281, avgwind=2.864942249701797\n",
    "![](./images/solar_01.png)<br>\n",
    "* Top 1 place for wind farm: Geohash='f9pz8wckcbw0', avgcloud=72.79739625711962, avgwind=9.304476408392457\n",
    "![](./images/wind_01.png)<br>\n",
    "* Top 1 place for solar and wind farm: Geohash='d7jjffc3g57b', avgcloud=19.093572009764035, avgwind=7.262419868791074, comb_solar_wind: -11.83115214097296\n",
    "![](./images/solar_wind_01.png)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 1: Advanced Analysis\n",
    "### You’ve had the opportunity to analyze two datasets thus far; now it’s time to analyze a dataset of your own. Find a dataset online and use Spark (or Hadoop) to analyze it. You should:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. [0.5 pt] Describe the dataset\n",
    "* Police Department Incident Reports: Historical 2003 to May 2018 in San Francisco   [source from]: https://data.sfgov.org/Public-Safety/Police-Department-Incident-Reports-Historical-2003/tmnf-yvry\n",
    "* What's in this Dataset?\n",
    "    * The .csv file(462.9 MB) contains:   \n",
    "        * Rows: 2.21M \n",
    "        * Columns: 13 \n",
    "        * Each row is a: Incident Report\n",
    "* Here are features in the dataset:\n",
    "    * IncidntNum\n",
    "    * Category\n",
    "    * Descript\n",
    "    * DayOfWeek\n",
    "    * Date\n",
    "    * Time\n",
    "    * PdDistrict\n",
    "    * Resolution\n",
    "    * Address\n",
    "    * X\n",
    "    * Y\n",
    "    * Location\n",
    "    * PdId\n",
    "* Preview\n",
    "![](./images/PDIR_dataset.png)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. [0.5 pt] Outline the types of insights you hope to gain from it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. [1 pt] Make hypotheses about what you might find\n",
    "* Where is the most dangerous area in San Francisco ?\n",
    "* What kind of incident happened most in San Francisco ?\n",
    "* List some high incidence areas for 24 hours a day in order to remind people to avoid/pay more attention in those areas in a specific time.\n",
    "* How many incidents were finally resolved ?\n",
    "* Are there any reasons that may raise/drop the crime rate? Time? Place? Resolution rate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
