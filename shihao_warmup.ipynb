{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up our Schema\n",
    "\n",
    "Spark can automatically create a schema for CSV files, but ours don't have headings. Let's set this up here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StructType(List(StructField(Timestamp,LongType,true),StructField(Geohash,StringType,true),StructField(geopotential_height_lltw,FloatType,true),StructField(water_equiv_of_accum_snow_depth_surface,FloatType,true),StructField(drag_coefficient_surface,FloatType,true),StructField(sensible_heat_net_flux_surface,FloatType,true),StructField(categorical_ice_pellets_yes1_no0_surface,FloatType,true),StructField(visibility_surface,FloatType,true),StructField(number_of_soil_layers_in_root_zone_surface,FloatType,true),StructField(categorical_freezing_rain_yes1_no0_surface,FloatType,true),StructField(pressure_reduced_to_msl_msl,FloatType,true),StructField(upward_short_wave_rad_flux_surface,FloatType,true),StructField(relative_humidity_zerodegc_isotherm,FloatType,true),StructField(categorical_snow_yes1_no0_surface,FloatType,true),StructField(u-component_of_wind_tropopause,FloatType,true),StructField(surface_wind_gust_surface,FloatType,true),StructField(total_cloud_cover_entire_atmosphere,FloatType,true),StructField(upward_long_wave_rad_flux_surface,FloatType,true),StructField(land_cover_land1_sea0_surface,FloatType,true),StructField(vegitation_type_as_in_sib_surface,FloatType,true),StructField(v-component_of_wind_pblri,FloatType,true),StructField(albedo_surface,FloatType,true),StructField(lightning_surface,FloatType,true),StructField(ice_cover_ice1_no_ice0_surface,FloatType,true),StructField(convective_inhibition_surface,FloatType,true),StructField(pressure_surface,FloatType,true),StructField(transpiration_stress-onset_soil_moisture_surface,FloatType,true),StructField(soil_porosity_surface,FloatType,true),StructField(vegetation_surface,FloatType,true),StructField(categorical_rain_yes1_no0_surface,FloatType,true),StructField(downward_long_wave_rad_flux_surface,FloatType,true),StructField(planetary_boundary_layer_height_surface,FloatType,true),StructField(soil_type_as_in_zobler_surface,FloatType,true),StructField(geopotential_height_cloud_base,FloatType,true),StructField(friction_velocity_surface,FloatType,true),StructField(maximumcomposite_radar_reflectivity_entire_atmosphere,FloatType,true),StructField(plant_canopy_surface_water_surface,FloatType,true),StructField(v-component_of_wind_maximum_wind,FloatType,true),StructField(geopotential_height_zerodegc_isotherm,FloatType,true),StructField(mean_sea_level_pressure_nam_model_reduction_msl,FloatType,true),StructField(temperature_surface,FloatType,true),StructField(snow_cover_surface,FloatType,true),StructField(geopotential_height_surface,FloatType,true),StructField(convective_available_potential_energy_surface,FloatType,true),StructField(latent_heat_net_flux_surface,FloatType,true),StructField(surface_roughness_surface,FloatType,true),StructField(pressure_maximum_wind,FloatType,true),StructField(temperature_tropopause,FloatType,true),StructField(geopotential_height_pblri,FloatType,true),StructField(pressure_tropopause,FloatType,true),StructField(snow_depth_surface,FloatType,true),StructField(v-component_of_wind_tropopause,FloatType,true),StructField(downward_short_wave_rad_flux_surface,FloatType,true),StructField(u-component_of_wind_maximum_wind,FloatType,true),StructField(wilting_point_surface,FloatType,true),StructField(precipitable_water_entire_atmosphere,FloatType,true),StructField(u-component_of_wind_pblri,FloatType,true),StructField(direct_evaporation_cease_soil_moisture_surface,FloatType,true)))\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "from pyspark.sql.types import StructType, StructField, FloatType, LongType, StringType\n",
    "\n",
    "feats = []\n",
    "f = open('features.txt')\n",
    "for line_num, line in enumerate(f):\n",
    "    if line_num == 0:\n",
    "        # Timestamp\n",
    "        feats.append(StructField(line.strip(), LongType(), True))\n",
    "    elif line_num == 1:\n",
    "        # Geohash\n",
    "        feats.append(StructField(line.strip(), StringType(), True))\n",
    "    else:\n",
    "        # Other features\n",
    "        feats.append(StructField(line.strip(), FloatType(), True))\n",
    "    \n",
    "schema = StructType(feats)\n",
    "\n",
    "print(schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a Dataframe\n",
    "\n",
    "Let's load our CSV into a 'dataframe' - Spark's abstraction for working with tabular data (built on top of RDDs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Timestamp=1430438400000, Geohash='dndf9tz5r8eb', geopotential_height_lltw=1915.593994140625, water_equiv_of_accum_snow_depth_surface=0.0, drag_coefficient_surface=0.0, sensible_heat_net_flux_surface=-12.571273803710938, categorical_ice_pellets_yes1_no0_surface=0.0, visibility_surface=24220.529296875, number_of_soil_layers_in_root_zone_surface=3.0, categorical_freezing_rain_yes1_no0_surface=0.0, pressure_reduced_to_msl_msl=101235.0, upward_short_wave_rad_flux_surface=4.25, relative_humidity_zerodegc_isotherm=95.0, categorical_snow_yes1_no0_surface=0.0, u-component_of_wind_tropopause=20.28228759765625, surface_wind_gust_surface=3.9325132369995117, total_cloud_cover_entire_atmosphere=98.0, upward_long_wave_rad_flux_surface=371.25927734375, land_cover_land1_sea0_surface=1.0, vegitation_type_as_in_sib_surface=10.0, v-component_of_wind_pblri=-3.47259521484375, albedo_surface=17.25, lightning_surface=0.0, ice_cover_ice1_no_ice0_surface=0.0, convective_inhibition_surface=-12.582763671875, pressure_surface=98873.0, transpiration_stress-onset_soil_moisture_surface=0.35999998450279236, soil_porosity_surface=0.5, vegetation_surface=74.75, categorical_rain_yes1_no0_surface=0.0, downward_long_wave_rad_flux_surface=352.7455749511719, planetary_boundary_layer_height_surface=3799.75, soil_type_as_in_zobler_surface=4.0, geopotential_height_cloud_base=1795.5, friction_velocity_surface=0.12634363770484924, maximumcomposite_radar_reflectivity_entire_atmosphere=-5.625, plant_canopy_surface_water_surface=0.17249999940395355, v-component_of_wind_maximum_wind=-36.30204772949219, geopotential_height_zerodegc_isotherm=1940.0, mean_sea_level_pressure_nam_model_reduction_msl=101249.0, temperature_surface=284.513916015625, snow_cover_surface=0.0, geopotential_height_surface=199.8200225830078, convective_available_potential_energy_surface=320.0, latent_heat_net_flux_surface=6.516082763671875, surface_roughness_surface=0.1750158965587616, pressure_maximum_wind=36921.1640625, temperature_tropopause=211.69955444335938, geopotential_height_pblri=85.20362854003906, pressure_tropopause=10801.0634765625, snow_depth_surface=0.0, v-component_of_wind_tropopause=-10.068893432617188, downward_short_wave_rad_flux_surface=24.375, u-component_of_wind_maximum_wind=37.93487548828125, wilting_point_surface=0.08374999463558197, precipitable_water_entire_atmosphere=14.878089904785156, u-component_of_wind_pblri=-0.27671051025390625, direct_evaporation_cease_soil_moisture_surface=0.08374999463558197)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from pyspark.storagelevel import StorageLevel\n",
    "# spark.conf.set(\"spark.sql.broadcastTimeout\", 36000)\n",
    "spark.conf.set(\"spark.sql.broadcastTimeout\", 1200)\n",
    "# df = spark.read.format('csv').option('sep', '\\t').schema(schema).load('/Volumes/evo/Datasets/NAM_2015_S/*')\n",
    "# df = spark.read.format('csv').option('sep', '\\t').schema(schema).load('hdfs://orion11:37000/nam_tiny.tdv')\n",
    "# df = spark.read.format('csv').option('sep', '\\t').schema(schema).load('hdfs://orion11:37000/data/nam/nam_201509.tdv.gz')\n",
    "# df = spark.read.format('csv').option('sep', '\\t').schema(schema).load('hdfs://orion11:37000/data/nam_s/*')\n",
    "df = spark.read.format('csv').option('sep', '\\t').schema(schema).load('hdfs://orion11:37000/data/nam/*')\n",
    "\n",
    "# df.cache()\n",
    "# df.persist(StorageLevel.DISK_ONLY)\n",
    "df.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [0.5 pt] Unknown Feature: Choose a feature from the data dictionary above that you have never heard of before. Inspect some of the values for the feature (such as its average, min, max, etc.) and try to guess what it measures. Was your hypothesis correct? (Note: if you are a professional meteorologist, you can skip this question ;-))\n",
    "\n",
    "* The surface_roughness_surface_surface feature interests me, and I guess it measures the body senses the roughness of the air.\n",
    "* Surface roughness often shortened to roughness, is a component of surface texture. It is quantified by the deviations in the direction of the normal vector of a real surface from its ideal form. https://en.wikipedia.org/wiki/Surface_roughness\n",
    "* Research uses surface roughness model to explore the impact of isolated surface roughness anomalies on the model climate. https://journals.ametsoc.org/doi/full/10.1175/2007JAS2509.1\n",
    "* Job running time: 35 mins (seems each value's calculation runs for 11 mins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+\n",
      "|max(surface_roughness_surface)|\n",
      "+------------------------------+\n",
      "|                      2.750016|\n",
      "+------------------------------+\n",
      "\n",
      "+------------------------------+\n",
      "|min(surface_roughness_surface)|\n",
      "+------------------------------+\n",
      "|                  1.5900003E-5|\n",
      "+------------------------------+\n",
      "\n",
      "+------------------------------+\n",
      "|avg(surface_roughness_surface)|\n",
      "+------------------------------+\n",
      "|            0.4834922812580254|\n",
      "+------------------------------+\n",
      "\n",
      "Job running time: 0:35:14\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg, min, max\n",
    "\n",
    "a = datetime.datetime.now().replace(microsecond=0)\n",
    "\n",
    "# What's the maximum value?\n",
    "df.select(max(df.surface_roughness_surface)).show()\n",
    "\n",
    "# What's the minimum value?\n",
    "df.select(min(df.surface_roughness_surface)).show()\n",
    "\n",
    "# What's the average value?\n",
    "df.select(avg(df.surface_roughness_surface)).show()\n",
    "\n",
    "b = datetime.datetime.now().replace(microsecond=0)\n",
    "\n",
    "print('Job running time:', b-a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From: https://data.planetos.com/datasets/noaa_nam_awips_12\n",
    "\n",
    "![](./images/planetos_srs_title.png)<br>\n",
    "![](./images/planetos_srs.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [0.5 pt] Hot hot hot: When and where was the hottest temperature observed in the dataset? Is it an anomaly?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hottest temperature observed: [Row(Timestamp=1440266400000, Geohash='d5dpds10m55b', temperature_surface=331.390625)]\n",
      "Job running time: 0:24:06\n"
     ]
    }
   ],
   "source": [
    "a = datetime.datetime.now().replace(microsecond=0)\n",
    "\n",
    "df.createOrReplaceTempView(\"TEMP_DF\")\n",
    "hottest = spark.sql(\"SELECT Timestamp, Geohash, temperature_surface FROM TEMP_DF \\\n",
    "                        WHERE temperature_surface in (SELECT MAX(temperature_surface) FROM TEMP_DF)\").collect()\n",
    "\n",
    "b = datetime.datetime.now().replace(microsecond=0)\n",
    "\n",
    "print('Hottest temperature observed:', hottest)\n",
    "\n",
    "print('Job running time:', b-a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./images/hottest.png)<br>\n",
    "* Timestamp=1440266400000 -----> GMT: Saturday, August 22, 2015 6:00:00 PM\n",
    "* Super hot, can't live.\n",
    "* Job running time: 24 mins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [1 pt] So Snowy: Find a location that is snowy all year (there are several). Locate a nearby town/city and provide a small writeup about it. Include pictures if you’d like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(Count=436, Geohash='c43k6uu1egxb')\n",
      "Row(Count=436, Geohash='c43kcu3t702p')\n",
      "Row(Count=434, Geohash='c41uhb4r5n00')\n",
      "Row(Count=434, Geohash='c41ueb1jyypb')\n",
      "Row(Count=432, Geohash='c41v48pupf00')\n",
      "Row(Count=422, Geohash='c438x5esgf00')\n",
      "Row(Count=421, Geohash='c43b05v7222p')\n",
      "Row(Count=421, Geohash='c41v98n9w0xb')\n",
      "Row(Count=417, Geohash='c438fqgmsm00')\n",
      "Row(Count=417, Geohash='c439n53vsxzz')\n",
      "Job running time: 0:11:51\n"
     ]
    }
   ],
   "source": [
    "a = datetime.datetime.now().replace(microsecond=0)\n",
    "\n",
    "df.createOrReplaceTempView(\"SNOWY_DF\")\n",
    "\n",
    "snow_1 = spark.sql(\"SELECT count(*) as Count, Geohash FROM SNOWY_DF \\\n",
    "                       WHERE categorical_snow_yes1_no0_surface = 1 group by Geohash \\\n",
    "                           order by count(*) DESC\").collect()\n",
    "\n",
    "for s in range(10):\n",
    "    print(snow_1[s])\n",
    "\n",
    "b = datetime.datetime.now().replace(microsecond=0)\n",
    "\n",
    "print('Job running time:', b-a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./images/snowy_location.png)<br>\n",
    "* One of the snowy locations: Geohash='c43k6uu1egxb', which is in Juneau (the capital city of Alaska) and near the Glacier Bay National Park and Preserve. In the Geohashes Google Map picture, we can see the area is covered by white snow, and it seems as a mountain peak and is often snow. https://en.wikipedia.org/wiki/Juneau,_Alaska\n",
    "* Job running time: 11 mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
